{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "from api_keys import cohere_api_key, open_ai_api_key\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents\n",
    "\n",
    "Documents comes from https://github.com/huggingface/transformers/blob/main/awesome-transformers.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nAwesome projects built with Transformers\\n\\nThis page lists awesome projects built on top of Transformers. Transformers is more than a toolkit to use pretrained\\nmodels: it's a community of projects built around it and the Hugging Face Hub. We want Transformers to enable\\ndevelopers, researchers, students, professors, engineers, and anyone else to build their dream projects.\\n\\nIn this list, we showcase incredibly impactful and novel projects that have pushed the field forward. We celebrate\\n100 of these projects as we reach the milestone of 100k stars as a community; but we're very open to pull requests\\nadding other projects to the list. If you believe a project should be here and it's not, then please, open a PR \\nto add it.\\n\\n\",\n",
       " '\\n\\ngpt4all\\n\\ngpt4all is an ecosystem of open-source chatbots trained on massive collections of clean assistant data including code, stories and dialogue. It offers open-source, large language models such as LLaMA and GPT-J trained in an assistant-style.\\n\\nKeywords: Open-source, LLaMa, GPT-J, instruction, assistant\\n\\n',\n",
       " '\\n\\nrecommenders\\n\\nThis repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. It goes over several aspects required to build efficient recommendation systems: data preparation, modeling, evaluation, model selection & optimization, as well as operationalization\\n\\nKeywords: Recommender systems, AzureML\\n\\n',\n",
       " '\\n\\nlama-cleaner\\n\\nImage inpainting tool powered by Stable Diffusion. Remove any unwanted object, defect, people from your pictures or erase and replace anything on your pictures.\\n\\nKeywords: inpainting, SD, Stable Diffusion\\n\\n',\n",
       " '\\n\\nflair\\n\\nFLAIR is a powerful PyTorch NLP framework, convering several important tasks: NER, sentiment-analysis, part-of-speech tagging, text and document embeddings, among other things.\\n\\nKeywords: NLP, text embedding, document embedding, biomedical, NER, PoS, sentiment-analysis\\n\\n',\n",
       " '\\n\\nmindsdb\\n\\nMindsDB is a low-code ML platform, which automates and integrates several ML frameworks into the data stack as \"AI Tables\" to streamline the integration of AI into applications, making it accessible to developers of all skill levels.\\n\\nKeywords: Database, low-code, AI table\\n\\n',\n",
       " '\\n\\nlangchain\\n\\nlangchain is aimed at assisting in the development of apps merging both LLMs and other sources of knowledge. The library allows chaining calls to applications, creating a sequence across many tools.\\n\\nKeywords: LLMs, Large Language Models, Agents, Chains\\n\\n',\n",
       " \"\\n\\nLlamaIndex\\n\\nLlamaIndex is a project that provides a central interface to connect your LLM's with external data. It provides various kinds of indices and retreival mechanisms to perform different LLM tasks and obtain knowledge-augmented results.\\n\\nKeywords: LLMs, Large Language Models, Data Retrieval, Indices, Knowledge Augmentation \\n\\n\",\n",
       " '\\n\\nParlAI\\n\\nParlAI is a python framework for sharing, training and testing dialogue models, from open-domain chitchat, to task-oriented dialogue, to visual question answering. It provides more than 100 datasets under the same API, a large zoo of pretrained models, a set of agents, and has several integrations.\\n\\nKeywords: Dialogue, Chatbots, VQA, Datasets, Agents\\n\\n',\n",
       " '\\n\\nsentence-transformers\\n\\nThis framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various task. Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity.\\n\\nKeywords: Dense vector representations, Text embeddings, Sentence embeddings\\n\\n',\n",
       " '\\n\\nludwig\\n\\nLudwig is a declarative machine learning framework that makes it easy to define machine learning pipelines using a simple and flexible data-driven configuration system. Ludwig is targeted at a wide variety of AI tasks. It provides a data-driven configuration system, training, prediction, and evaluation scripts, as well as a programmatic API.\\n\\nKeywords: Declarative, Data-driven, ML Framework\\n\\n',\n",
       " '\\n\\nInvokeAI\\n\\nInvokeAI is an engine for Stable Diffusion models, aimed at professionals, artists, and enthusiasts. It leverages the latest AI-driven technologies through CLI as well as a WebUI.\\n\\nKeywords: Stable-Diffusion, WebUI, CLI\\n\\n',\n",
       " '\\n\\nPaddleNLP\\n\\nPaddleNLP is an easy-to-use and powerful NLP library particularly targeted at the Chinese languages. It has support for multiple pre-trained model zoos, and supports a wide-range of NLP tasks from research to industrial applications.\\n\\nKeywords: NLP, Chinese, Research, Industry\\n\\n',\n",
       " \"\\n\\nstanza\\n\\nThe Stanford NLP Group's official Python NLP library. It contains support for running various accurate natural language processing tools on 60+ languages and for accessing the Java Stanford CoreNLP software from Python.\\n\\nKeywords: NLP, Multilingual, CoreNLP\\n\\n\",\n",
       " '\\n\\nDeepPavlov\\n\\nDeepPavlov is an open-source conversational AI library. It is designed for the development of production ready chat-bots and complex conversational systems, as well as research in the area of NLP and, particularly, of dialog systems.\\n\\nKeywords: Conversational, Chatbot, Dialog\\n\\n',\n",
       " '\\n\\nalpaca-lora\\n\\nAlpaca-lora contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA). The repository provides training (fine-tuning) as well as generation scripts.\\n\\nKeywords: LoRA, Parameter-efficient fine-tuning\\n\\n',\n",
       " \"\\n\\nimagen-pytorch\\n\\nAn open-source Implementation of Imagen, Google's closed-source Text-to-Image Neural Network that beats DALL-E2. As of release, it is the new SOTA for text-to-image synthesis.\\n\\nKeywords: Imagen, Text-to-image\\n\\n\",\n",
       " \"\\n\\nadapter-transformers\\n\\nadapter-transformers is an extension of HuggingFace's Transformers library, integrating adapters into state-of-the-art language models by incorporating AdapterHub, a central repository for pre-trained adapter modules. It is a drop-in replacement for transformers, which is regularly updated to stay up-to-date with the developments of transformers.\\n\\nKeywords: Adapters, LoRA, Parameter-efficient fine-tuning, Hub\\n\\n\",\n",
       " '\\n\\nNeMo\\n\\nNVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new https://developer.nvidia.com/conversational-ai#started.\\n\\nKeywords: Conversational, ASR, TTS, LLMs, NLP\\n\\n',\n",
       " '\\n\\nRunhouse\\n\\nRunhouse allows to send code and data to any of your compute or data infra, all in Python, and continue to interact with them normally from your existing code and environment. Runhouse developers mention:\\n\\n> Think of it as an expansion pack to your Python interpreter that lets it take detours to remote machines or manipulate remote data.\\n\\nKeywords: MLOps, Infrastructure, Data storage, Modeling\\n\\n',\n",
       " '\\n\\nMONAI\\n\\nMONAI is a PyTorch-based, open-source framework for deep learning in healthcare imaging, part of PyTorch Ecosystem. Its ambitions are:\\n- developing a community of academic, industrial and clinical researchers collaborating on a common foundation;\\n- creating state-of-the-art, end-to-end training workflows for healthcare imaging;\\n- providing researchers with the optimized and standardized way to create and evaluate deep learning models.\\n\\nKeywords: Healthcare imaging, Training, Evaluation\\n\\n',\n",
       " '\\n\\nsimpletransformers\\n\\nSimple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model. It supports a wide variety of NLP tasks.\\n\\nKeywords: Framework, simplicity, NLP\\n\\n',\n",
       " '\\n\\nJARVIS\\n\\nJARVIS is a system attempting to merge LLMs such as GPT-4 with the rest of the open-source ML community: leveraging up to 60 downstream models in order to perform tasks identified by the LLM.\\n\\nKeywords: LLM, Agents, HF Hub\\n\\n',\n",
       " '\\n\\ntransformers.js\\n\\ntransformers.js is a JavaScript library targeted at running models from transformers directly within the browser.\\n\\nKeywords: Transformers, JavaScript, browser\\n\\n',\n",
       " '\\n\\nbumblebee\\n\\nBumblebee provides pre-trained Neural Network models on top of Axon, a neural networks library for the Elixir language. It includes integration with ü§ó Models, allowing anyone to download and perform Machine Learning tasks with few lines of code.\\n\\nKeywords: Elixir, Axon\\n\\n',\n",
       " '\\n\\nargilla\\n\\nArgilla is an open-source platform providing advanced NLP labeling, monitoring, and workspaces. It is compatible with many open source ecosystems such as Hugging Face, Stanza, FLAIR, and others.\\n\\nKeywords: NLP, Labeling, Monitoring, Workspaces\\n\\n',\n",
       " '\\n\\nhaystack\\n\\nHaystack is an open source NLP framework to interact with your data using Transformer models and LLMs. It offers production-ready tools to quickly build complex decision making, question answering, semantic search, text generation applications, and more.\\n\\nKeywords: NLP, Framework, LLM\\n\\n',\n",
       " \"\\n\\nspaCy\\n\\nspaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. It offers support for transformers models through its third party package, spacy-transformers.\\n\\nKeywords: NLP, Framework\\n\\n\",\n",
       " '\\n\\nspeechbrain\\n\\nSpeechBrain is an open-source and all-in-one conversational AI toolkit based on PyTorch.\\nThe goal is to create a single, flexible, and user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies, including systems for speech recognition, speaker recognition, speech enhancement, speech separation, language identification, multi-microphone signal processing, and many others.\\n\\nKeywords: Conversational, Speech\\n\\n',\n",
       " '\\n\\nskorch\\n\\nSkorch is a scikit-learn compatible neural network library that wraps PyTorch. It has support for models within transformers, and tokenizers from tokenizers.\\n\\nKeywords: Scikit-Learn, PyTorch\\n\\n',\n",
       " '\\n\\nbertviz\\n\\nBertViz is an interactive tool for visualizing attention in Transformer language models such as BERT, GPT2, or T5. It can be run inside a Jupyter or Colab notebook through a simple Python API that supports most Huggingface models.\\n\\nKeywords: Visualization, Transformers\\n\\n',\n",
       " '\\n\\nmesh-transformer-jax\\n\\nmesh-transformer-jax is a haiku library using the xmap/pjit operators in JAX for model parallelism of transformers. This library is designed for scalability up to approximately 40B parameters on TPUv3s. It was the library used to train the GPT-J model.\\n\\nKeywords: Haiku, Model parallelism, LLM, TPU\\n\\n',\n",
       " '\\n\\ndeepchem\\n\\nDeepChem aims to provide a high quality open-source toolchain that democratizes the use of deep-learning in drug discovery, materials science, quantum chemistry, and biology.\\n\\nKeywords: Drug discovery, Materials Science, Quantum Chemistry, Biology\\n\\n',\n",
       " '\\n\\nOpenNRE\\n\\nAn Open-Source Package for Neural Relation Extraction (NRE). It is targeted at a wide range of users, from newcomers to relation extraction, to developers, researchers, or students.\\n\\nKeywords: Neural Relation Extraction, Framework\\n\\n',\n",
       " '\\n\\npycorrector\\n\\nPyCorrector is a Chinese Text Error Correction Tool. It uses a language model to detect errors, pinyin feature and shape feature to correct Chinese text errors. it can be used for Chinese Pinyin and stroke input method.\\n\\nKeywords: Chinese, Error correction tool, Language model, Pinyin\\n\\n',\n",
       " '\\n\\nnlpaug\\n\\nThis python library helps you with augmenting nlp for machine learning projects. It is a lightweight library featuring synthetic data generation for improving model performance, support for audio and text, and compatibility with several ecosystems (scikit-learn, pytorch, tensorflow).\\n\\nKeywords: Data augmentation, Synthetic data generation, Audio, NLP\\n\\n',\n",
       " '\\n\\ndream-textures\\n\\ndream-textures is a library targeted at bringing stable-diffusion support within Blender. It supports several use-cases, such as image generation, texture projection, inpainting/outpainting, ControlNet, and upscaling.\\n\\nKeywords: Stable-Diffusion, Blender\\n\\n',\n",
       " '\\n\\nseldon-core\\n\\nSeldon core converts your ML models (Tensorflow, Pytorch, H2o, etc.) or language wrappers (Python, Java, etc.) into production REST/GRPC microservices.\\nSeldon handles scaling to thousands of production machine learning models and provides advanced machine learning capabilities out of the box including Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests, Canaries and more.\\n\\nKeywords: Microservices, Modeling, Language wrappers\\n\\n',\n",
       " '\\n\\nopen_model_zoo\\n\\nThis repository includes optimized deep learning models and a set of demos to expedite development of high-performance deep learning inference applications. Use these free pre-trained models instead of training your own models to speed-up the development and production deployment process.\\n\\nKeywords: Optimized models, Demos\\n\\n',\n",
       " '\\n\\nml-stable-diffusion\\n\\nML-Stable-Diffusion is a repository by Apple bringing Stable Diffusion support to Core ML, on Apple Silicon devices. It supports stable diffusion checkpoints hosted on the Hugging Face Hub.\\n\\nKeywords: Stable Diffusion, Apple Silicon, Core ML\\n\\n',\n",
       " '\\n\\nstable-dreamfusion\\n\\nStable-Dreamfusion is a pytorch implementation of the text-to-3D model Dreamfusion, powered by the Stable Diffusion text-to-2D model.\\n\\nKeywords: Text-to-3D, Stable Diffusion\\n\\n',\n",
       " '\\n\\ntxtai\\n \\ntxtai is an open-source platform for semantic search and workflows powered by language models. txtai builds embeddings databases, which are a union of vector indexes and relational databases enabling similarity search with SQL. Semantic workflows connect language models together into unified applications.\\n\\nKeywords: Semantic search, LLM\\n\\n',\n",
       " '\\n\\ndjl\\n\\nDeep Java Library (DJL) is an open-source, high-level, engine-agnostic Java framework for deep learning. DJL is designed to be easy to get started with and simple to use for developers. DJL provides a native Java development experience and functions like any other regular Java library. DJL offers a Java binding for HuggingFace Tokenizers and easy conversion toolkit for HuggingFace model to deploy in Java.\\n\\nKeywords: Java, Framework\\n\\n',\n",
       " '\\n\\nlm-evaluation-harness\\n\\nThis project provides a unified framework to test generative language models on a large number of different evaluation tasks. It has support for more than 200 tasks, and supports different ecosystems: HF Transformers, GPT-NeoX, DeepSpeed, as well as the OpenAI API.\\n\\nKeywords: LLM, Evaluation, Few-shot\\n\\n',\n",
       " \"\\n\\ngpt-neox\\n\\nThis repository records EleutherAI's library for training large-scale language models on GPUs. The framework is based on NVIDIA's Megatron Language Model and has been augmented with techniques from DeepSpeed as well as some novel optimizations. It is focused on training multi-billion-parameter models.\\n\\nKeywords: Training, LLM, Megatron, DeepSpeed\\n\\n\",\n",
       " '\\n\\nmuzic\\n\\nMuzic is a research project on AI music that empowers music understanding and generation with deep learning and artificial intelligence. Muzic was created by researchers from Microsoft Research Asia.\\n\\nKeywords: Music understanding, Music generation\\n\\n',\n",
       " '\\n\\ndalle-flow\\n\\nDALL¬∑E Flow is an interactive workflow for generating high-definition images from a text prompt. Itt leverages DALL¬∑E-Mega, GLID-3 XL, and Stable Diffusion to generate image candidates, and then calls CLIP-as-service to rank the candidates w.r.t. the prompt.\\nThe preferred candidate is fed to GLID-3 XL for diffusion, which often enriches the texture and background. Finally, the candidate is upscaled to 1024x1024 via SwinIR.\\n\\nKeywords: High-definition image generation, Stable Diffusion, DALL-E Mega, GLID-3 XL, CLIP, SwinIR\\n\\n',\n",
       " '\\n\\nlightseq\\n\\nLightSeq is a high performance training and inference library for sequence processing and generation implemented in CUDA. It enables highly efficient computation of modern NLP and CV models such as BERT, GPT, Transformer, etc. It is therefore best useful for machine translation, text generation, image classification, and other sequence related tasks.\\n\\nKeywords: Training, Inference, Sequence Processing, Sequence Generation\\n\\n',\n",
       " '\\n\\nLaTeX-OCR\\n\\nThe goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code.\\n\\nKeywords: OCR, LaTeX, Math formula\\n\\n',\n",
       " \"\\n\\nopen_clip\\n\\nOpenCLIP is an open source implementation of OpenAI's CLIP.\\n\\nThe goal of this repository is to enable training models with contrastive image-text supervision, and to investigate their properties such as robustness to distribution shift. \\nThe starting point is an implementation of CLIP that matches the accuracy of the original CLIP models when trained on the same dataset. \\n\\nSpecifically, a ResNet-50 model trained with this codebase on OpenAI's 15 million image subset of YFCC achieves 32.7% top-1 accuracy on ImageNet.\\n\\nKeywords: CLIP, Open-source, Contrastive, Image-text\\n\\n\",\n",
       " '\\n\\ndalle-playground\\n\\nA playground to generate images from any text prompt using Stable Diffusion and Dall-E mini.\\n\\nKeywords: WebUI, Stable Diffusion, Dall-E mini\\n\\n',\n",
       " '\\n\\nFedML\\n\\nFedML is a federated learning and analytics library enabling secure and collaborative machine learning on decentralized data anywhere at any scale.\\n\\nIt supports large-scale cross-silo federated learning, and cross-device federated learning on smartphones/IoTs, and research simulation.\\n\\nKeywords: Federated Learning, Analytics, Collaborative ML, Decentralized\\n\\n',\n",
       " '\\n\\ngpt-code-clippy\\n\\nGPT-Code-Clippy (GPT-CC) is an open source version of GitHub Copilot, a language model -- based on GPT-3, called GPT-Codex -- that is fine-tuned on publicly available code from GitHub.\\n\\nKeywords: LLM, Code\\n\\n',\n",
       " '\\n\\nTextAttack\\n\\nTextAttack üêô is a Python framework for adversarial attacks, data augmentation, and model training in NLP.\\n\\nKeywords: Adversarial attacks, Data augmentation, NLP\\n\\n',\n",
       " '\\n\\nOpenPrompt\\n\\nPrompt-learning is a paradigm to adapt pre-trained language models (PLMs) to downstream NLP tasks, which modify the input text with a textual template and directly uses PLMs to conduct pre-trained tasks. This library provides a standard, flexible and extensible framework to deploy the prompt-learning pipeline. OpenPrompt supports loading PLMs directly from https://github.com/huggingface/transformers.\\n\\n',\n",
       " '\\n\\ntext-generation-webui\\n\\ntext-generation-webui is a Gradio Web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.\\n\\nKeywords: LLM, WebUI\\n\\n',\n",
       " '\\n\\nlibra\\n\\nAn ergonomic machine learning library for non-technical users. It focuses on ergonomics and on ensuring that training a model is as simple as it can be.\\n\\nKeywords: Ergonomic, Non-technical\\n\\n',\n",
       " '\\n\\nalibi\\n\\nAlibi is an open source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local and global explanation methods for classification and regression models.\\n\\nKeywords: Model inspection, Model interpretation, Black-box, White-box\\n\\n',\n",
       " '\\n\\ntortoise-tts\\n\\nTortoise is a text-to-speech program built with the following priorities: strong multi-voice capabilities, and highly realistic prosody and intonation.\\n\\nKeywords: Text-to-speech\\n\\n',\n",
       " '\\n\\nflower\\n\\nFlower (flwr) is a framework for building federated learning systems. The design of Flower is based on a few guiding principles: customizability, extendability, framework agnosticity, and ease-of-use.\\n\\nKeywords: Federated learning systems, Customizable, Extendable, Framework-agnostic, Simplicity\\n\\n',\n",
       " '\\n\\nfast-bert\\n\\nFast-Bert is a deep learning library that allows developers and data scientists to train and deploy BERT and XLNet based models for natural language processing tasks beginning with Text Classification. It is aimed at simplicity.\\n\\nKeywords: Deployment, BERT, XLNet\\n\\n',\n",
       " \"\\n\\ntowhee\\n\\nTowhee makes it easy to build neural data processing pipelines for AI applications. We provide hundreds of models, algorithms, and transformations that can be used as standard pipeline building blocks. Users can use Towhee's Pythonic API to build a prototype of their pipeline and automatically optimize it for production-ready environments.\\n\\nKeywords: Data processing pipeline, Optimization\\n\\n\",\n",
       " '\\n\\nalibi-detect\\n\\nAlibi Detect is an open source Python library focused on outlier, adversarial and drift detection. The package aims to cover both online and offline detectors for tabular data, text, images and time series. Both TensorFlow and PyTorch backends are supported for drift detection.\\n\\nKeywords: Adversarial, Outlier, Drift detection\\n\\n',\n",
       " \"\\n\\nFARM\\n\\nFARM makes Transfer Learning with BERT & Co simple, fast and enterprise-ready. It's built upon transformers and provides additional features to simplify the life of developers: Parallelized preprocessing, highly modular design, multi-task learning, experiment tracking, easy debugging and close integration with AWS SageMaker.\\n\\nKeywords: Transfer Learning, Modular design, Multi-task learning, Experiment tracking\\n\\n\",\n",
       " \"\\n\\naitextgen\\n\\nA robust Python tool for text-based AI training and generation using OpenAI's GPT-2 and EleutherAI's GPT Neo/GPT-3 architecture.\\naitextgen is a Python package that leverages PyTorch, Hugging Face Transformers and pytorch-lightning with specific optimizations for text generation using GPT-2, plus many added features.\\n\\nKeywords: Training, Generation\\n\\n\",\n",
       " '\\n\\ndiffgram\\n\\nDiffgram aims to integrate human supervision into platforms. We support your team programmatically changing the UI (Schema, layout, etc.) like in Streamlit. This means that you can collect and annotate timely data from users. In other words, we are the platform behind your platform, an integrated part of your application, to ship new & better AI products faster.\\n\\nKeywords: Human supervision, Platform\\n\\n',\n",
       " '\\n\\necco\\n\\nExplain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).\\n\\nKeywords: Model explainability\\n\\n',\n",
       " '\\n\\ns3prl\\n\\ns3prl stands for Self-Supervised Speech Pre-training and Representation Learning. Self-supervised speech pre-trained models are called upstream in this toolkit, and are utilized in various downstream tasks.\\n\\nKeywords: Speech, Training\\n\\n',\n",
       " '\\n\\nru-dalle\\n\\nRuDALL-E aims to be similar to DALL-E, targeted to Russian.\\n\\nKeywords: DALL-E, Russian\\n\\n',\n",
       " '\\n\\nDeepKE\\n\\nDeepKE is a knowledge extraction toolkit for knowledge graph construction supporting cnSchemaÔºålow-resource, document-level and multimodal scenarios for entity, relation and attribute extraction.\\n\\nKeywords: Knowledge Extraction, Knowledge Graphs\\n\\n',\n",
       " '\\n\\nNebuly\\n\\nNebuly is the next-generation platform to monitor and optimize your AI costs in one place. The platform connects to all your AI cost sources (compute, API providers, AI software licenses, etc) and centralizes them in one place to give you full visibility on a model basis. The platform also provides optimization recommendations and a co-pilot model that can guide during the optimization process. The platform builds on top of the open-source tools allowing you to optimize the different steps of your AI stack to squeeze out the best possible cost performances.\\n\\nKeywords: Optimization, Performance, Monitoring\\n\\n',\n",
       " '\\n\\nimaginAIry\\n\\nOffers a CLI and a Python API to generate images with Stable Diffusion. It has support for many tools, like image structure control (controlnet), instruction-based image edits (InstructPix2Pix), prompt-based masking (clipseg), among others.\\n\\nKeywords: Stable Diffusion, CLI, Python API\\n\\n',\n",
       " '\\n\\nsparseml\\n\\nSparseML is an open-source model optimization toolkit that enables you to create inference-optimized sparse models using pruning, quantization, and distillation algorithms. Models optimized with SparseML can then be exported to the ONNX and deployed with DeepSparse for GPU-class performance on CPU hardware.\\n\\nKeywords: Model optimization, Pruning, Quantization, Distillation\\n\\n',\n",
       " '\\n\\nopacus\\n\\nOpacus is a library that enables training PyTorch models with differential privacy. It supports training with minimal code changes required on the client, has little impact on training performance, and allows the client to online track the privacy budget expended at any given moment.\\n\\nKeywords: Differential privacy\\n\\n',\n",
       " '\\n\\nLAVIS\\n\\nLAVIS is a Python deep learning library for LAnguage-and-VISion intelligence research and applications. This library aims to provide engineers and researchers with a one-stop solution to rapidly develop models for their specific multimodal scenarios, and benchmark them across standard and customized datasets. It features a unified interface design to access\\n\\nKeywords: Multimodal, NLP, Vision\\n\\n',\n",
       " \"\\n\\nbuzz\\n\\nBuzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.\\n\\nKeywords: Audio transcription, Translation\\n\\n\",\n",
       " \"\\n\\nrust-bert\\n\\nRust-native state-of-the-art Natural Language Processing models and pipelines. Port of Hugging Face's Transformers library, using the tch-rs crate and pre-processing from rust-tokenizers. Supports multi-threaded tokenization and GPU inference. This repository exposes the model base architecture, task-specific heads and ready-to-use pipelines.\\n\\nKeywords: Rust, BERT, Inference\\n\\n\",\n",
       " '\\n\\nEasyNLP\\n\\nEasyNLP is an easy-to-use NLP development and application toolkit in PyTorch, first released inside Alibaba in 2021. It is built with scalable distributed training strategies and supports a comprehensive suite of NLP algorithms for various NLP applications. EasyNLP integrates knowledge distillation and few-shot learning for landing large pre-trained models, together with various popular multi-modality pre-trained models. It provides a unified framework of model training, inference, and deployment for real-world applications.\\n\\nKeywords: NLP, Knowledge distillation, Few-shot learning, Multi-modality, Training, Inference, Deployment\\n\\n',\n",
       " '\\n\\nTurboTransformers\\n\\nA fast and user-friendly runtime for transformer inference (Bert, Albert, GPT2, Decoders, etc) on CPU and GPU.\\n\\nKeywords: Optimization, Performance\\n\\n',\n",
       " '\\n\\nhivemind\\n\\nHivemind is a PyTorch library for decentralized deep learning across the Internet. Its intended usage is training one large model on hundreds of computers from different universities, companies, and volunteers.\\n\\nKeywords: Decentralized training\\n\\n',\n",
       " '\\n\\ndocquery\\n\\nDocQuery is a library and command-line tool that makes it easy to analyze semi-structured and unstructured documents (PDFs, scanned images, etc.) using large language models (LLMs). You simply point DocQuery at one or more documents and specify a question you want to ask. DocQuery is created by the team at Impira.\\n\\nKeywords: Semi-structured documents, Unstructured documents, LLM, Document Question Answering\\n\\n',\n",
       " '\\n\\nCodeGeeX\\n\\nCodeGeeX is a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages. It has several unique features:\\n- Multilingual code generation\\n- Crosslingual code translation\\n- Is a customizable programming assistant\\n\\nKeywords: Code Generation Model\\n\\n',\n",
       " '\\n\\nktrain\\n\\nktrain is a lightweight wrapper for the deep learning library TensorFlow Keras (and other libraries) to help build, train, and deploy neural networks and other machine learning models. Inspired by ML framework extensions like fastai and ludwig, ktrain is designed to make deep learning and AI more accessible and easier to apply for both newcomers and experienced practitioners.\\n\\nKeywords: Keras wrapper, Model building, Training, Deployment\\n\\n',\n",
       " \"\\n\\nFastDeploy\\n\\nFastDeploy is an Easy-to-use and High Performance AI model deployment toolkit for Cloud, Mobile and Edge with packageout-of-the-box and unified experience, endend-to-end optimization for over fire160+ Text, Vision, Speech and Cross-modal AI models. Including image classification, object detection, OCR, face detection, matting, pp-tracking, NLP, stable diffusion, TTS and other tasks to meet developers' industrial deployment needs for multi-scenario, multi-hardware and multi-platform.\\n\\nKeywords: Model deployment, CLoud, Mobile, Edge\\n\\n\",\n",
       " '\\n\\nunderthesea\\n\\nunderthesea is a Vietnamese NLP toolkit. Underthesea is a suite of open source Python modules data sets and tutorials supporting research and development in Vietnamese Natural Language Processing. We provides extremely easy API to quickly apply pretrained NLP models to your Vietnamese text, such as word segmentation, part-of-speech tagging (PoS), named entity recognition (NER), text classification and dependency parsing.\\n\\nKeywords: Vietnamese, NLP\\n\\n',\n",
       " '\\n\\nhasktorch\\n\\nHasktorch is a library for tensors and neural networks in Haskell. It is an independent open source community project which leverages the core C++ libraries shared by PyTorch.\\n\\nKeywords: Haskell, Neural Networks\\n\\n',\n",
       " '\\n\\ndonut\\n\\nDonut, or Document understanding transformer, is a new method of document understanding that utilizes an OCR-free end-to-end Transformer model.\\n\\nDonut does not require off-the-shelf OCR engines/APIs, yet it shows state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing).\\n\\nKeywords: Document Understanding\\n\\n',\n",
       " '\\n\\ntransformers-interpret\\n\\nTransformers Interpret is a model explainability tool designed to work exclusively with the transformers package.\\n\\nIn line with the philosophy of the Transformers package Transformers Interpret allows any transformers model to be explained in just two lines. Explainers are available for both text and computer vision models. Visualizations are also available in notebooks and as savable png and html files\\n\\nKeywords: Model interpretation, Visualization\\n\\n',\n",
       " '\\n\\nmlrun\\n\\nMLRun is an open MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications, significantly reducing engineering efforts, time to production, and computation resources. With MLRun, you can choose any IDE on your local machine or on the cloud. MLRun breaks the silos between data, ML, software, and DevOps/MLOps teams, enabling collaboration and fast continuous improvements.\\n\\nKeywords: MLOps\\n\\n',\n",
       " '\\n\\nFederatedScope\\n\\nFederatedScope is a comprehensive federated learning platform that provides convenient usage and flexible customization for various federated learning tasks in both academia and industry. Based on an event-driven architecture, FederatedScope integrates rich collections of functionalities to satisfy the burgeoning demands from federated learning, and aims to build up an easy-to-use platform for promoting learning safely and effectively.\\n\\nKeywords: Federated learning, Event-driven\\n\\n',\n",
       " '\\n\\npythainlp\\n\\nPyThaiNLP is a Python package for text processing and linguistic analysis, similar to NLTK with focus on Thai language.\\n\\nKeywords: Thai, NLP, NLTK\\n\\n',\n",
       " '\\n\\nFlagAI\\n\\nFlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.\\n\\nKeywords: Large models, Training, Fine-tuning, Deployment, Multi-modal\\n\\n',\n",
       " \"\\n\\npyserini\\n\\npyserini is a Python toolkit for reproducible information retrieval research with sparse and dense representations. Retrieval using sparse representations is provided via integration with the group's Anserini IR toolkit. Retrieval using dense representations is provided via integration with Facebook's Faiss library.\\n\\nKeywords: IR, Information Retrieval, Dense, Sparse\\n\\n\",\n",
       " '\\n\\nbaal\\n\\nbaal is an active learning library that supports both industrial applications and research usecases. baal currently supports Monte-Carlo Dropout, MCDropConnect, deep ensembles, and semi-supervised learning.\\n\\nKeywords: Active Learning, Research, Labeling\\n\\n',\n",
       " '\\n\\ncleanlab\\n\\ncleanlab is the standard data-centric AI package for data quality and machine learning with messy, real-world data and labels. For text, image, tabular, audio (among others) datasets, you can use cleanlab to automatically: detect data issues (outliers, label errors, near duplicates, etc), train robust ML models, infer consensus + annotator-quality for multi-annotator data, suggest data to (re)label next (active learning).\\n\\nKeywords: Data-Centric AI, Data Quality, Noisy Labels, Outlier Detection, Active Learning  \\n\\n\\n',\n",
       " '## Designing Data-Intensive Applications\\n\\nby Martin Kleppmann\\n\\n### Chapter 1: Reliable, Scalable, and Maintainable Applications\\n\\n* Many applications are data-intensive and not compute intensive, meaning the biggest problems are usually the amount of data, the complexity of data, and the speed at which it is changing.\\n\\n#### Thinking About Data Systems\\n\\n* No single tool can meet all data processing and storage needs. Instead work is broken down into tasks that can be performed efficiently on a single tool, and application code stitches those tools together.\\n* When you create an API in front of several tools, you have created a new, special-purpose data system from smaller, general-purpose components.\\n* This book focuses on three main concerns: reliability, scalability, and maintainability.\\n\\n#### Reliability\\n\\n* Reliability means the system continues to work correctly, even when things go wrong.\\n* Things that can go wrong are called *faults*, and systems that anticipate faults and can cope with them are called *fault-tolerant* or *resilient*.\\n* A *fault* is when one component of the system deviates from its spec, whereas a *failure* is when the system as a whole stops providing the required service to the user.\\n* You cannot reduce the possibility of a fault to zero, and so you must design fault-tolerance mechanisms that prevent faults from causing failures.\\n\\n##### Hardware Faults\\n\\n* Hard disks have a MTTF of 10 to 50 years, and so on a storage cluster with 10,000 disks, we should expect on average one disk to die per day.\\n\\n##### Software Errors\\n\\n* Systemic errors, such as leap second bugs or cascading failures, are correlated across nodes and so tend to cause more system failures than uncorrelated hardware faults.\\n\\n##### Human Errors\\n\\n* Allow quick recovery from human errors, such as making it fast to rollback configuration changes, and to roll out code gradually.\\n* Set up detailed and clear monitoring, such as performance metrics and error rates.\\n\\n#### Scalability\\n\\n* Scalability is the term we use to describe a system\\'s ability to cope with increased load.\\n\\n##### Describing Load\\n\\n* Load can be described with a few numbers that we call *load parameters*. The best choice of parameters depends on the architecture of your system.\\n* For Twitter, the distribution of followers per user is a key load parameter for describing scalability, as it determines the fan-out load.\\n\\n##### Describing Performance\\n\\n* You can look at what happens when load increases in two ways:\\n  * When you increase a load parameter and keep system resources fixed, how is the performance of your system affected?\\n  * When you increase a load parameter, how much do you need to increase the resources if you want to keep performance unchanged?\\n* Response time what the client sees, including the time to process the request (the service time) and network delays and queuing delays.\\n* Latency is the duration that a request is waiting to be handled ‚Äì during which it is *latent*, awaiting service.\\n* The mean is not a good metrics to describe the \"typical\" response time, as it doesn\\'t tell you how many users experienced that delay. Prefer percentiles instead.\\n* High percentiles of response times, or tail latencies, are important because they directly affect users\\' experience of the service.\\n* Reducing response times at high percentiles is difficult because they are easily affected by random events outside your control, and the benefits are diminishing.\\n* When artificially generating load to test scalability, the client must send requests independently of the response time, or else it will artificially keep the queues shorter than in reality, thereby skewing the measurements.\\n* Even if you call multiple backend services in parallel, the end-user request must wait for the slowest of the parallel calls to complete.\\n* Averaging percentiles, such as when combining data from several machines, is mathematically meaningless. The right way of aggregating response time data is to add the histograms.\\n\\n##### Approaches for Coping with Load\\n\\n* Scaling up, or vertical scaling, is moving to a more powerful machine. Scaling out, or horizontal scaling, distributes the load across multiple smaller machines.\\n* An architecture that scales well for a particular application is built around assumptions of operations will be common and which will be rare\\xa0‚Äì the load parameters.\\n* In an early-stage startup it\\'s more important to be able to iterate quickly on product features than to scale beyond some hypothetical future load.\\n\\n#### Maintainability\\n\\n* The majority of the cost of software is not in its initial development, but its ongoing maintenance.\\n* Three important design principles for maintainability are:\\n  * Operability: Make it easy for the operations teams to keep the system running smoothly.\\n  * Simplicity: Make it easy for new engineers to understand the system by removing complexity.\\n  * Evolvability (or extensibility): Make it easy for engineers to adapt the system to unanticipated use cases as requirements change.\\n\\n##### Operability: Making Life Easy for Operations\\n\\n* Good operations can work around the limitations of bad software, but good software cannot run reliably with bad operations.\\n* A good operations team works to preserve the organization\\'s knowledge about the system, even as individuals come and go.\\n* Data systems with good operability provide good default behavior but allow overriding, and exhibit predictable behavior to minimize surprises.\\n\\n##### Simplicity: Managing Complexity\\n\\n* Making a system simpler does not necessarily mean reducing its functionality; it can also mean removing *accidental complexity*.\\n* Accidental complexity is complexity that is not inherent in the problem that the software solves, but arises only from the implementation.\\n* Good abstractions are one of the best tools for removing accidental complexity by hiding implementation details, but finding good abstractions is very hard.\\n\\n##### Evolvability: Making Change Easy\\n\\n* Simple and easy-to-understand systems are usually easier to modify than complex ones.\\n\\n#### Summary\\n\\n* Functional requirements are what an application should do.\\n* Nonfunctional requirements are general properties like security, reliability, compliance, scalability, compatibility, and maintainability.\\n\\n### Chapter 2: Data Models and Query Languages\\n\\n* Data models are the most important part of developing software, because they affect not only how the software is written, but how we think about the problem that we\\'re solving.\\n* Layered software abstractions allow different groups of people to work together effectively.\\n\\n#### Relational Model Versus Document Model\\n\\n##### The Birth of NoSQL\\n\\n* The term NoSQL has been retroactively reinterpreted as _Not Only SQL_.\\n\\n##### The Object-Relational Mismatch\\n\\n* Impedance mismatch is the disconnect between objects in the application code and the database model of tables, rows, and columns.\\n* A JSON representation of a document-oriented database has better locality than an equivalent multi-table schema.\\n\\n##### Many-to-One and Many-to-Many Relationships\\n\\n* In a document-oriented database, if information is duplicated and that information is changed, then all redundant copies need to be updated. Normalization is the key idea behind removing such duplication.\\n\\n##### Are Document Databases Repeating History?\\n\\n###### The relational model\\n\\n* A key insight of the relational model was that you only need to build a query optimizer once, and then all database clients can benefit from it.\\n\\n###### Comparison to document databases\\n\\n* For many-to-one and many-to-many relationships, a related item is referenced by a *foreign key* in the relational model, and a *document reference* in the document model.\\n\\n##### Relational Versus Document Databases Today\\n\\n* The document data model offers greater schema flexibility, better performance from locality, and for some applications it is closer to the data structures used by the application.\\n* The relational model offers better support for joins, and many-to-one and many-to-many relationships.\\n\\n###### Which data model leads to simpler application code?\\n\\n* If your application data has a document-like structure (i.e. a tree of one-to-many relationships, where typically the entire tree is loaded at once), then a document model is likely a good idea.\\n* But if your application uses many-to-many relationships, the document model becomes less appealing.\\n\\n###### Schema flexibility in the document model\\n\\n* Document databases are sometimes called *schemaless*, but that\\'s misleading ‚Äì there is an implicit schema assumed by the application, but not enforced by the database.\\n* A more accurate term is *schema-on-read* in contrast with *schema-on-write*.\\n* Schema-on-read is similar to dynamic (runtime) type checking in programming languages, whereas schema-on-write is similar to static (compile-type) type checking.\\n* In cases where all records are expected to have the same structure, schemas are a useful mechanism for documenting and enforcing a data structure.\\n\\n###### Data locality for queries\\n\\n* The locality advantage only applies if you need large parts of the document at the same time.\\n\\n#### Query Languages for Data\\n\\n* An imperative language tells the computer to perform certain operations in a certain order.\\n* In a declarative query language like SQL or relational algebra, you specify the pattern of the data you want, but not *how* to achieve that goal.\\n* A declarative query language also hides implementation details of the database engine, allowing it to add performance improvements without requiring changes to queries.\\n* Declarative languages are more amenable to parallel execution because they specify the pattern of the results, but not the algorithm used to determine them.\\n\\n##### MapReduce Querying\\n\\n* MapReduce is based on a `map` (or `collect`) and `reduce` (or `fold` or `inject`) functions that exist in many functional languages.\\n* The `map` and `reduce` functions must be pure, thereby allowing the database to run them anywhere, and to rerun them on failure.\\n\\n#### Graph-Like Data Models\\n\\n##### Property Graphs\\n\\n* A property graph model, each vertex and edge has a collection of properties (i.e. key-value pairs), and each edge has a label to describe the relationship between its vertices.\\n* To get the set of incoming and outgoing edges for a vertex, you can query the `edges` table by `head_vertex` or `tail_vertex` respectively.\\n\\n##### Graph Queries in SQL\\n\\n* In a relational database, you usually know in advance which joins you need in your query.\\n* In a graph query, you may need to traverse a variable number of edges before you find the vertex you\\'re looking for ‚Äì\\xa0i.e. the number of joins is not fixed in advance.\\n\\n##### Triple-Stores and SPARQL\\n\\n* In a triple-store, all information is stored in the form of very simple three-part statements: (_subject_, _predicate_, _object_).\\n* The object is one of two things:\\n  * A value in a primitive data type, and so the predicate and object are equivalent to the key and value of a property on the subject vertex. (E.g. (_lucy_, _age_, _33_) is like a vertex `lucy` with properties `{\"age\": 33}`.)\\n  * Another vertex in the graph, and so the predicate is an edge, the subject is the tail vertex, and the object is the head vertex. (E.g. (_alice_, _married_to_, _bob_) has an edge _married_to_ between _alice_ and _bob_.)\\n\\n##### The Foundation: Datalog\\n\\n* Datalog generalizes the triple-store model, writing the triple as _predicate(subject, object)_.\\n* A Datalog rule applies if the system can find a match for _all_ predicates on the right side of the `:-` operator.\\n* When a rule applies, it\\'s as though the left side of the `:-` was added to the database, with variables replaced by the values they matched.\\n\\n### Chapter 3: Storage and Retrieval\\n\\n* There is a big difference between storage engines optimized for transactional workloads and those optimized for analytics.\\n\\n#### Data Structures That Power Your Database\\n\\n* *Log* is defined as an append-only sequence of records. It doesn\\'t have to be human-readable, and instead might be binary.\\n* Well-chosen indexes speed up read queries, but every index slows down writes, because the index also needs to be updated every time data is written.\\n\\n##### Hash Indexes\\n\\n* If new and updated values are blindly written to a log, that log can be divided into segments. Compacting a segment keeps only the most recent value for each key, discarding the older ones.\\n* Since compaction makes segments much smaller (assuming a key is updated several times in one segment), compaction can also merge several segments together.\\n* A background thread can merge and compact segments. While it happens, the database can still serve reads from old segment files, and write requests to the latest segment file.\\n* A *tombstone* marks a deleted key. When segments are merged, the tombstone tells the merging process to discard all previous values for the key.\\n* Data file segments are append-only and otherwise immutable, and so they can be read concurrently by multiple threads.\\n* Append-only design is advantageous in several ways:\\n  * Appending and segment merging are sequential write operations, which are generally much faster than random writes.\\n  * Crash recovery are much simpler if segment files are append-only or immutable, e.g. you avoid crashing while overwriting a value and leaving old and new data spliced together.\\n  * Merging old segments avoids the problem of data files getting fragmented over time.\\n\\n##### SSTables and LSM-Trees\\n\\n* Segment files sorted by key is a format called *Sorted String Table*, or *SSTable* for short.\\n* This has several advantages over log segments with hash indexes:\\n  * Merging segments is efficient even if the files exceed available memory, by using an algorithm similar to mergesort.\\n  * Your in-memory index of keys to file offsets can be sparse, and so the total size of your keys can exceed available memory.\\n  * If your in-memory index of keys is sparse, reads may scan over several key-value pairs on disk anyway, which can be compressed. This reduces disk space and I/O bandwidth use.\\n\\n###### Constructing and maintaining SSTables\\n\\n* Writes update a *memtable*, or an in-memory balanced tree. When its size exceeds some threshold, it can be written to disk as an SSTable file.\\n* Read requests first query the memtable and then query on-disk segments in order from newest to oldest.\\n* We can append every update to a log before updating the memtable. The log is not in sorted order, but can be used to restore the memtable after a crash.\\n\\n###### Making an LSM-tree out of SSTables\\n\\n* Storage engines based on the principle of merging and compacting sorted files are called LSM (Log-Structured Merge) storage engines.\\n\\n###### Performance optimizations\\n\\n* An LSM storage engine can use bloom filters to avoid querying the memtable and all segments for a key that does not exist.\\n* Since data is stored in sorted order, you can efficiently perform range queries, and because disk writes are essential the LSM-tree can support remarkably high write throughput.\\n\\n##### B-Trees\\n\\n* B-trees remain the standard index implementation in almost all relational databases, and many non-relational databases use them too.\\n* B-trees break down the database into fixed-size *blocks* or *pages* (usually 4KB) and read or write one page at a time. This maps closely to the underlying disk hardware.\\n* The number of references to child pages in one page of the B-tree is called its *branching factor*, and is typically several hundred.\\n\\n###### Making B-trees reliable\\n\\n* To make the database resilient to crashes, many B-tree implementations also persist to an append-only log called the *write-ahead log* (WAL, or *redo log*).\\n* When the database comes back up after a crash, the write-ahead log is used to restore the B-tree back to a consistent state.\\n\\n###### B-tree optimizations\\n\\n* In pages in the interior of a tree, we do not need to store full keys, but only enough information to act as boundaries between key ranges.\\n* B-trees try to ensure leaf pages are in sequential order on disk. This is easier for LSM-trees, which rewrite large segments of the storage in one go during merging.\\n\\n##### Comparing B-Trees and LSM-Trees\\n\\n* LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads.\\n\\n###### Advantages of LSM-trees\\n\\n* *Write amplification* is where one write results in multiple writes over the database\\'s lifetime, such as when compacting and merging SSTables.\\n* Write amplification is of concern with SSDs, which can only overwrite blocks a limited number of times before wearing out.\\n* LSM-trees typically sustain higher write throughput than B-trees because they sometimes have lower write-amplification, and because they sequentially write data.\\n\\n###### Downsides of LSM-trees\\n\\n* The bigger the database gets, the more disk bandwidth is required for compaction. But this bandwidth is limited and can increase response times at higher percentiles.\\n* Relational databases implement transaction isolation using locks on ranges of keys. Because each key exists at exactly one place in a B-tree index, those locks can be attached directly to the tree.\\n\\n##### Other Indexing Structures\\n\\n###### Storing values within the index\\n\\n* With secondary indexes, each index references a location in the *heap file* that belongs to the corresponding row.\\n* A *clustered index* stores the indexed row directly within an index, to avoid the performance penalty upon reads of hopping from the index to the heap file.\\n* A compromise between a clustered index and a non-clustered index is a *covering index*, which stores *some* of the table\\'s columns within the index (the index *covers* the query).\\n\\n###### Multi-column indexes\\n\\n* Multi-dimensional indexes allow querying several columns at once, which is important for geospatial data. But more commonly indexes like R-trees are used.\\n\\n###### Full-text search and fuzzy indexes\\n\\n* In Lucene, the in-memory index is a finite state automaton over the characters in the keys, similar to a trie.\\n* This automaton can be transformed into a *Levenshtein automaton*, which supports efficient search for words within a given edit distance.\\n\\n###### Keeping everything in memory\\n\\n* In-memory databases are faster because they avoid the overheads of encoding in-memory data structures in a form that can be written to disk.\\n* In-memory databases are not faster because they don\\'t need to read from disk ‚Äì the operating system caches recently used disk blocks in memory anyway.\\n\\n#### Transaction Processing or Analytics?\\n\\n* *Transaction processing* just means allowing clients to make low-latency reads and writes, as opposed to *batch-processing* jobs which only run periodically.\\n* OLTP, or *online transaction processing*, is the access pattern for \"interactive applications\" driven by the user\\'s input.\\n* OLAP, or *online analytic processing*, scans over a huge number of records, reading only a few columns per record, and computes aggregates (e.g. sum, count, or average).\\n* The separate database on which OLAP queries are run is called the *data warehouse*.\\n\\n##### Data Warehousing\\n\\n* *Extract-Transform-Load* or ETL is the process of extracting data from the OLTP database, transforming it into an analysis-friendly schema, and loading it into the data warehouse.\\n\\n###### The divergence between OLTP databases and data warehouses\\n\\n* OLTP and OLAP are increasingly becoming two separate storage and query engines, which happen to be accessible through a common SQL interface.\\n\\n##### Stars and Snowflakes: Schemas for Analytics\\n\\n* Many data warehouses are used in a formulaic style known as *star schema* (also known as *dimensional modeling*).\\n* The center of the schema is a *fact table* where each row represents an event that occurred at a particular time.\\n* Columns in the fact table are attributes or foreign keys into *dimension tables*, which represent the\\t*who*, *what*, *where*, *when*, *why*, and *how* of the event.\\n* The name *star schema* comes from visualizing the fact table in the middle, connected to its surrounding dimension tables like rays in a star.\\n* A variation of this is the *snowflake schema*, where dimensions are further broken down into subdimensions.\\n* In a typical data warehouse, tables are often very wide: fact tables have over 100 columns, and sometimes several hundred.\\n\\n#### Column-Oriented Storage\\n\\n* Although fact tables are often over 100 columns wide, a typical data warehouse query only accesses 4 or 5 of them at one time.\\n* *Column-oriented storage* stores all the values from each column together, instead of all the values from each row together.\\n* The column-oriented storage layout relies on each column file containing the rows in the same order.\\n\\n##### Column Compression\\n\\n* We can convert a column with *n* distinct values into n separate bitmaps, with one bitmap for each distinct value, and one bit for each row.\\n* If *n* is large, then the bitmaps will be sparse, and the bitmaps can additionally be run-length encoded to make them very compact.\\n* An `IN` SQL query can be implemented by a bitwise *OR* of bitmaps, while an `AND` SQL query can be implemented by a bitwise *AND* of bitmaps.\\n\\n###### Memory bandwidth and vectorized processing\\n\\n* *Vectorized processing* is where operators like bitwise *AND* and *OR* operate on chunks of compressed column data directly.\\n\\n##### Sort Order in Column Storage\\n\\n* In column store, the ordering of rows is irrelevant, and so inserting a new row requires simply appending to each of the column files.\\n* If after choosing an order, the primary sort column does not have many distinct values, then a simple run-length encoding can compress it efficiently.\\n* Run-length encoding compresses the first sort key best, as subsequent sort keys will not have such long runs of repeated values.\\n\\n###### Several different sort orders\\n\\n* In a column store, there normally aren\\'t pointers to data elsewhere, only columns containing values.\\n\\n##### Writing to Column-Oriented Storage\\n\\n* Unlike B-trees, which use an update-in-place approach, LSM-trees allow inserting a row with compressed columns without rewriting all the column files.\\n* When using an LSM-tree, it doesn\\'t matter whether the in-memory store is row-oriented or column-oriented.\\n\\n##### Aggregation: Data Cubes and Materialized Views\\n\\n* A materialized view is an actual copy of the query results written to disk, while a virtual view is just a shortcut for writing queries.\\n* A *data cube* or *OLAP cube* is a materialized view. It is a grid of aggregates grouped by different dimensions.\\n* The advantage of a materialized data cube is that certain queries become very fast because they have effectively been precomputed.\\n* The disadvantage is that a data cube doesn\\'t have the same flexibility as querying the raw data.\\n\\n#### Summary\\n\\n* Analytic workloads require sequentially scanning across a large number of rows, and so indexes are much less relevant.\\n* Instead it is more important to encode data compactly, in order to minimize the amount of data that the query needs to read from disk.\\n\\n### Chapter 4: Encoding and Evolution\\n\\n* Backward compatibility means newer code can read data written by older code. Forward compatibility means older code can read data written by newer code.\\n* Forward compatibility is trickier because it requires older code to ignore additions made by a newer version of the code.\\n\\n#### Formats for Encoding Data\\n\\n##### Language-Specific Formats\\n\\n* Using a language\\'s built-in encoding functionality is bad from interoperability, security, efficiency, and versioning perspectives.\\n\\n##### JSON, XML, and Binary Variants\\n\\n* JSON distinguishes strings and numbers, but it doesn\\'t distinguish integers and floating-point numbers, and it doesn\\'t specify a precision.\\n* Base64-encoding a binary string to use it in XML or JSON increases the data size by 33%.\\n* Simply having different organizations agree on a single format is far more important than the aesthetics or efficiency of a format.\\n\\n###### Binary encoding\\n\\n* Because JSON and XML don\\'t prescribe a schema, they need to include all the object field names within the encoded data.\\n* It\\'s not clear whether the small space reduction by MessagePack, a binary encoding for JSON, is worth the loss of human-readability.\\n\\n##### Thrift and Protocol Buffers\\n\\n* Confusingly, Thrift has two binary encoding formats, called *BinaryProtocol* and *CompactProtocol*.\\n* Field tags are like aliases for fields ‚Äì\\xa0they are a compact way of saying what field we\\'re talking about without spelling out its name.\\n* Variable length integers use the top bit of each byte to specify whether there are still more bytes to come.\\n* With variable length integers, the numbers -64 to 63 are encoded in one byte, -8192 to 8191 are encoded in two bytes, etc. Bigger numbers use more bytes.\\n* The `required` keyword of Protocol Buffers enables a runtime check that fails if the field is not set, which is useful for catching bugs.\\n\\n###### Field tags and schema evolution\\n\\n* With Thrift and Protocol Buffers, you can change a field name, but changing its tag renders all existing encoded data invalid.\\n* If old code reads data written by new code, including a new field with an unrecognized tag, it can simply ignore that field. This maintains forward compatibility.\\n\\n###### Datatypes and schema evolution\\n\\n* With Protocol Buffers, it\\'s okay to change an `optional` (single-valued) field into a `repeated` (multi-valued) field.\\n\\n##### Avro\\n\\n* Avro has two schema languages: Avro IDL is intended for human editing, and a JSON equivalent that is more easily machine-readable.\\n* An encoded byte string specifies nothing to identify fields or their data types. It simply consists of values concatenated together.\\n* Consequently the binary data can only be decoded correctly if the code reading the data is using the *exact same schema* as the code that wrote the data.\\n\\n###### The writer\\'s schema and the reader\\'s schema\\n\\n* While decoding data, Avro resolves the differences between the *writer\\'s schema* and the *reader\\'s schema*, and translates read data from the former to the latter. This enables schema evolution.\\n\\n###### Schema evolution rules\\n\\n* Adding a field that has no default value breaks backward compatibility. Removing a field that has no default value breaks forward compatibility.\\n* Avro doesn\\'t have `optional` or `required` keywords like Thrift and Protocol buffers, but instead allows defining union types with `null` as a value.\\n\\n###### Dynamically generated schemas\\n\\n* Because an Avro schema doesn\\'t define tag numbers, it is friendlier to dynamically generated schemas, e.g. creating an Avro schema from a relational database schema.\\n\\n##### The Merits of Schemas\\n\\n* Database vendors provide a driver (e.g. using the ODBC or JDBC APIs) that decode responses from a database\\'s network protocol into in-memory data structures.\\n* Schema evolution allows the same flexibility as schemaless/schema-on-read JSON databases, while also providing better guarantees about your data and better tooling.\\n\\n#### Modes of Dataflow\\n\\n##### Dataflow through Databases\\n\\n* In an environment where the application is changing, it\\'s likely that some processes accessing the database will be running newer code and some will be running older code.\\n* Consequently a value in the database might be written by a newer version of the code, and then later read by an older version. And so forward compatibility is required.\\n* If an application decodes a database value into model objects, and then later re-encodes those model objects, the unknown fields may be lost in that translation process.\\n\\n###### Different values written at different times\\n\\n* Data outlives code: While deploys may replace older code with newer code within minutes, data in the database may have last been encoded and written years ago.\\n* Rewriting (migrating) data into a new schema is possible, but it is expensive on a large data set, and so most databases avoid it if possible.\\n* Schema evolution allows a database to appear as if it was encoded with a single schema, although the underlying storage may contain records encoded with various historical schema versions.\\n\\n##### Dataflow Through Services: REST and RPC\\n\\n* The application-specific APIs of services provide encapsulation, by restricting what clients can and cannot do.\\n* A key design goal of service-oriented/microservices architecture is to make the application easier to change and maintain by making services independently deployable and evolvable.\\n\\n###### Web services\\n\\n* The two popular approaches to web services are REST and SOAP.\\n* REST emphasizes data formats, using URLs for identifying resources and using HTTP features for cache-control, authentication, and content-type negotiation.\\n* SOAP is an XML protocol for API requests, and comes with a sprawling set of related standards (the *web standards framework*, or *WS-*) that add various features.\\n* The API of a SOAP web service is described using an XML-based language called the Web Services Descriptive Language, or WSDL. WSDL enables code generation so that a client can access a remote service using local classes and method calls.\\n* WSDL is not designed to be human-readable, and SOAP messages are often too complex to construct manually, so tooling, code generation, and IDEs fill in the gaps.\\n* Despite the ostensible standards, interoperability between different vendors\\' implementations often causes problems in practice.\\n\\n###### The problems with remote procedure calls (RPCs)\\n\\n* The RPC model tries to make a request to a remote service look the same as calling a method within the same process ‚Äì\\xa0an abstraction called *location transparency*.\\n* Variable latency, timeouts, retries and idempotence semantics, and serialization all mean there\\'s no point in location transparency. Calling a remote service is fundamentally different.\\n\\n###### Current directions for RPC\\n\\n* The new generation of RPC frameworks is more explicit about the fact that a remote request is different from a local function call.\\n* RESTful APIs have the advantage of being good for experimentation and debugging, support by all mainstream programming languages and platforms, and a vast ecosystem of available tools.\\n* For these reasons, REST is used for public APIs, while RPC frameworks mostly focus on requests between services owned by the same organization.\\n\\n###### Data encoding and evolution for RPC\\n\\n* Between services using RPC, we assume servers will be updated first, and clients second. You therefore need backward compatibility on requests, and forward compatibility on responses.\\n* The backward and forward compatibility properties of an RPC scheme are inherited from whatever encoding it uses, such as Thrift or gRPC.\\n\\n##### Message-Passing Dataflow\\n\\n* In *asynchronous message-passing* systems, messages are delivered to an intermediary called a *message broker* which stores the message temporarily.\\n* Message brokers can act as a buffer if the recipient is unavailable or overloaded, redeliver messages to crashed processes, deliver messages to multiple recipients, and decouple producers and consumers.\\n\\n###### Message brokers\\n\\n* A process sends a message to a named *queue* or *topic*, and the broker ensures delivery of the message to one or more *consumers* or *subscribers* of that queue or topic.\\n\\n###### Distributed actor frameworks\\n\\n* In the *actor model*, logic is encapsulated by actors that communicate via asynchronous messages, where delivery of each message is not guaranteed (even within the same process).\\n* In a *distributed actor framework*, location transparency works better than with RPC, because the actor model already assumes that messages may be lost.\\n* A distributed actor framework essentially integrates a message broker and the actor programming model into a single framework.\\n\\n### Chapter 5: Replication\\n\\n* Reasons to replicate data include reducing access latency by moving data geographically close to users, increasing availability, and increasing read throughput.\\n* All the difficulty in replication lies in handling *changes* to replicated data.\\n\\n#### Leaders and Followers\\n\\n* *Leader-based replication*, or *active/passive replication*, is the most common solution to ensuring that data is persisted to all replicas.\\n* Whenever the leader writes new data to its local storage, it also sends the data change to each of its followers via a *replication log* or *change stream*.\\n\\n##### Synchronous Versus Asynchronous Replication\\n\\n* With *synchronous* replication, the leader waits until a follower has confirmed it received a write before reporting success to the user and making it visible to other clients.\\n* With *asynchronous* replication, the leader sends the write to a follower, but doesn\\'t wait for a response.\\n* If a synchronous follower does not respond because of a crash, network fault, etc. then no writes can be processed, and so fully synchronous replication is impractical.\\n* In practice replication may be *semi-synchronous*, where one of the followers is synchronous and the rest are asynchronous.\\n* Leader-based replication is often completely asynchronous:\\n  * As an advantage, if the leader fails and is not recoverable, any writes that have not yet been replicated are lost.\\n  * As a disadvantage, the leader can continue processing writes, even if all of its followers have fallen behind.\\n\\n##### Setting Up New Followers\\n\\n* A new follower must ingest a consistent snapshot of the leader\\'s database, and then consume all updates from the replication log since that time.\\n* A position in the leader\\'s replication log is called the *log sequence number* in PostgreSQL, and the *binlog coordinates* in MySQL.\\n\\n##### Handling Node Outages\\n\\n###### Follower failure: Catch-up recovery\\n\\n* The follower can connect to the leader and, via the replication log, consume all data changes that occurred while the follower was disconnected.\\n\\n###### Leader failure: Failover\\n\\n* A *failover* requires promoting one follower as the new leader, reconfiguring clients to send data to the new leader, and reconfiguring other followers to consume data changes from the new leader.\\n* Most systems simply rely on a timeout to determine that the leader has failed.\\n* The best candidate for leadership is usually the replica with the most up-to-date changes from the old leader, to minimize any data loss.\\n* If asynchronous replication is used, the new leader may not have received all writes from the old leader before it failed. If the former leader later rejoins the cluster after the new leader has processed new writes, it usually discards the un-replicated writes.\\n* In a *split brain*, two nodes both believe they are the leader. If there is no process for resolving conflicting writes, this leads to data loss or corruption.\\n* Because failovers are fraught with things that can go wrong, some operations teams prefer to perform them manually.\\n\\n##### Implementation of Replication Logs\\n\\n###### Statement-based replication\\n\\n* Replicating every write request (statement) from a leader to its followers has many edge cases (e.g. non-determinism from `NOW()` or `RAND()`) and is not preferred.\\n\\n###### Write-ahead log (WAL) shipping\\n\\n* The WAL details which bytes were changed in which blocks. It\\'s thus a poor choice for a replication log, e.g. you cannot run different versions of the database on leaders and followers.\\n\\n###### Logical (row-based) log replication\\n\\n* A logical log is a sequence of records describing writes to database tables with row granularity. This is the approach MySQL\\'s binlog uses.\\n* Logical logs are decoupled from storage engine internals and therefore backward compatible, and are also easier for external applications to parse.\\n\\n###### Trigger-based replication\\n\\n* A trigger can log data changes into a separate table for reading by an external process.\\n* Trigger-based replication has greater overhead than other replication methods and is more error-prone, but offers increased flexibility.\\n\\n\\n#### Problems with Replication Lag\\n\\n* If an application reads from an asynchronous follower, it may see outdated information if the follower has fallen behind. This effect is known as *eventual consistency*.\\n* The *replication lag* to a follower may usually be only a fraction of a second, but given high load or network problems, it can increase to several minutes.\\n\\n##### Reading Your Own Writes\\n\\n* *Read-your-writes* consistency is also known as *read-after-write* consistency.\\n* If data is accessed from multiple devices, you may want to provide a stronger guarantee of *cross-device* read-after-write consistency.\\n* If your replicas are distributed across different data centers, there is no guarantee that connections from different devices will be routed to the same data center.\\n\\n##### Monotonic Reads\\n\\n* If the first query by a user goes to an up-to-date replica, but the second query goes to a stale replica, then the user may observe data *moving backward in time*.\\n* *Monotonic reads* ensures this anomaly does not happen. It is a weaker guarantee than strong consistency, but a stronger guarantee than eventual consistency.\\n* One way of achieving monotonic reads is to ensure that each user always queries data from the same replica, e.g. using the hash of the user ID.\\n\\n##### Consistent Prefix Reads\\n\\n* *Consistent prefix reads* guarantees that if a sequence of writes happen in a certain order, then anyone reading those writes will see them appear in the same order.\\n* In a sharded database, partitions operate independently, and so there is no global ordering of writes. A user\\'s query might return some parts of the database in an older state, and some in a newer state.\\n\\n##### Solutions for Replication Lag\\n\\n* When working with an eventually consistent system, it\\'s worth thinking about how the application behaves if replication lag increases to several minutes or hours.\\n* Pretending that replication is synchronous when it is in fact asynchronous is a recipe for problems later.\\n\\n#### Multi-Leader Replication\\n\\n* In a *multi-leader* configuration, or *active-active replication*, there are multiple leaders accepting writes, with each acting as a follower to the other leaders.\\n\\n##### Use Cases for Multi-Leader Replication\\n\\n###### Multi-datacenter operation\\n\\n* You can have a leader in each data center. Within a datacenter, regular leader-follower replication is used; between datacenters, leaders replicate changes to each other.\\n* With multiple datacenters, the inter-datacenter network delay is hidden from users, which means the perceived performance may be better.\\n* The biggest downside of multi-leader replication is the need to resolve write conflicts if the same data is concurrently modified in two different datacenters.\\n* Auto-incrementing keys, triggers, and integrity constraints are problematic, and so multi-leader replication is considered dangerous territory to be avoided if possible.\\n\\n###### Clients with offline operation\\n\\n* Clients with offline operation are like multi-leader replication between datacenters, but each device is a \"datacenter\" and network connections between them are highly unreliable.\\n\\n##### Handling Write Conflicts\\n\\n###### Synchronous versus asynchronous conflict detection\\n\\n* With synchronous conflict detection, you lose the advantage of each replica accepting writes independently, and so you might as well just use single-leader replication.\\n\\n###### Converging toward a consistent state\\n\\n* In a multi-leader configuration there is no defined ordering of writes. But the database must resolve each conflict in a *convergent way*, yielding a single final value across all replicas.\\n* Approaches to achieving convergent conflict resolution include:\\n  * *Last write wins* (LWW): Append the timestamp to each write, and apply the write with the largest value, discarding the others. This is dangerously prone to data loss.\\n  * Somehow merge the values together.\\n  * Record the conflict in an explicit data structure that preserves all information, and write application code to resolve the conflict later.\\n\\n###### Custom conflict resolution logic\\n\\n* *Conflict-free replicated data types* (CDRTs) are data structures (e.g. sets, maps, ordered lists, etc) that automatically resolve conflicts in sensible ways.\\n* *Mergeable persistent data structures* track history explicitly and use a three-way merge function (similar to Git).\\n* *Operational transformations* are for concurrent editing of an ordered list of items, and are used by Etherpad and Google Docs.\\n\\n##### Multi-Leader Replication Topologies\\n\\n* The most general topology is all-to-all in which every leader sends its writes to every other leader.\\n* The fault tolerance of a more densely connected topology is better because it allows messages to travel along different paths, avoiding a single point of failure.\\n* One problem with all-to-all topologies is that if some network links are faster than others, some messages can overtake others, e.g. an update arriving before the preceding insert.\\n\\n#### Leaderless Replication\\n\\n* Amazon\\'s Dynamo has popularized abandoning the concept of a leader and allowing any replica to directly accept writes from clients.\\n\\n##### Writing to the Database When a Node is Down\\n\\n* Writes are sent to all nodes in parallel. To account for a node being down and missing a write, clients read from nodes in parallel, and rely on version numbers to retain the latest value.\\n\\n###### Read repair and anti-entropy\\n\\n* With *read repair*, a client reads from several nodes in parallel and then writes the latest value back to any replicas from which it received stale data. This works well for values that are frequently read.\\n* An *anti-entropy process* runs in the background, comparing data between replicas and fixing stale data. This does not copy writes in any particular order and may be slow to fix differences.\\n\\n###### Quorums for reading and writing\\n\\n* If there are *n* replicas, every write must be confirmed by *w* nodes to be considered successful, and we must query at least *r* nodes for each read.\\n* As long as *w + r > n*, we expect to get an up-to-date value when reading, because at least one of the *r* nodes we\\'re reading from must be up-to-date.\\n* Reads and writes that obey these *r* and *w* parameters are called *quorum* reads and writes.\\n* Normally reads and writes are always sent to all *n* nodes in parallel, and *r* and *w* determine how many nodes we wait for.\\n\\n##### Limitations of Quorum Consistency\\n\\n* Often *r* and *w* are chosen to be more than *n/2* nodes, because that ensures *w + r > n* while tolerating up to *n/2* (rounded down) node failures.\\n* If you choose *w + r ‚â§ n* you are more likely to read stale values, but this configuration allows for lower latency and higher availability.\\n* Even with *w + r > n* there are edge cases where stale values are returned:\\n  * If two writes occur concurrently, it is not clear which one happened first. You must merge the concurrent writes.\\n  * If a write happens concurrently with a read, the write may be reflected only on some of the replicas.\\n  * If a write failed on some nodes and overall succeeded on fewer than *w* replicas, it is not rolled back on the replicas where it succeeded.\\n\\n###### Monitoring staleness\\n\\n* In leader-based replication, the replication lag is computed by subtracting the follower\\'s replication log position from that of the leader.\\n* In systems with leaderless replication, there is no fixed order in which writes are applied, which makes monitoring difficult.\\n\\n##### Sloppy Quorums and Hinted Handoff\\n\\n* Network interruptions may cut off a client from a large number of database nodes, such that a quorum for reads and writes cannot be reached.\\n* In a *sloppy quorum*, writes and reads still require *w* and *r* successful responses, but those may include nodes not among the *n* \"home\" nodes for a value.\\n* *Hinted handoff* is when, upon the network interruption being fixed, any writes that nodes temporarily accepted are sent to the appropriate \"home\" nodes.\\n* Even when *w + r > n*, a client cannot be sure it read the latest value for a key, because the latest value may be temporarily written to nodes outside of *n*.\\n\\n##### Detecting Concurrent Writes\\n\\n* When concurrently writing to a Dynamo-style database with the same key, events may arrive in a different order at different nodes due to variable network delays and partial failures.\\n\\n###### Last write wins (discarding concurrent writes)\\n\\n* If we have some unambiguous way to determine which write is more \"recent,\" and if every write is copied to every replica, then replicas will eventually converge to the same value.\\n* Concurrent writes do not have a natural ordering, and so *last write wins* (LWW) imposes an ordering by associating each write with a timestamp.\\n* LWW trades durability for convergence: Several concurrent writes to the same key may be reported as successful to the client, but only one of the writes will survive.\\n* If losing data is not acceptable, then LWW is a poor choice for conflict resolution.\\n\\n###### The \"happens-before\" relationship and concurrency\\n\\n* An operation A *happens before* another operation B if B knows about A, or depends on A, or builds on A in some way.\\n* We can simply say that two operations are *concurrent* if neither happens before the other (i.e. neither knows about the other).\\n* Exact time does not matter: If the network is slow, two operations can occur some time apart but still appear to be concurrent, because the network prevented one operation from being able to know about the other.\\n\\n###### Capturing the happens-before relationship\\n\\n* A server can determine whether two operations are concurrent by looking at version numbers\\xa0‚Äì\\xa0it does not need to interpret the value itself.\\n* When the server receives a write with a particular version number:\\n  * It can overwrite all values with that version number or below, since the client must have merged them into the new value.\\n  * It must keep all values with a higher version number, because those values are concurrent with the incoming write.\\n\\n###### Merging concurrently written values\\n\\n* If several operations happen concurrently, clients must clean up afterward by merging the concurrently written *sibling* values.\\n* Merging sibling values is essentially the same problem as conflict resolution in multi-leader replication.\\n* Merging sibling values is complex and error prone, while CRDTs can automatically merge siblings in sensible ways.\\n\\n###### Version vectors\\n\\n* With multiple replicas, we need to use a version number *per replica* as well as per key, so that we know which values to overwrite and which to preserve as siblings.\\n* The collection of version numbers from all the replicas is called a *version vector*. It is also sometimes called a *vector clock*, even though they are not the same.\\n\\n### Chapter 6: Partitioning\\n\\n* Partitioning enables scalability: By distributing data across many disks, the query load can be distributed across many processors.\\n\\n#### Partitioning and Replication\\n\\n* Partitioning is usually combined with replication so that copies of each partition are stored on multiple nodes for fault tolerance.\\n\\n#### Partitioning of Key-Value Data\\n\\n* If some partitions have more data or queries than others, then the partitioning is *skewed* and is less effective.\\n* A partition with a disproportionately high load is called a *hot spot*.\\n\\n##### Partitioning by Key Range\\n\\n* When assigning a contiguous range of keys to each partition, the ranges are not necessarily evenly spaced, because your data may not be evenly distributed.\\n* Certain access patterns can lead to hot spots with key range partitioning, e.g. partitioning by timestamp and then always writing to the partition for today.\\n\\n##### Partitioning by Hash of Key\\n\\n* When using a hash function to determine the partition for a given key, the hash function does not need to be cryptographically strong.\\n* Each partition is responsible for a range of hashes of keys. Assuming the hash function is uniform, the partition boundaries can be evenly spaced.\\n* Keys that were once adjacent are now scattered across all the partitions, so their sort order is lost.\\n* In Cassandra, the first part of the key is hashed to determine the partition, while the remaining columns are used as a concatenated index into Cassandra\\'s SSTables.\\n* Cassandra achieves a compromise between the partitioning strategies: If a query specifies a fixed value for the first column, it can perform an efficient range scan over the other columns.\\n\\n##### Skewed Workloads and Relieving Hot Spots\\n\\n* Most systems are not able to automatically compensate for a highly skewed workload, and so it\\'s the responsibility of the application reduce the skew.\\n\\n#### Partitioning and Secondary Indexes\\n\\n* The problem with secondary indexes is that they don\\'t map neatly to partitions.\\n\\n##### Partitioning Secondary Indexes by Document\\n\\n* Each partition can maintain its own secondary indexes, covering only the documents in the partition. Such a document-partitioned index is called a *local index*.\\n* Querying such a partitioned database requires a *scatter/gather* operation. Even if querying the partitions in parallel, scatter/gather is prone to tail latency amplification.\\n\\n##### Partitioning Secondary Indexes by Term\\n\\n* Rather than each partition having its own secondary index, we can construct a *global index* that covers data in all partitions.\\n* Such an index is *term-partitioned* because the term you\\'re looking for determines the partition of the index.\\n* A client needs to query only the partition containing the term it wants, but a write to a single document may update multiple terms and in turn update multiple partitions of the index.\\n* In practice, updates to global secondary indexes are often asynchronous.\\n\\n#### Rebalancing Partitions\\n\\n* The process of moving load from one node to another is called *rebalancing*.\\n\\n##### Strategies for Rebalancing\\n\\n###### How not to do it: hash mod N\\n\\n* The problem with the *mod N* approach is that if the number of nodes *N* changes, then most keys will need to be moved from one node to another.\\n\\n###### Fixed number of partitions\\n\\n* We can create more partitions than nodes, and then assign multiple partitions to each node. As nodes are added or removed, this assignment changes.\\n* In this configuration, the number of partitions does not change, nor does the assignment of keys to partitions.\\n* By assigning more partitions to nodes that are more powerful, you can force these nodes to take a greater share of the load.\\n\\n###### Dynamic partitioning\\n\\n* With *dynamic partitioning*, partitions are split and merged dynamically based on their size, similar to interior nodes in a B-tree.\\n* An advantage of dynamic partitioning is that the number of partitions adapts to the total data volume.\\n* Dynamic partitioning can be used with both key range-partitioned data, as well as with hash-partitioned data.\\n\\n###### Partitioning proportionally to nodes\\n\\n* With dynamic partitioning, the number of partitions is proportional to the size of the data set. With a fixed number of partitions, the size of each partition is.\\n* A third option is a fixed number of partitions per node. Since a larger data volume generally requires a larger number of nodes to store, this also keeps the size of each partition stable.\\n\\n##### Operations: Automatic or Manual Rebalancing\\n\\n* If not done correctly, rebalancing can overload the network or the nodes and harm the performance of other requests while it is happening.\\n* If a node is overloaded, then automated rebalancing may lead other nodes to conclude the node is dead and move load away from it. This increases load on the other nodes and the network, potentially causing a cascading failure.\\n* While fully automated rebalancing can be convenient, it\\'s good to have a human in the loop to prevent operational surprises.\\n\\n#### Request Routing\\n\\n* *Service discovery* addresses which node a client should connect to, and is a critical requirement for software accessible over a network with high availability.\\n* Any component making the routing decision (a random node, a routing tier, or a client) must learn about changes in the assignment of partitions to nodes.\\n* Many distributed systems rely on a separate coordination service such as ZooKeeper to track this cluster metadata.\\n* Cassandra and Riak use a *gossip protocol* among the nodes to disseminate any changes in cluster state.\\n* When a random node or a routing tier makes the routing decision, clients can use DNS to find their IP addresses as their assignment changes slowly.\\n\\n### Chapter 7: Transactions\\n\\n* Transactions *simplify the programming model* for applications accessing a database.\\n* A *safety guarantee* is when the database prevents a potential error and concurrency issue, and so the application does not have to compensate for it.\\n* Sometimes there are advantages to weakening transaction guarantees or abandoning them entirely, e.g. for higher performance or higher availability.\\n\\n#### The Slippery Concept of a Transaction\\n\\n* Transactions are not the antithesis of stability. But like every other technical design choice, they come with trade-offs.\\n\\n##### The Meaning of ACID\\n\\n* In practice, one database\\'s implementation of *ACID* does not equal that of another. ACID has become mostly a marketing term.\\n\\n###### Atomicity\\n\\n* Atomicity is not about concurrency, but about ensuring that when a transaction is aborted, that no data was changed and the operation can be safely retried.\\n\\n###### Consistency\\n\\n* Consistency means that there exist invariants about your data, but it is the application\\'s responsibility to define its transactions so that they preserve consistency.\\n* Atomicity, isolation, and durability are properties of the database, whereas consistency is actually a property of the application.\\n\\n###### Isolation\\n\\n* Isolation means that concurrently executing transactions do not step on each other\\'s toes.\\n* Isolation is sometimes formalized as *serializability*, where each transaction can pretend like it\\'s the only transaction running, but this carries a large performance penalty.\\n\\n###### Durability\\n\\n* Durability means that once a transaction has committed successfully, its written data will not be forgotten, even if there is a hardware fault or the database crashes.\\n* To provide durability, a database must wait until writing to non-volatile storage or replication has completed before reporting a transaction as committed.\\n\\n##### Single-Object and Multi-Object Operations\\n\\n* Atomicity gives all-or-nothing guarantees when writing data in a transaction, and isolation ensures concurrently running transactions don\\'t interfere with each other.\\n\\n###### Single-object writes\\n\\n* For a single object in a storage engine, atomicity can be implemented using a log for crash recovery, and isolation can be implemented using a lock on each object.\\n* Compare-and-set and other single-object operations are not transactions, because they do not allow grouping multiple operations on multiple objects.\\n\\n###### The need for multi-object transactions\\n\\n* Use cases where writes to several different objects need to be coordinated include:\\n  * Inserting several records that refer to one another and ensuring that their foreign keys are valid.\\n  * Updating denormalized data across several documents together.\\n  * Updating secondary indexes to reflect a changed value.\\n\\n###### Handling errors and aborts\\n\\n* If the database is in danger of violating its guarantee of atomicity, isolation, or durability, then it would rather abandon the transaction than half-finish.\\n\\n#### Weak Isolation Levels\\n\\n* Concurrency issues happen when one transaction reads data that is concurrently modified by another, or when two transactions try to concurrently modify the same data.\\n* Databases try to hide concurrency issues from application developers by providing *transaction isolation*.\\n* *Serializable* isolation means the database guarantees that transactions have the same effect as if they ran *serially*.\\n\\n##### Read Committed\\n\\n* The most basic level of transaction isolation is *read committed*, which guarantees *no dirty reads* and *no dirty writes*.\\n\\n###### No dirty reads\\n\\n* *No dirty reads* means when reading from the database, you will only see data that has been committed.\\n* Any writes by a transaction only become visible to others when that transaction commits, and then all its writes become visible at once.\\n* No dirty reads means transactions won\\'t see data in a partially updated state, and that a transaction won\\'t see data that is later rolled back.\\n\\n###### No dirty writes\\n\\n* *No dirty writes* means when writing to the database, you will only overwrite data that has been committed.\\n* Instead of allowing overwriting of uncommitted data, databases usually delay the second write until the first write\\'s transaction has committed or aborted.\\n* If transactions update multiple objects, dirty writes allow interleaving the writes to the multiple objects.\\n\\n###### Implementing read committed\\n\\n* Read committed is the default setting in Oracle 11g, PostgreSQL, SQL Server 2012, MemSQL, and many other databases.\\n* To prevent dirty reads, a transaction must acquire a row-level lock before writing and then relinquish it upon committing or aborting.\\n* When a transaction updates a object, other transactions read the old value until the transaction commits, at which point they begin reading the updated value.\\n\\n##### Snapshot Isolation and Repeatable Read\\n\\n* *Read skew* is an example of a *non-repeatable read*, where a transaction querying an object at different times sees different values because of concurrent updates.\\n* *Read skew* is considered acceptable under read committed isolation, as the client is indeed reading only committed data.\\n* *Snapshot isolation* provides a consistent snapshot of the database, so the transaction sees all data that was committed at the start of the transaction.\\n* Snapshot isolation is a boon for long-running read-only queries such as backups and analytics.\\n* Snapshot isolation is supported by PostgreSQL, MySQL with the InnoDB storage engine, Oracle, SQL Server, and others.\\n\\n###### Implementing snapshot isolation\\n\\n* A key principle of snapshot isolation is that readers never block writers, and writers never block readers. Reads do not require any locks.\\n* With *multi-version concurrency control*, the database keeps several different versions of a committed object so that various in-progress transactions can see the database state at different points in time.\\n* When a transaction deletes a row, a garbage collector deletes it later, only when it is certain no transaction can any longer access the deleted data.\\n\\n###### Visibility rules for observing a consistent snapshot\\n\\n* When a transaction starts, the database enumerates all the IDs of in-progress transactions, and their writes are ignored even if they subsequently commit.\\n* Any writes made by transactions with a later transaction ID ‚Äì which are strictly increasing, and therefore started after the current transaction ‚Äì\\xa0are ignored.\\n* An object is visible if both of the following conditions are true:\\n  * When the reader\\'s transaction started, the transaction that created the object had already committed.\\n  * The object is not marked for deletion, or if it is, the transaction that requested deletion had not yet committed when the reader\\'s transaction started.\\n\\n###### Indexes and snapshot isolation\\n\\n* With a *copy-on-write* strategy for B-tree indexes, parent pages\\xa0‚Äì up to the root of the tree\\xa0‚Äì are copied and updated to point to the new versions of their child pages.\\n* With this strategy, any pages that are not affected by a write do not need to be copied, and remain immutable.\\n\\n###### Repeatable read and naming confusion\\n\\n* The SQL standard\\'s definition of isolation levels is flawed ‚Äì it is ambiguous, imprecise, and not an implementation-independent standard.\\n* No one really knows what repeatable read means: Databases implementing it provide very different guarantees, and most don\\'t satisfy a formal definition.\\n\\n##### Preventing Lost Updates\\n\\n* The *lost update* can occur if an application reads some value, modifies it, and writes back the modified value (a *read-modify-write* operation).\\n\\n###### Atomic write operations\\n\\n* Atomic operations are usually implemented by acquiring an exclusive lock on the object to update. This technique is known as *cursor stability*.\\n* Object-relational mapping frameworks make it easy to accidentally perform unsafe read-modify-write operations instead of relying on atomic operations provided by the database.\\n\\n###### Explicit locking\\n\\n* The `FOR UPDATE` clause in SQL indicates that the database should take a lock on all rows returned by the query.\\n\\n###### Automatically detecting lost updates\\n\\n* PostgreSQL\\'s repeatable read, Oracle\\'s serializable, and SQL Server\\'s snapshot isolation levels automatically detect a lost update and abort the corresponding transaction.\\n\\n###### Compare-and-set\\n\\n* An update statement with a `WHERE` clause specifying the old value may not stop a lost update if it reads from an old snapshot while a concurrent write occurs.\\n\\n###### Conflict resolution and replication\\n\\n* Locks and compare-and-set operations assume a single up-to-date copy of the data, but multi-leader or leaderless replication does not guarantee this.\\n* But atomic operations can work well in a replicated context, especially if they are commutative.\\n* While the *last write wins* (LWW) conflict resolution strategy is prone to lost updates, it is the default in many replicated databases.\\n\\n##### Write Skew and Phantoms\\n\\n* *Write skew* generalizes the lost update problem: Two transactions read the same objects, and then concurrently update some of those objects.\\n* In the special case where different transactions update the same object, you a lost update or a dirty write anomaly, depending on the timing.\\n* If you cannot use a serializable isolation level to prevent write skew, the second-best option is to explicitly lock rows that the transaction depends on.\\n\\n###### Phantoms causing write skew\\n\\n* In all cases of write skew, a `SELECT` query finds rows that match some condition, and then an `UPDATE` statement changes the set of rows matching that condition.\\n* If the `SELECT` query checks for the *absence* of rows matching a search condition, and the write adds a matching row, then `SELECT FOR UPDATE` returning no rows can\\'t attach locks to anything.\\n* A *phantom* is when a write in one transaction changes the result of a query in another transaction.\\n* Snapshot isolation avoids phantoms for read-only queries, but in read-write transactions, phantoms can lead to tricky cases of write skew.\\n\\n###### Materializing conflicts\\n\\n* *Materializing conflicts* takes a phantom and turns it into a lock conflict on a concrete set of rows that exist in the database.\\n\\n#### Serializability\\n\\n* Serializable isolation is the strongest isolation level, guaranteeing that transactions executing in parallel yield the same result as if they ran serially.\\n* This means that the database prevents *all* possible race conditions.\\n\\n##### Actual Serial Execution\\n\\n* Redis, VoltDB, and more use a single-threaded execution model with all data in RAM, which allows transactions to execute much faster than if they had to load data from disk.\\n\\n###### Encapsulating transactions in stored procedures\\n\\n* Almost all OLTP applications keep transactions short by avoiding interactively waiting for a user within a transaction, because we\\'re slow at making up our minds.\\n* Even after removing the user, an interactive style between the application and database would yield dreadful throughput in a database without concurrency.\\n* Instead, single-threaded transaction processing requires the application to submit the entire transaction code to the database as a *stored procedure*.\\n\\n###### Pros and cons of stored procedures\\n\\n* With stored procedures and in-memory data, executing all transactions on a single thread become feasible.\\n\\n###### Partitioning\\n\\n* For applications with high write throughput, the single-threaded transaction processor can become a serious bottleneck.\\n* If you can ensure that each transaction only reads and writes data within some partition, then each partition to have its own independent transaction processing thread.\\n* Since cross-partition transactions have additional coordination overhead, they are much slower than single-partition transactions.\\n\\n##### Two-Phase Locking (2PL)\\n\\n* Two-phase *locking* sounds very similar to two-phase *commit*, but they are completely different things.\\n* In 2PL, writers don\\'t just block other writers, but they also block readers. Additionally, readers can block writers.\\n\\n###### Implementation of two-phase locking\\n\\n* 2PL is used by the serializable isolation level in MySQL (InnoDB) and SQL Server, and is the repeatable read isolation level in DB2.\\n* Each object in the database has a lock that can be acquired in *shared mode* or *exclusive mode*:\\n  * Multiple readers can hold the lock in shared mode simultaneously, but they must wait for any transaction with an exclusive lock.\\n  * A writer must first acquire the lock in exclusive mode. A transaction that first reads and then writes can upgrade its shared lock to an exclusive lock.\\n* The \"two-phase\" name comes from a transaction acquiring locks in the first phase, and then releasing its locks in the second phase (at the end of the transaction).\\n* The database automatically detects deadlocks betweens transactions and aborts one so that the others can make progress.\\n\\n###### Performance of two-phase locking\\n\\n* Transaction throughput and query latency with 2PL is worse than under weak isolation because the locking and exclusive access reduces concurrency.\\n* Databases running 2PL have unstable latencies and can be very slow at high percentiles if there is contention in the workload.\\n* Deadlocks happen much more frequently with 2PL, and so clients can waste significant effort retrying transactions repeatedly.\\n\\n###### Predicate locks\\n\\n* To prevent phantoms, a *predicate lock* allows locking all objects that match some search condition.\\n* A predicate lock applies even to objects that don\\'t yet exist in the database, but which might be added in the future (phantoms).\\n\\n###### Index-range locks\\n\\n* Checking for matching locks is time consuming, so most databases with 2PL implement a simplified approximation of predicate locking called *index-range locking*.\\n* It\\'s safe to simplify a predicate by making it match a larger set of objects, as any write that matches the original predicate will also match the approximations.\\n* If there is no suitable index where a range lock can be attached, the database can fall back to a shared lock on the entire table.\\n\\n##### Serializable Snapshot Isolation (SSI)\\n\\n* *Serializable snapshot isolation* (SSI) provides full serializability, but has only a small performance penalty compared to snapshot isolation.\\n* SSI is the serializable isolation level in PostgreSQL since 9.1, and FoundationDB uses a similar algorithm.\\n\\n###### Pessimistic versus optimistic concurrency control\\n\\n* 2PL uses *pessimistic* concurrency control: if anything can go wrong (as indicated by another transaction holding a lock), then a transaction must wait.\\n* SSI uses *optimistic* concurrency control: a transaction will always forge ahead, but the database will abort it before committing if anything bad happened (i.e. isolation was violated).\\n* Given enough capacity, and if contention between transactions is not too high, optimistic concurrency control techniques perform better than pessimistic ones.\\n\\n###### Decisions based on an outdated premise\\n\\n* Under snapshot isolation, the result of a query before updating may no longer be up-to-date when the transaction commits, because it may have been concurrently modified.\\n* We say the transaction was based on a *premise*, or a fact that was true at the start of the transaction, but may no longer be true by the end.\\n* To provide serializable isolation, the database must detect when a transaction may have acted on an outdated premise and consequently abort it.\\n\\n###### Detecting stale MVCC reads\\n\\n* When a transaction wants to commit, the database checks whether other transactions have committed writes that it ignored earlier because of MVCC visibility rules. If so, the database aborts the transaction.\\n\\n###### Detecting writes that affect prior reads\\n\\n* When a transaction writes to the database, it notifies transactions that recently read the affected data that what they read may no longer be up to date.\\n\\n###### Performance of serializable snapshot isolation\\n\\n* Less detailed tracking of reads and writes is faster but less precise, and consequently may lead to more transactions being aborted than necessary.\\n* The big advantage of SSI over 2PL is that a transaction doesn\\'t need to block waiting for locks held by another transaction, thereby making query latency more predictable and less variable.\\n\\n#### Summary\\n\\n* Transactions are an abstraction layer allowing applications to pretend that certain concurrency problems and certain kinds of faults don\\'t exist.\\n* Only serializable isolation protects against all types of anomalies and race conditions in a database.\\n\\n### Chapter 8: The Trouble with Distributed Systems\\n\\n#### Faults and Partial Failures\\n\\n* An individual computer with good software is either fully functional or completely broken, but not something in between.\\n* If an internal fault occurs, we prefer to crash completely rather than returning a wrong result, which are confusing and difficult to debug.\\n* A *partial failure* in a distributed system is where some parts break in an unpredictable way, even though other parts of the system are working fine.\\n* The possibility of partial failures and their non-determinism is what makes distributed systems hard to work with.\\n\\n##### Cloud Computing and Supercomputing\\n\\n* In a system with thousands of nodes, it\\'s reasonable to assume that *something* is always broken.\\n* We must accept the possibility of partial failure and incorporate fault-tolerance, thereby building a reliable system from unreliable components.\\n* Suspicion, pessimism, and paranoia pay off in distributed systems, so consider and test for a wide range of possible faults in your environment.\\n\\n#### Unreliable Networks\\n\\n* We focus on *shard-nothing distributed systems*, which are a collection of machines connected by a network.\\n* In this kind of network, one node can send a message to another node, but the node makes no guarantees when it will arrive, or if it will arrive at all.\\n* If you send a request to another node and don\\'t receive a response, it is impossible to tell why.\\n* When a timeout occurs, you still don\\'t know whether the remote node got your request or not.\\n\\n##### Network Faults in Practice\\n\\n* A *network partition* or *netsplit* is when one part of the network is cut off from the rest due to a network fault.\\n* Whenever any communication happens over a network, it may fail ‚Äì there is no avoiding it.\\n\\n##### Detecting Faults\\n\\n* Many systems need to automatically detect faulty nodes, but the uncertainty about the network makes it difficult to tell whether a node is working or not.\\n* Even if TCP acknowledges that a packet was delivered, the application may have crashed before processing it. The client needs a positive response from the application itself to confirm its processing.\\n* Conversely, if something goes wrong, you have to assume that you will get no response at all.\\n\\n##### Timeouts and Unbounded Delays\\n\\n* Prematurely declaring a node as dead is problematic because if it\\'s actually alive and in the middle of performing some action, having another node take over may perform the action twice.\\n* If the system is struggling with high load, declaring nodes dead prematurely can transfer their load to other nodes, causing a cascading failure.\\n* Asynchronous networks have unbounded delays and most server implementations cannot guarantee handling a request within some maximum time.\\n\\n###### Network congestion and queueing\\n\\n* *Network congestion* is when a packet experiences delay because it is queued up for sending onto a busy network link.\\n* When a VM is paused while another VM runs in a virtualized environment, it cannot consume data from the network, and so any incoming data is queued by the virtual machine monitor.\\n* Because UDP does not perform control flow and does not retransmit lost packets, it is a better choice than TCP when delayed data is worthless.\\n* Queueing delays have an especially wide range when a system is close to its maximum capacity, as long queues build up quickly.\\n* In multi-tenant environments, a *noisy neighbor* can use a lot of the resources without your knowledge and introduce highly variable network delays.\\n* Rather than using fixed timeouts, systems can continually measure the distribution of response times and their variability and adjust their timeouts accordingly.\\n\\n##### Synchronous Versus Asynchronous Networks\\n\\n* A *synchronous* network with reserved resources for communication has no queueing, and so the maximum end-to-end latency is fixed. We call this a *bounded delay*.\\n\\n###### Can we not simply make network delays predictable?\\n\\n* Ethernet an IP are packet-switched protocols, which suffer from queueing and thus unbounded delays in the network.\\n* Protocols using packet switching is optimized for *bursty traffic*, and dynamically adapt the rate of data transfer to the available network capacity.\\n* With careful use of *quality of service* (prioritization of packets) and *admission control* (rate limiting senders) we can provide statistically bounded delay.\\n* There is no \"correct\" value for timeouts - they need to be determined experimentally.\\n\\n#### Unreliable Clocks\\n\\n* The quartz crystal oscillator powering a clock on a node is not perfectly accurate, and may be faster or slower than those of other machines.\\n\\n##### Monotonic Versus Time-of-Day Clocks\\n\\n* Modern computers have a *time-of-day clock* and a *monotonic clock*.\\n\\n###### Time-of-day clocks\\n\\n* A time-of-day clock returns the current date and time according to some calendar, also known as the *wall-clock time*.\\n* If such a clock is too-far ahead of the NTP server it\\'s synchronizing with, it can be forcibly reset and jump back to a previous time. This makes it unsuitable for measuring elapsed time.\\n\\n###### Monotonic clocks\\n\\n* The name *monotonic clock* comes from the fact that it is guaranteed to always move forward.\\n* The difference between two monotonic clock values tells you how much time has elapsed, but the *absolute* value of the clock is meaningless.\\n* NTP allows the monotonic clock to be sped up or slowed down by up to 0.05%, but it cannot cause the clock to jump forward or backward.\\n* The resolution of monotonic clocks is quite good, as they can measure time intervals in microseconds or less.\\n\\n##### Clock Synchronization and Accuracy\\n\\n* Google assumes a drift of 6 ms for a clock that is synchronized every 30 seconds, or 17 seconds for a server that is synchronized once per day.\\n* If a computer\\'s clock differs too much from an NTP server, it may refuse to synchronize, or the local clock will be forcibly reset.\\n* The best way for NTP servers to handle leap seconds is to perform the leap second adjustment gradually over the course of the day, a technique known as *smearing*.\\n* When a VM is paused so that another VM can run, the application experiences the pause as the clock suddenly jumping forward.\\n\\n##### Relying on Synchronized Clocks\\n\\n* Incorrect clocks easily go unnoticed. Any bug that results is more likely to be silent and subtle data loss than a dramatic crash.\\n* Any node who\\'s clock drifts too far from the others should be declared dead and removed from the cluster.\\n\\n###### Timestamps for ordering events\\n\\n* Fundamental problems with last write wins given inaccurate clocks include:\\n  * A node with a lagging clock cannot overwrite values previously written by a node with a fast clock until the clock skew between them has elapsed.\\n  * Two nodes can independently generate writes with the same timestamp, especially when the clock has only millisecond precision.\\n* NTP\\'s synchronization accuracy is limited by the network round-trip time, as well as quartz drift.\\n* *Logical clocks* are based on incrementing counters rather than oscillating quartz crystal and are a safer alternative for ordering events.\\n* *Physical clocks* are time-of-day and monotonic clocks, which measure actual elapsed time.\\n\\n###### Clock readings have a confidence interval\\n\\n* Given quartz drift, don\\'t think of a clock reading as a point in time, but as a range of times within a confidence interval.\\n* Google\\'s *TrueTime* API in Spanner explicitly returns a confidence interval with the earliest possible and latest possible timestamp.\\n\\n###### Synchronized clocks for global snapshots\\n\\n* The most common implementation of snapshot isolation requires a monotonically increasing transaction ID.\\n* With lots of small, rapid transactions, creating transaction IDs in a distributed system becomes an untenable bottleneck.\\n* If you have two non-overlapping TrueTime confidence intervals in Spanner, then one definitely happened before the other.\\n* To ensure transaction timestamps reflect reality, Spanner waits for the confidence interval length before committing a read-write transaction. This ensures that any transaction that may read the data has a non-overlapping confidence interval.\\n* Google deploys a GPS receiver or atomic clock in each data center to synchronize clocks within 7 ms.\\n\\n##### Process Pauses\\n\\n* Reasons for *preempting* a running thread for a long time include:\\n  * The programming language time might run garbage collection and \"stop the world.\"\\n  * The hypervisor can switch to a different virtual machine. The CPU time spent in other virtual machines is known as *steal time*.\\n  * Unexpected disk access, such as the Java class loader lazily loading class files on first use.\\n  * The server did not disable paging, and so a simple memory access results in a page fault that loads a page from disk into memory.\\n  * The Unix process being paused by a `SIGSTOP` signal, and so it yields all CPU cycles until it receives a `SIGCONT` signal.\\n* A node in a distributed system must assume its execution can be paused, during which the rest of the world keeps moving and may even declare the node dead.\\n\\n###### Response time guarantees\\n\\n* A *hard real-time* system is one where if the software does not respond by some *deadline*, the entire system may fail.\\n* In embedded systems, *real-time* means that a system is carefully designed and tested to meet specified timing guarantees in all circumstances.\\n* Real-time systems may have lower throughput, since they have to prioritize timely responses above all else.\\n\\n###### Limiting the impact of garbage collection\\n\\n* If the runtime can warn the application that a node soon requires a GC pause, it can shift traffic to other nodes while the garbage collection runs.\\n\\n#### Knowledge, Truth, and Lies\\n\\n* A node in the network cannot *know* anything for sure ‚Äì\\xa0it can only make guesses based on messages it does or does not receive via the network.\\n* In a distributed system, we can state the assumptions we are making about the behavior (the system model) and design the system in a way to meet those assumptions.\\n\\n##### The Truth is Defined by the Majority\\n\\n* Many distributed algorithms rely on a *quorum*, where decisions require some minimum number of votes from several nodes in order to reduce the dependence on any single node.\\n* Most commonly the quorum is an absolute majority of more than half the nodes, as you cannot have two such majorities with conflicting decisions.\\n\\n###### The leader and the clock\\n\\n* There can be problems if the majority of nodes have declared some node dead, but that node still believes it is the single node responsible for some thing.\\n\\n###### Fencing tokens\\n\\n* With *fencing*, when a lock server grants a lock or lease, it can also return a *fencing token*, or a number that increases every time.\\n* A resource protected by the lock service can check tokens and reject any requests with an older token than one that has already been processed.\\n* It\\'s unwise for a service to assume that clients are well behaved, as the people running the clients and those running the service likely have very different priorities.\\n\\n##### Byzantine Faults\\n\\n* A *Byzantine fault* is when there is the risk that nodes may \"lie,\" or send arbitrarily faulty or corrupted responses.\\n* The *Byzantine Generals Problem* imagines there are *n* generals who need to agree, but this is hampered by generals who are traitors.\\n* A system is *Byzantine fault-tolerant* if it operates correctly even if some nodes are not obeying the protocol, or if attackers are interfering with the network.\\n* In most server-side data systems, the cost of deploying Byzantine fault-tolerant solutions makes the impracticable.\\n\\n##### System Model and Reality\\n\\n* A *system model* is an abstraction that describes what things an algorithm may assume.\\n* There are three system models for timing assumptions:\\n  * A *synchronous model* assumes bounded network delay, bounded process pauses, and bounded clock error.\\n  * A *partially asynchronous model* mostly behaves like the synchronous model, but can sometimes exceed its bounds for network delay, process pauses, and clock drift.\\n  * An *asynchronous model* does not allow the model to make any timing assumptions, and in fact does not even have a clock.\\n* There are three system models for node failures:\\n  * A *crash-stop faults model* assumes a node can fail only by crashing, and once it has crashed it is gone forever.\\n  * A *crash-recovery faults model* assumes the node can crash but respond at some later time, and that stable storage is preserved while in-memory state is lost.\\n  * A *Byzantine faults model* assumes nodes can do absolutely anything.\\n* For modeling real systems, the most useful model is partially asynchronous with crash-recovery faults.\\n\\n###### Correctness of an algorithm\\n\\n* An algorithm is *correct* if it always satisfies its *properties* in all situations that we assume may occur in that system model.\\n\\n###### Safety and liveness\\n\\n* Safety is often informally defined as *nothing bad happens*, and liveness as *something good eventually happens*.\\n* If a safety property is violated, we can point to the exact time at which it was broken. At this time the damage is done, and cannot be undone.\\n* A liveness property may not hold at some point in time, but there is always hope that it may be satisfied in the future.\\n* For distributed algorithms, it\\'s common to require that safety properties *always* hold. Meaning even if all nodes crash, the algorithm must not return a wrong result.\\n* With liveness properties we can make caveats. The definition of the partially synchronous model requires that eventually the system returns to a synchronous state.\\n\\n###### Mapping system models to the real world\\n\\n* System models help distill the complexity of real systems to a set of faults that we can reason about, so that we can understand the problem and try to solve it systematically.\\n\\n#### Summary\\n\\n* If you can avoid opening Pandora\\'s box and simply keep things on a single machine, it\\'s generally worth doing so.\\n* Distributed sequence number generators like Twitter\\'s Snowflake cannot guarantee that ordering is consistent with causality, because the timescale at which blocks of IDs are assigned is longer than the timescale of database reads and writes.\\n\\n### Chapter 9: Consistency and Consensus\\n\\n* The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on them.\\n* *Split brain* is when nodes believe they are the leader, and it often leads to data loss.\\n\\n#### Consistency Guarantees\\n\\n* A better name for eventual consistency may be *convergence*, as we expect all replicas to eventually converge to the same value.\\n* The edge cases of eventual consistency only become apparent when there is a fault in the system or at high concurrency.\\n* Transaction isolation is about avoiding race conditions due to concurrently executing transactions. Distributed consistency is about coordinating the state of replicas in the face of delays and faults.\\n\\n#### Linearizability\\n\\n* *Linearizability* (or *strong consistency*) is to make a system appear as if there were only one copy of the data, and all operations on it are atomic.\\n* Linearizability is a *recency guarantee*: As soon as a client completes a write, all clients reading from the database must be able to see the value just written.\\n\\n##### What Makes a System Linearizable?\\n\\n* A linearizable system appears as if it has only a single copy of the data.\\n* A *concurrent read* is a read that overlaps in time with the write operation. Because we don\\'t know whether the write has taken effect when the read operation is processed, it may return either the old value or the new value.\\n* In a linearizable system there must be some point in time at which a register is updated to its new value, and so all subsequent reads must return the new value even if the write operation has not yet completed.\\n* It is possible to test whether a system is linearizable by recording the timings of all requests and responses, and checking whether they can be arranged in a valid sequential order.\\n* Serializability is a isolation property of transactions which guarantees that the transactions behave as if they had executed in some serial order.\\n* Implementations of serializability based on two-phase locking or actual serial execution are typically linearizable.\\n* Snapshot isolation is not linearizable because a consistent snapshot does not include writes that are more recent than the snapshot.\\n\\n##### Relying on Linearizability\\n\\n###### Locking and leader election\\n\\n* Electing a leader requires one node successfully acquiring a lock. This system must be linearizable, as all nodes must agree on which node owns the lock.\\n* Consensus algorithms implement linearizability in a fault-tolerant way, but linearizable storage is the foundation for these coordination tasks.\\n\\n###### Constraints and uniqueness guarantees\\n\\n* A hard uniqueness constraint requires linearizability, while constraints like foreign key or attribute constraints do not require linearizability.\\n\\n###### Cross-channel timing dependencies\\n\\n* Without the recency guarantees of linearizability, you must be concerned with race conditions between two communication channels.\\n\\n##### Implementing Linearizable Systems\\n\\n* The simplest way to implement linearizability is to rely on only a single copy of the data, but that does not tolerate faults.\\n* Single-leader replication is potentially linearizable if you make all reads from the leader\\xa0‚Äì which assumes you know for sure who the leader is ‚Äì or from synchronously updated followers.\\n* Multi-leader replication are generally not linearizable, because they concurrently process writes on multiple nodes and asynchronously replicate them to other nodes.\\n* Leaderless replication systems with LWW conflict resolution or sloppy quorums are not linearizable.\\n* Quorums where *w + r > n* can be linearizable if readers perform read repair synchronously before returning results, and if writers read the latest quorum state before sending writes.\\n  * Only linearizable read and write operations can be performed in this way; a linearizable compare-and-set operation cannot, because it requires a consensus algorithm.\\n\\n##### The Cost of Linearizability\\n\\n* The *CAP theorem* states that applications that don\\'t require linearizability can be more tolerant of failures:\\n  * If your application requires linearizability, then a disconnected replica is *unavailable* and must either wait until the network is fixed, or return an error.\\n  * If your application doesn\\'t require linearizability, then a replica can remain *available* and process requests even if disconnected.\\n* The CAP theorem encouraged database engineers to explore a wider design space of distributed shared-nothing systems, which were more suitable for implementing large-scale web services.\\n* CAP is sometimes presented as *Consistent, Available, Partitioned ‚Äì pick 2* but partitions are a kind of fault and so they will happen whether you like it or not.\\n* A better way of presenting CAP is *Consistent or Available when Partitioned*, as you must choose linearizability or total availability when a network fault occurs.\\n* CAP has little practical value for designing systems as it doesn\\'t say anything about network delays, dead nodes, or other trade-offs.\\n\\n###### Linearizability and network delays\\n\\n* The response times of linearizable reads and writes is very high in a network with highly variable network delays, and so weaker consistency models can be much faster.\\n\\n#### Ordering Guarantees\\n\\n* The leader in single-leader replication determines the *order of writes* in the replication log, while serializability ensures that transactions behave as if executed in *some sequential order*.\\n\\n##### Ordering and Causality\\n\\n* Ordering is recurring because it helps preserve *causality*. Cases where causality is important include:\\n  * Snapshot isolation provides a snapshot consistent with causality: The effects of all operations that happened causally before that point in time are visible, but no operations that happened causally afterward are seen.\\n  * Serializable snapshot isolation detects write skew by tracking the causal dependencies between transactions.\\n* A *causally consistent* system obeys the ordering imposed by causality, where chains of causally dependent operations define the causal order in the system.\\n\\n###### The causal order is not a total order\\n\\n* In a linearizable system, we have a *total order* of operations: For any two operations, we can always say which happened first.\\n* Other consistency models offer a *partial order*: Two events are ordered if they are causally related, but they are incomparable if they are concurrent.\\n* There are no concurrent operations in a linearizable datastore, as there must be a single timeline along which all operations are totally ordered.\\n* Concurrency means that the timeline branches and merges again, and that operations on different branches are incomparable (i.e. concurrent).\\n\\n###### Linearizability is stronger than causal consistency\\n\\n* Linearizability ensures causality, which is what makes linearizable systems simple to understand and appealing.\\n* Causal consistency is the strongest consistency model that does not slow down due to network delays, and remains available in the face of network failures.\\n\\n###### Capturing causal dependencies\\n\\n* Concurrent operations may be processed in any order, but if one operation happened before another, then they must be processed in that order on every replica.\\n* Causal consistency must track causal dependencies across the entire database. And to determine the causal ordering, the database must know which version of the data was read by the application.\\n\\n##### Sequence Number Ordering\\n\\n* Causality is an important theoretical concept, but actually keeping track of all causal dependencies can become impracticable.\\n* Monotonically increasing *sequence numbers* can create a total ordering that is consistent with causality: If operation A happened before operation B, then A occurs before B in the total order.\\n\\n###### Noncausal sequence number generators\\n\\n* Multiple leaders independently generating sequence numbers will not correctly capture the ordering of operations across different nodes, and are not consistent with causality.\\n\\n###### Lamport timestamps\\n\\n* If each node has a unique identifier and a count of the number of operations it has processed, its Lamport timestamp is the pair *(counter, node ID)*.\\n* Each node and client maintains the maximum counter value it has seen so far and includes it on every request. When a node receives a request or response with a maximum counter value greater than its own counter value, it immediately increases its own counter to that maximum.\\n* The ordering from the Lamport timestamps is consistent with causality, because every causal dependency results in an increased timestamp.\\n* Version vectors can distinguish whether two operations are concurrent or whether one is casually dependent on the other, whereas Lamport timestamps enforce a total ordering.\\n\\n###### Timestamp ordering is not sufficient\\n\\n* To implement something like a uniqueness constraint for usernames, it\\'s not sufficient to have a total ordering of operations -\\xa0you also need to know when that ordering is finalized.\\n\\n##### Total Order Broadcast\\n\\n* *Total order broadcast* or *atomic broadcast* is the problem of how to scale the system if the throughput is greater than a single leader can handle, and how to handle failover if the leader fails.\\n* Total ordering across all partitions in a partitioned database is possible, but it requires additional coordination.\\n* Total order broadcast is a protocol for exchanging messages between nodes and requires two safety guarantees: reliable delivery and totally ordered delivery.\\n\\n###### Using total order broadcast\\n\\n* Consensus services such as ZooKeeper and etcd actually implement total order broadcast.\\n* *State machine replication* is the use of total order broadcast for database replication: Every replica processes every write in the same order.\\n* Total order broadcast can be framed as creating a *log*, where delivering a message is like appending to the log.\\n* ZooKeeper uses total order broadcast to implement a lock service: Every request to acquire the lock is appended as a message to the log, and all messages in the log are sequentially ordered. The sequence number, or `zxid`, serves as a fencing token.\\n\\n###### Implementing linearizable storage using total order broadcast\\n\\n* You can build linearizable storage on top of total order broadcast by appending a message for each write or read, and then processing the write or read upon reading the message from the log.\\n\\n###### Implementing total order broadcast using linearizable storage\\n\\n* For every message you want to send through total order broadcast, increment-and-get the linearizable integer, and then attach the register value as a sequence number to the message.\\n* It can be proved that a linearizable compare-and-set (or increment-and-get) register and total order broadcast are both *equivalent to consensus*.\\n\\n#### Distributed Transactions and Consensus\\n\\n* Consensus is needed for leader election, as all nodes need to agree on which node is the leader.\\n* In the *atomic commit problem*, all nodes participating in a distributed transaction need to agree on the outcome of the transaction (either all aborting or all committing).\\n\\n##### Atomic Commit and Two-Phase Commit (2PC)\\n\\n* Atomicity ensures that a secondary index stays consistent with the primary data.\\n\\n###### From single-node to distributed atomic commit\\n\\n* The deciding moment for whether a transaction commits or aborts is the moment at which the disk finishes writing the commit record: After this moment, the transaction is committed.\\n* Once a transaction has been committed on one node, it cannot be retracted if it later turns out that it was aborted on another node.\\n* If a transaction was allowed to abort after committing, it would violate *read committed isolation*, and any transactions that read the committed data would be based on data that was retroactively declared not to have existed.\\n\\n###### Introduction to two-phase commit\\n\\n* 2PC (two-phase commit) provides atomic commit in a distributed database, whereas 2PL (two-phase locking) provides serializable isolation.\\n* 2PC requires a *coordinator*, or *transaction manager*, which is often implemented as a library within the same application process that is requesting the transaction.\\n* When an application is ready to commit, the coordinator begins phase 1, sending a *prepare* request to every node asking if it is able to commit.\\n* If all nodes reply \"yes\" then the coordinator sends a *commit* request in phase 2 and the commit actually takes place.\\n\\n###### A system of promises\\n\\n* By replying \"yes\" to the coordinator, a node promises to commit the transaction without error if requested.\\n* The *commit point* is when the coordinator writes to its transaction log on disk whether it will go ahead with phase 2 or abort, so that its decision is preserved in case it subsequently crashes.\\n* If the decision was to commit, then that decision must be enforced, no matter how many retires it takes.\\n\\n###### Coordinator failure\\n\\n* After a node has voted \"yes\" to a prepare request, it must wait to hear from the coordinator whether to commit or abort.\\n* If a node replies to a prepare request and then the coordinator crashes or the network fails, the node\\'s transaction state is called *in doubt* or *uncertain*.\\n* Upon the coordinator recovering from a crash, it computes the status of all in-doubt transactions by reading its transaction log, and aborts any without a commit record.\\n\\n##### Distributed Transactions in Practice\\n\\n* Distributed transactions are criticized for causing operational problems, killing performance, and promising more than they can deliver.\\n* The performance cost inherent in 2PC is due to the additional disk forcing (`fsync`) that is required for crash recovery, and the additional network round-trips.\\n* Database-internal distributed transactions can often work quite well, while transactions spanning heterogeneous technologies are a lot more challenging.\\n\\n###### XA transactions\\n\\n* X/Open XA (for eXtended Architecture) is a C API for interfacing with a transaction coordinator. It is supported by many traditional databases and message brokers.\\n* In practice the transaction coordinator implementing the XA API is a library loaded into the same process as the application issuing the transaction.\\n\\n###### Holding locks while in doubt\\n\\n* Database transactions take a row-level exclusive lock on any rows they modify, and serializable isolation requires acquiring a shared lock on any rows read by the transaction.\\n* Using 2PC, a transaction must hold onto locks throughout the entire time it is in doubt, which can cause large parts of your application to be unavailable until the in-doubt transaction is resolved.\\n\\n###### Recovering from coordinator failure\\n\\n* Orphaned in-doubt transactions do occur, which hold locks and block other transactions until they are either manually committed or rolled back by an administrator.\\n\\n###### Limitations of distributed transactions\\n\\n* When the transaction coordinator is part of the application server, the application servers are no longer stateless because the coordinator logs are a critical part of the durable system state.\\n* Distributed transactions tend to *amplify failures*, because if any part of the system is broken then the distributed transaction also fails.\\n\\n##### Fault-Tolerant Consensus\\n\\n* Consensus is where one or more nodes may *propose* values, and a consensus algorithm *decides* on one of those values.\\n* The algorithm must have uniform agreement (no two nodes decide differently), integrity (no node decides twice), validity (a node can decide only on a proposed value), and termination (every node that does not crash eventually decides a value).\\n* The termination property formalizes the idea of fault tolerance, as the consensus algorithm must make progress.\\n* Termination is a liveness property, whereas uniform agreement, integrity, and validity are all safety properties.\\n* Any consensus algorithm requires at least a majority of nodes to be functioning correctly in order to assure termination. This majority can safely form a quorum.\\n* A large-scale outage can stop the consensus system from being able to process requests, but it cannot corrupt the system by causing it to make invalid decisions.\\n\\n###### Consensus algorithms and total order broadcast\\n\\n* Total order is equivalent to repeated rounds of consensus: In each round, nodes propose the message that they want to send next, and then decide on the next message to be delivered in the total order.\\n\\n###### Epoch numbering and quorums\\n\\n* Consensus protocols like Raft and Paxos define an *epoch number*, and guarantee that within each epoch the leader is unique.\\n* Every time the leader is thought to be dead, a vote is started among the nodes to elect a new leader. This election is given an incremented epoch number.\\n* The quorums for voting for a leader and for voting on a leader\\'s proposal must overlap. So if a vote on a proposal succeeds, then at least one of the nodes that voted for it must have also participated in the most recent leader election.\\n* If the vote on a proposal doesn\\'t reveal any higher-numbered epoch, the current leader can conclude that no leader election with a higher epoch number has happened, and so it is still the leader.\\n\\n###### Limitations of consensus\\n\\n* The process by which nodes vote on proposals before they are decided is a kind of synchronous replication.\\n* Most consensus algorithms assume a fixed set of nodes that participate in voting, meaning you cannot add or remove nodes in the cluster.\\n* In geographically distributed systems, nodes can falsely believe the leader to have failed because of a transient network issue, leading to frequently leader elections that degrade performance.\\n\\n##### Membership and Coordination Services\\n\\n* ZooKeeper and etc are designed to hold small amounts of data that fit entirely in memory, which is replicated across all nodes using a fault-tolerant total order broadcast algorithm.\\n* Using an atomic compare-and-set operation, you can implement a lock in ZooKeeper: If several nodes concurrently try to perform the same operation, only one will succeed.\\n* ZooKeeper provides a monotonically increasing fencing token by totally ordering all operations and giving each one a monotonically increasing transaction ID (`zxid`) and version number (`cversion`).\\n\\n###### Allocating work to nodes\\n\\n* Leader election and assigning partitioned resources to nodes can be achieved by judicious use of atomic operations, ephemeral nodes, and notifications in ZooKeeper.\\n* ZooKeeper provides a way of \"outsourcing\" some of the work of coordinating nodes (consensus, operation ordering, and failure detection) to an external service.\\n\\n###### Service discovery\\n\\n* ZooKeeper, etc, and consul are often used for *service discovery*, which returns the set of IP addresses that are running a given service.\\n* Replicas that asynchronously receive the log of all decisions of the consensus algorithm but don\\'t participate in the voting can serve read requests that don\\'t need to be linearizable.\\n\\n### Chapter 10: Batch Processing\\n\\n* There are three types of systems:\\n  * Services (online systems), where servers wait for a request to arrive and then attempt to send a response back as quickly as possible. Availability and response time are most important metrics.\\n  * Bach processing (offline systems), where a job processes some large amount of input data to create output data. Throughput is the most important metric.\\n  * Stream processing (near-realtime systems), where a stream job operates on events shortly after they happen.\\n\\n#### Batch Processing with Unix Tools\\n\\n##### Simple Log Analysis\\n\\n###### Sorting versus in-memory aggregation\\n\\n* If data to sort exceeds available memory, chunks can be sored in memory and written to disk as segment files, and then multiple sorted segments can be merged together into a larger sorted file.\\n* The `sort` utility handles larger-than-memory datasets by spilling to disk, and automatically parallelizes sorting across multiple CPU cores.\\n\\n##### The Unix Philosophy\\n\\n* The philosophy described in 1978 says: Make each program do one thing well, and expect the output of every program to be the input to another, as yet unknown, program.\\n* A Unix shell like `bash` allows us to easily *compose* small programs into surprisingly powerful data processing jobs.\\n\\n###### A uniform interface\\n\\n* In Unix, all programs use a file descriptor as the input/output interface. That file is just an ordered sequence of bytes.\\n\\n###### Separation of logic and wiring\\n\\n* Pipes allow you to attach the `stdout` of one process to the `stdin` of another process with an in-memory buffer, without writing all intermediate data to disk.\\n* Separating the input/output wiring from the program logic makes it easier to compose small tools into bigger systems.\\n\\n###### Transparency and experimentation\\n\\n* Unix tools are successful in part because it is easy to see what is going on:\\n  * The input files are normally treated as immutable, and so you can run the commands repeatedly.\\n  * You can pipe the output into `less` and look to see if it has the expected form at any point.\\n  * By writing the output of one stage to a file and using that file as the input to the next stage, you can restart the later stage without re-running the entire pipeline.\\n\\n#### MapReduce and Distributed Filesystems\\n\\n* Hadoop Distributed File System (HDFS) is based on the shard-nothing principle requiring no special hardware, only computers connected by a conventional network.\\n* HDFS consists of a daemon process running on each machine for I/O, and a central server called NameNode to track which file blocks are stored on which machine.\\n\\n##### MapReduce Job Execution\\n\\n* The *mapper* of a MapReduce job can generate any number of key-value pairs for a given input.\\n* The *reducer* processes the collection of values for a given key and can produce output records.\\n* The role of the mapper is to prepare the data by putting it in a form that is suitable for sorting, and the role of the reducer is to process the data that has been sorted.\\n\\n###### Distributed execution of MapReduce\\n\\n* The mapper and reducer do not know where each input record is coming from or output record is going to, so the framework can handle those complexities.\\n* The MapReduce framework *puts computation near the data* by attempting to run each mapper on one of the machines that stores the record of the input file.\\n* While the number of map tasks is determined by the number of input file blocks, the number of reduce tasks is configured by the job author.\\n* Each map task partitions its output by reducer, based on the hash of the key. Each of these partitions is written to a sorted file on the mapper\\'s local disk.\\n* The reducers download the files of sorted key-value pairs for their partition. The process of partitioning by reducer, sorting, and copying data partitions from mappers to reducers is known as the *shuffle*.\\n* When the reduce task merges the files from the mapper it preserves their sorted order, and so the values are iterated over in sorted order.\\n\\n###### MapReduce workflows\\n\\n* It\\'s common for MapReduce jobs to be chained together into *workflows*, where the output of one job becomes the input to the next job.\\n* Chained MapReduce jobs are less like pipelines of Unix commands and more like a sequence of commands where each command\\'s output is written to a temporary file, which the next command reads from.\\n\\n##### Reduce-Side Joins and Grouping\\n\\n* Associations between records include a *foreign key* in a relational model, *document reference* in a document model, or an *edge* in a graph model.\\n* A join is necessary whenever you have some code that needs to access records on both sides of that association.\\n* In the context of batch processing, a join means resolving _all_ occurrences of some association within a data set, and is not limited to a subset of records.\\n\\n###### Example: analysis of user activity events\\n\\n* To achieve good throughput in a batch process, computation must be (as much as possible) local to one machine. Requests over the network on a per-record basis is too slow.\\n* When data must be joined together, it should be co-located on the same machine.\\n\\n###### Sort-merge joins\\n\\n* A *secondary sort* ensures that a reducer first processes all records on one side of the association first, and then all records on the other side.\\n* A sort-merge join has its name because mapper output is sorted by key, and the reducers then merge together the sorted lists of records from both sides of the join.\\n\\n###### Bringing related data together in the same place\\n\\n* Having lined up all the data in advance, the reducer can be a simple, single-threaded piece of code that churns through records with high throughput and low memory overhead.\\n* Because MapReduce separates the physical network communication aspects of computation from the application logic, it can transparently retry failed tasks without affecting the application logic.\\n\\n###### GROUP BY\\n\\n* The simplest way of implementing `GROUP BY` in MapReduce is to configure the mappers so that all produced key-value pairs use the desired grouping key, and then aggregate them in the reducer.\\n\\n###### Handling skew\\n\\n* Keys with a disproportionate number of values, or *hot keys*, can lead to *skew* or *hot spots* where one reducer must process significantly more records than the others.\\n* This is problematic because any subsequent jobs must wait for the slowest reducer to complete before they can start.\\n* To aggregate values for a hot key, a first stage of aggregation can be distributed across multiple reducers, and then a second stage combines those values to create a single value for the key.\\n\\n##### Map-Side Joins\\n\\n* A *reduce-side join* where join logic belongs to the reducers is flexible, but all that sorting, copying to reducers, and merging of reducer inputs can be expensive.\\n* A *map-side join* makes assumptions about the size, sorting, and partitioning of input data, but removes the needs for reducers and sorting.\\n\\n###### Broadcast hash joins\\n\\n* In a *broadcast hash join*, all the data on one side of the association is loaded into memory and is used directly by the mapper.\\n* The term *broadcast* refers to how the small input is \"broadcast\" to all partitions of the large input, and *hash* refers to its use of a hash table.\\n\\n###### Partitioned hash joins\\n\\n* In a *partitioned hash join* all records you might want to join belong to the same partition, and so each mapper can read only one partition from each of the input data sets.\\n* This only works if both of the join\\'s inputs have the same number of partitions, with records assigned to partitions based on the same key and the same hash function.\\n\\n###### Map-side merge joins\\n\\n* If the input data sets are not only partitioned in the same way but also sorted by the same key, the mapper can perform the merging operation normally done by a reducer.\\n\\n###### MapReduce workflows with map-side joins\\n\\n* The output of a reduce-side join is partitioned and sorted by the join key, whereas the output of a map-side join is partitioned and sorted in the same way as the large input.\\n\\n##### The Output of Batch Workflows\\n\\n* Batch processing is closer to analytics in that it typically scans over large portions of an input data set, but its output is often not a report but some other structure.\\n\\n###### Key-value stores as batch process output\\n\\n* To persist the output of a batch process into a database for querying, writing one record at a time to the database from the mapper or reducer is a bad idea:\\n  * Making a network request for every single record is orders of magnitude slower than the normal throughput of a batch task.\\n  * If all mappers or reducers concurrently write to the same database, then that database can become overwhelmed.\\n  * Partially completed jobs may be visible to other systems, while MapReduce provides a clean all-or-nothing guarantee for job output.\\n* A better solution is to create a new database inside the batch job and write it as files to the job\\'s output directory in the distributed filesystem.\\n\\n###### Philosophy of batch process outputs\\n\\n* By treating inputs as immutable and avoiding side-effects, batch jobs not only achieve good performance but also become easier to maintain.\\n* The idea of being able to recover from buggy code has been called *human fault tolerance*.\\n* The automatic retry of a map or reduce task is safe only because inputs are immutable and outputs from failed tasks are discarded by the MapReduce framework.\\n\\n##### Comparing Hadoop to Distributed Databases\\n\\n* Hadoop is somewhat like a distributed version of Unix, where HDFS is the filesystem and MapReduce is a quirky implementation of a Unix process.\\n\\n###### Diversity of storage\\n\\n* In practice, making data available quickly ‚Äì even if in a difficult-to-use, raw format ‚Äì\\xa0is more valuable than trying to decide on the ideal data model up front.\\n* A \"data lake\" or \"enterprise data hub\" collects data in its raw form, allowing its collection to be expedited.\\n* Instead of forcing the producer of a dataset to bring it into a standardized format, the interpretation of the data becomes the consumer\\'s problem.\\n* The _sushi principle_ says raw data is better, as dumping the raw data allows a consumer to transform the data into whatever form is ideal for its purposes.\\n\\n###### Diversity of processing models\\n\\n* MapReduce is too limiting or performs too badly for some types of processing, and so various other processing models have been developed on top of Hadoop.\\n* These various processing models can all be run on a single shared-use cluster of machines, all accessing the same files on the distributed system.\\n* The Hadoop ecosystem includes random-access OLTP databases such as HBase, and MPP (Massively Parallel Processing) databases such as Impala.\\n\\n###### Designing for frequent faults\\n\\n* Unlike MPP databases, MapReduce can tolerate the failure of a MapReduce map or reduce task by retrying it, and it eagerly writes data to disk for fault tolerance and in case the data cannot fit entirely in memory.\\n* MapReduce was designed to tolerate frequent faults because MapReduce jobs at Google were run at low priority and could be preempted at any time by higher-priority processes requiring their resources.\\n\\n#### Beyond MapReduce\\n\\n* MapReduce is robust, but other tools are sometimes orders of magnitude faster for other types of processing.\\n\\n##### Materialization of Intermediate State\\n\\n* The process of writing out intermediate state to files is called *materialization*.\\n* Pipes in Unix do not fully materialize the intermediate state, but instead *stream* one command\\'s output to the next command\\'s input using a small in-memory buffer.\\n* Unlike Unix pipes, MapReduce fully materializing intermediate state has downsides:\\n  * Having to wait until all of the preceding job\\'s tasks have completed slows down the execution of the workflow as a whole.\\n  * Mappers are often redundant because they just read back the same file that was just written by a reducer, and prepare it for the next stage of partitioning and sorting.\\n  * Storing intermediate state in a distributed filesystem replicates it across several nodes, which is overkill for temporary data.\\n\\n###### Dataflow engines\\n\\n* *Dataflow engines* model the flow of data through several processing stages. The user-defined functions for operating on the data are called *operators*.\\n* Advantages over the MapReduce model include:\\n  * Expensive work like sorting is only done in places where it is required, rather than always happening between every map and reduce stage.\\n  * There are no unnecessary map tasks, since the work done by a mapper can often be incorporated into the preceding reduce operator.\\n  * Because all joins and data dependencies are explicitly declared, the scheduler can make locality optimizations.\\n  * Intermediate state between operators can be kept in memory or written to local disk, which requires less I/O than writing it to HDFS.\\n  * Operators can start executing as soon as their input is ready instead of waiting for the entire preceding stage to finish.\\n\\n###### Fault tolerance\\n\\n* If a machine fails and the intermediate state on a machine is lost, the dataflow engine can recompute it from the preceding stages.\\n* If a computation is not deterministic, then the original data is not the same as the lost data, and downstream operators must reconcile the contradictions between the old and new data.\\n* The solution to such nondeterministic operators is to kill the downstream operators as well, and run them again on the new data.\\n* If the intermediate data is much smaller than the source data, or if the computation is very CPU-intensive, it\\'s cheaper to materialize the intermediate data than to recompute it.\\n\\n###### Discussion of materialization\\n\\n* While materialized datasets on HDFS are still usually the inputs and outputs of a job, the dataflow engine saves the job from writing all intermediate state to the filesystem.\\n\\n##### Graphs and Iterative Processing\\n\\n* Many graph algorithms traverse one edge at a time, joining one vertex with an adjacent one to propagate some information, and repeating until some condition is met like no more edges to follow, or some metric converging.\\n\\n###### The Pregel processing model\\n\\n* The *Pregel* model, or *bulk synchronous processing model* (BSP), is a popular optimization for batch processing graphs.\\n* One vertex can \"send a message\" to another vertex, usually along an edge in a graph ‚Äì similar to how mappers can conceptually send messages to reducers by key.\\n* In each iteration, a function is called for each vertex, passing the function all the messages that were sent to that vertex ‚Äì again like a reducer.\\n\\n###### Fault tolerance\\n\\n* Pregel implementations guarantee that messages are processed exactly once at their destination vertex in the following iteration.\\n\\n###### Parallel execution\\n\\n* Graph algorithms often have a lot of cross-machine communication overhead, as the intermediate state is often bigger than the original graph.\\n* If your graph can fit in memory on a single computer, it\\'s likely that a single-machine or even single-threaded algorithm will outperform a distributed batch process.\\n\\n##### High Level APIs and Languages\\n\\n* Now that MapReduce is robust at scale, attention has turned to improving the programming model, improving efficiency, and broadening the set of problems that these technologies can solve.\\n\\n###### The move toward declarative query languages\\n\\n* The freedom to run arbitrary code is what has long distinguished batch processing systems of MapReduce heritage from MPP databases.\\n* By incorporating declarative aspects into high-level APIs, batch processing frameworks can use query optimizers to achieve comparable performance with MPP databases.\\n\\n###### Specialization for different domains\\n\\n* As batch processing systems gain high-level declarative operators, and MPP databases become more programmable, the two are beginning to look more alike.\\n\\n#### Summary\\n\\n* Distributed batch processing systems need to solve partitioning, where all related data (e.g. all records with the same key) are brought together in the same place, and fault tolerance.\\n* Callback functions like mappers and reducers are assumed to be stateless and without side-effects, allowing the framework to hide some of the hard distributed systems problems behind its abstraction.\\n\\n### Chapter 11: Stream Processing\\n\\n* A lot of data is unbounded because it changes gradually over time. Batch processors just artificially divide the data into chunks of fixed duration.\\n* The problem with daily batch processes is that changes in the input are only reflected in the output a day later, which is too slow for many impatient users.\\n* A \"stream\" refers to data that is incrementally made available over time.\\n\\n#### Transmitting Event Streams\\n\\n* In a stream processing context, a record is commonly referred to as an *event*.\\n* An event is generated once by a *producer* or *publisher*, and is processed by multiple *subscribers* or *recipients*. Related events are grouped together into a *topic* or *stream*.\\n* When moving toward continual processing with low delays, polling can become expensive; instead it\\'s better for consumers to be notified when new events appear.\\n\\n##### Messaging Systems\\n\\n* Two questions are worth asking in a publish/subscribe model:\\n  * What happens if producers send messages faster than consumers can process them? There are three options: dropping messages, buffering messages, and applying backpressure.\\n  * What happens if nodes crash or temporarily go offline? If you can lose messages then you can probably get higher throughput and lower latency on the same hardware.\\n\\n###### Direct messaging from producers to consumers\\n\\n* StatsD uses unreliable UDP messaging for collecting metrics from all machines on the network and monitoring them.\\n* Direct messaging systems generally require the application code to be aware of the possibility of message loss.\\n* If a consumer is offline, it may miss messages that were sent while it is unreachable.\\n\\n###### Message brokers\\n\\n* A *message broker* or *message queue* is a database optimized for handling message streams.\\n* Centralizing data in the broker allows clients to come and go, and the question of durability is moved to the broker instead.\\n\\n###### Message brokers compared to databases\\n\\n* Databases usually keep data until it is explicitly deleted, whereas most message brokers automatically delete a message when it has been successfully delivered to consumers.\\n* Since message brokers quickly delete messages, it\\'s assumed that their queues are short.\\n\\n###### Multiple consumers\\n\\n* Given multiple consumers reading messages in the same topic, two patterns of messaging are used:\\n  * Load balancing: Each message is delivered to one of the consumers, and you can add consumers to parallelize the processing.\\n  * Fan-out: Each message is delivered to *all* of the consumers, allowing several independent consumers to consume the same topic of messages without affecting each other.\\n* The two patterns can be combined, e.g. two separate groups of consumers can subscribe to the same topic.\\n\\n###### Acknowledgments and redelivery\\n\\n* Message brokers use *acknowledgments* where a client must explicitly tell the broker when it has finished processing a message so that it can be removed from the queue.\\n* The combination of load balancing and redelivery inevitably leads messages to be reordered. To avoid the issue, you can disable load balancing by enforcing a separate queue per consumer.\\n\\n##### Partitioned Logs\\n\\n* Receiving a message is destructive if the acknowledgment causes it to be deleted from the broker, so you cannot run the same consumer again and get the same result.\\n* When adding a new consumer, it typically only starts receiving messages sent after the time it was registered, and so any prior messages cannot be recovered.\\n* A *log-based message broker* combines the durable storage approach of databases with the low-latency notification facilities of messaging.\\n\\n###### Using logs for message storage\\n\\n* A topic is a group of partitions that all carry messages of the same type.\\n* Within each partition, the broker assigns a monotonically increasing sequence number, or *offset*, to every message. There is no total ordering of messages across partitions.\\n\\n###### Logs compared to traditional messaging\\n\\n* To load balance across a group of consumers, the broker can assign entire partitions to each consumer in a consumer group, as opposed to assigning individual messages.\\n* Each consumer consumes *all* the messages in the partitions it has been assigned. This has some downsides:\\n  * The number of consumers in a group can be at most the number of log partitions in that topic.\\n  * If a single message is slow to process, it holds up the processing of subsequent messages in that partition.\\n\\n###### Consumer offsets\\n\\n* If a consumer node fails, another node in the consumer group is assigned the failed consumer\\'s partitions, and it starts consuming messages at the last recorded offset.\\n\\n###### Disk space usage\\n\\n* To reclaim disk space, the log is divided into segments, and periodically the oldest segments are deleted or moved to archive storage.\\n* The log therefore implements a bounded-size buffer, also known as a *circular buffer* or *ring buffer*, that discards old messages when it gets full.\\n\\n###### When consumers cannot keep up with producers\\n\\n* If a consumer falls so far behind that the messages it requires are older than what is retained on disk, then it cannot read those messages and they are effectively dropped.\\n* You can monitor how far a consumer is behind the head of the log, or its lag, and raise an alert if it falls behind significantly.\\n\\n#### Databases and Streams\\n\\n* The fact that something was written to the database is itself an event that can be captured, stored, and processed.\\n* A replication log is a stream of database write events, produced by the leader as it processes transactions, and consumed by replicas.\\n\\n##### Keeping Streams in Sync\\n\\n* Often data is persisted somewhere and then needs to be replicated to several different places, such as a search index or cache.\\n* Dual writes, where the application explicitly writes to each of the systems, suffer from two problems:\\n  * A *race condition* where upon two clients dual writing a value, one client may write to the first system first and the second system second.\\n  * One of the writes may fail while the other succeeds, thereby leaving the data in the two systems in an inconsistent state.\\n\\n##### Change Data Capture\\n\\n* *Change data capture* (CDC) is the process of observing all data changes to a database and extracting them in a form in which they can be replicated to other systems.\\n\\n###### Implementing change data capture\\n\\n* CDC ensures that all changes made to the system of record are also reflected in the derived data systems so that they too have an accurate copy of the data.\\n* A log-based message broker is well suited for transporting the change events from the source database to the derived systems, because it preserves the ordering of messages.\\n* Parsing the replication log of a database is a robust approach to CDC, but it comes with challenges such as handling schema changes.\\n\\n###### Initial snapshot\\n\\n* If you do not have the entire log history, you must start with a consistent snapshot.\\n* The snapshot must correspond to a known position or offset in the change log, so that you know at which point to start applying changes after the snapshot has been processed.\\n\\n###### Log compaction\\n\\n* A compaction or merging process running in the background looks for messages with the same key, and keeps only the most recent update for each key.\\n* The disk usage required by a compacted log depends only on the current contents of the database, and not the number of writes that have ever occurred in the database.\\n* To rebuild a derived data system, start a new consumer from offset 0 of the log-compacted topic, and sequentially scan over all messages in the log.\\n\\n##### Event Sourcing\\n\\n* *Event sourcing* stores all changes to the application state as a log of change events, but applies the idea at a different level of abstraction:\\n  * In CDC, the application uses the database in a mutable way, updating and deleting records. The application layer can be unaware that CDC is occurring.\\n  * In event sourcing, the application logic is built on the basis of immutable events written to an event log. Events reflect things that happened at the application level, rather than low-level state changes.\\n* From an application point of view it is more meaningful to record the user\\'s actions as immutable events rather than recording the effects of those actions on a mutable database.\\n\\n###### Deriving current state from the event log\\n\\n* Applications using event sourcing must transform the log of events (representing data *written* to the system) into application state that is suitable for display to the user.\\n* Log compaction is not possible with event sourcing, as later events do not override prior events, and so you need the full history of events to construct the final state.\\n* Applications using event sourcing can restore from a snapshot of the current state, but the intention is that the system can reprocess the full event log whenever required.\\n\\n###### Commands and events\\n\\n* When a request from a user first arrives, it is initially a *command*: At this point it may still fail, for example because some integrity constraint is violated.\\n* If the validation is successful and the command is accepted, it becomes an *event*, which is durable and immutable. It becomes a *fact*.\\n* A consumer of the event stream cannot reject an event, as it\\'s already an immutable part of the log. So validation of the command must happen synchronously before it becomes an event.\\n\\n##### State, Streams, and Immutability\\n\\n* Mutable state and an append-only log are two sides of the same coin: The log of all changes, or the *changelog*, represents the evolution of state over time.\\n* You might say that the application state is what you get when you integrate an event stream over time, and a change stream is what you get when you differentiate the state by time.\\n* You can consider the log as truth. The database is a cache of the subset of the log, namely the contents of the latest record values in the logs.\\n\\n###### Advantages of immutable events\\n\\n* If you accidentally deploy buggy code that writes bad data to a database, recovery is much harder if the code is able to destructively overwrite data.\\n\\n###### Deriving several views from the same event log\\n\\n* By separating mutable state from the immutable event log, you can derive several different read-oriented representations from the same log of events.\\n* You can use the event log to build a separate read-optimized view for some new feature, and run it alongside the existing system until transitioning over and decommissioning the old system.\\n* Storing data is normally quite straightforward if you don\\'t have to worry about how it is going to be queried, as most schema complexity comes from supporting querying the data.\\n* Separating the form in which data is written from the form it is read is called *command query responsibility segregation*, or CQRS.\\n\\n###### Concurrency control\\n\\n* Because event sourcing and CDC is asynchronous, the user may write to a log, and then read from a log-derived view and find that their write has not yet been reflected in the read view.\\n* Because event sourcing requires only a single, atomic write\\xa0‚Äì namely appending an event to a log\\xa0‚Äì which is a simpler concurrency model than using multi-object transactions to update data in multiple places.\\n\\n###### Limitations to immutability\\n\\n* Workloads with many updates and deletes on comparatively few entities may cause the immutable history to grow prohibitively large.\\n* Given a large history of changes, the performance of compaction and garbage collection becomes crucial for operational robustness.\\n* Immutability is also at odds with having to delete the data for administrative or legal reasons, such as GDPR.\\n\\n#### Processing Streams\\n\\n* There are three options for processing a stream:\\n  * You can take the data in the events and write it to a database, cache, search index, or similar storage system, from where it can then be queried by other clients.\\n  * You can push the events to the users in some way, so that a human is the ultimate consumer of a stream.\\n  * You can process one or more input streams to produce one or more output streams.\\n* A piece of code that processes one or more streams to produce other, derived streams is called an *operator* or a *job*.\\n\\n##### Uses of Stream Processing\\n\\n###### Complex event processing\\n\\n* *Complex event processing* (CEP) allows specifying rules to search for certain patterns of events in a stream, similar to a regular expression operating on a string.\\n* CEP systems often use a high-level declarative query language like SQL to describe patterns of events for detection.\\n* A processing engine maintains a state machine that performs the required matching on the input streams, and emits a *complex event* with the details of a matching event pattern.\\n* CEP engines function in an manner opposite to normal databases: The queries are stored long-term, and the data that they operate on is transient.\\n\\n###### Stream analytics\\n\\n* Analytics is less interested in finding specific event sequences, and is more oriented toward aggregations and statistical metrics over a large number of events.\\n* The time interval over which you aggregate is known as a *window*.\\n* Probabilistic algorithms produce approximate results, but have the advantage of requiring significantly less memory in the stream processor than exact algorithms.\\n\\n###### Search on streams\\n\\n* Running in a manner opposite to normal search engines, searching a stream requires storing the queries, and the documents they search over are transient.\\n\\n##### Reasoning About Time\\n\\n* Many stream processing frameworks use the local system clock on the processing machine (or *processing time*) to determine windowing, but this breaks down if there is significant processing lag.\\n\\n###### Event time versus processing time\\n\\n* Processing may be delayed from queueing, network faults, contention in the broker or processor, a restart of the stream consumer, or reprocessing of past events.\\n* Confusing event time and processing time leads to bad data.\\n\\n###### Knowing when you\\'re ready\\n\\n* You can never be sure when you\\'ve received all of the events for a given window, or whether there are some events still to come.\\n* You have two options for handling *straggler* events that arrive after the window has already been declared complete: Ignore them, or publish a corrective value with stragglers included.\\n\\n###### Whose clock are you using, anyway?\\n\\n* Assigning timestamps to events is even more difficult when events can be buffered at several points in the system, such as on a mobile device that is offline and later reconnects.\\n* To estimate the offset between the server and device clocks, subtract the timestamp at which the event was sent to the server according to the device clock from the timestamp at which the event was received by the server according to the server clock.\\n* Applying this computed offset to the event timestamp allows you to estimate the true time at which the event occurred.\\n\\n###### Types of windows\\n\\n* A *tumbling window* has a fixed length, and every event belongs to exactly one window.\\n* A *hopping window* has a fixed length, but windows may overlap to provide smoothing. You can implement this by computing 1-minute tumbling windows, and then aggregating over several adjacent windows.\\n* A *sliding window* contains all events that occur within some interval of each other. You can implement this by buffering events sorted by time, and removing old events that have expired from the window.\\n* A *session window* has no fixed duration, but instead groups together all events for the same user that occur closely together in time. The window ends when the user is inactive for some time.\\n\\n##### Stream Joins\\n\\n* The fact that new events can appear anytime on a stream makes joins on streams more challenging than in batch jobs.\\n\\n###### Stream-stream join (window join)\\n\\n* The stream processor maintains state that is updated whenever an event occurs, which is referenced upon processing subsequent events.\\n\\n###### Stream-table join (stream enrichment)\\n\\n* *Enriching* a stream means augmenting its events with additional data queried from a database.\\n* If the stream processor has a local copy of the database (similar to map-side joins), then that database can be kept up to date using change data capture.\\n* Unlike a stream-stream join, a stream-table join uses a window that reaches back to the beginning of time (a conceptually infinite window) with newer versions of records overwriting old ones.\\n\\n###### Table-table join (materialized view maintenance)\\n\\n* A table-table join is a stream process that maintains a materialized join between two tables.\\n\\n###### Time-dependence of joins\\n\\n* All three joins above require the stream processor to maintain some state based on one join input, and to query that state on messages from the other input.\\n* The ordering of the events that maintain the state is important, and in a partitioned log, there is no ordering guarantee of events across different streams or partitions.\\n* Because state changes over time, and you join with some state, you must ask yourself what point in time you are using for the join.\\n* If the ordering of events across streams is undetermined, then the join is nondeterministic, and the events may be interleaved differently when you run the job again.\\n* This issue is known as a *slowly changing dimension* (SCD) in data warehouses and is often addressed by using a unique identifier for a particular version of a joined record.\\n* A unique identifier makes the join deterministic, but log compaction is no longer possible because all versions of the records in the table must be retained.\\n\\n##### Fault Tolerance\\n\\n* The batch processing approach to fault tolerance exposes *exactly-once semantics*, where it appears as though every record was processed exactly once.\\n* In stream processing, waiting until a task is finished before making its output visible is not an option, because a stream is infinite.\\n\\n###### Microbatching and checkpointing\\n\\n* *Microbatching* breaks the stream into small blocks and treats each block like a miniature batch process.\\n* With microbatching, small batches incur greater scheduling and coordination overhead, while larger batches mean a longer delay before results of the stream processor become available.\\n* An alternative is to periodically generate rolling checkpoints of state and write them to durable storage.\\n* Once the output leaves the stream processor, the framework cannot discard the output of a failed batch. Restarting a failed task causes its external side-effects to happen twice, regardless of checkpointing or microbatching.\\n\\n###### Atomic commit revisited\\n\\n* To give the appearance of exactly-once processing in the presence of faults, all outputs and side-effects of processing an event must happen if and only if the processing is successful.\\n* More restricted environments, which do not attempt to persist data across heterogeneous environments, can implement an atomic commit facility efficiently.\\n\\n###### Idempotence\\n\\n* An idempotent operation is one that you can perform multiple times, and it has the same effect as if you had performed it only once.\\n* Using idempotence relies on restarting a task to replay the same messages in the same order, the processing must be deterministic, and no other node may concurrently update the same value.\\n\\n###### Rebuilding state after a failure\\n\\n* Any stream process that requires state (e.g. any windowed aggregations or tables for joins) must ensure that the state can be recovered after a failure.\\n* A performant solution is to keep the state local to the stream processor, and then to replicate it periodically.\\n* If the state is a local replica of a database, maintained by CDC, then the database can be rebuilt from the log-compacted change stream.\\n\\n#### Summary\\n\\n* There are three types of joins that may appear in stream processes:\\n  * *Stream-stream joins*: Both input streams consist of activity events, and the join operator searches for related events that occur in some window of time.\\n  * *Stream-table joins*: One input stream consists of activity events, while the other is a database changelog that keeps a local copy of a database up to date. This emits enriched activity events.\\n  * *Table-table joins*: Both input streams are database changelogs. The result is a stream of changes to the materialized view of the join between the two tables.\\n\\n### Chapter 12: The Future of Data Systems\\n\\n#### Data Integration\\n\\n* There is unlikely one piece of software that is suitable for all circumstances in which data is used, and so we must cobble together several pieces of software to provide the application\\'s functionality.\\n\\n##### Combining Specialized Tools by Deriving Data\\n\\n* As the number of different representations of the data increases, the integration problem becomes harder.\\n* The need for data integration often only becomes apparent when you zoom out and consider the dataflows across an entire organization.\\n\\n###### Reasoning about dataflows\\n\\n* You must have clarity on the inputs and outputs, namely where data is written first, and which representations are derived from which sources.\\n* By funneling all user input through a single system that decides on an ordering for all writes, it\\'s easier to derive other representations of the data by processing the writes in the same order.\\n* Whether to use change data capture or an event sourcing log is less important than simply the principle of deciding on a total order.\\n\\n###### Derived data versus distributed transactions\\n\\n* Distributed transactions decide on an ordering of writes by using locks for mutual exclusion, while CDC and event sourcing use a log for ordering.\\n* Distributed transactions use atomic commit to ensure that changes take effect only once, while log-based systems are often based on deterministic retry and idempotence.\\n* Transactions systems usually provide linearizability and reading your own writes, while derived data systems are asynchronous and do not offer such guarantees.\\n\\n###### The limits of total ordering\\n\\n* As systems scale, limitations of total ordering begin to emerge:\\n  * To scale you may need to partition the log across multiple machines, but the order of events across two different partitions is ambiguous.\\n  * If servers are are spread across multiple geographically distributed datacenters, there is an undefined ordering of events between events in two different datacenters.\\n  * When two events originate in different micro-services, then there is no defined order for those events.\\n* Deciding on a total order of events is known as a *total order broadcast*, which is equivalent to consensus. It\\'s an open research problem to design consensus algorithms that work well beyond a single node in a geographically distributed setting.\\n\\n###### Ordering events to capture causality\\n\\n* Where there is no causal link between events, the lack of a total order is not a big problem since concurrent events can be ordered arbitrarily.\\n* Logical timestamps can provide total ordering without coordination, so they may help in cases where total order broadcast is not feasible.\\n* If you can log an event to record the state of the system the user saw before making a decision, and give that event a unique identifier, then any later events can refer to that event identifier to record the causal dependency.\\n\\n##### Batch and Stream Processing\\n\\n* The goal of data integration is to ensure that data ends up in the right form in all the right places.\\n\\n###### Maintaining derived state\\n\\n* No matter what the derived data is, it\\'s helpful to think in terms of data pipelines that derive one thing from another, pushing state changes in one system through functional application code and applying the effects to derived systems.\\n* Asynchrony is what makes systems based on event logs robust, as a fault in one part of the system is contained locally.\\n\\n###### Reprocessing data for application evolution\\n\\n* With reprocessing it is possible to re-structure a dataset into a completely different model in order to better serve new requirements.\\n* Derived views allow *gradual* evolution. A gradual migration has little risk as you always have a working system to go back to.\\n\\n###### The lambda architecture\\n\\n* The lambda architecture records incoming data by appending immutable events to an always-growing dataset, from which read-optimized views are derived.\\n* A stream processor consumes the events and quickly produces an approximate update to a view. A batch processor later consumes the *same* set of events and produces a corrected version of the derived view.\\n* The stream processor can use fast approximate algorithms while the batch process uses slower exact algorithms.\\n* The lambda architecture has several practical problems:\\n  * The same logic must be implemented in both a batch and in a stream processing framework.\\n  * The batch pipeline and the stream pipeline outputs must be merged in order to respond to user requests.\\n  * Batch processing on large datasets is expensive, and so the batch pipeline often needs to process incremental batches rather than reprocessing everything, which adds complexity.\\n\\n#### Unbundling Databases\\n\\n* Relational databases want to give programmers a high-level abstraction that hide the complexities of data structures on disk, concurrency, crash recovery, and so on.\\n* The NoSQL movement wants to apply a Unix-esque approach of low-level abstractions to the domain of distributed OLTP data storage.\\n\\n##### Composing Data Storage Technologies\\n\\n###### The meta-database of everything\\n\\n* The dataflow across an entire organization looks like one huge database: Each batch, stream, or ETL process acts like a database subsystem that keeps indexes or materialized views up to date.\\n* There are two avenues by which different storage and processing tools can be composed into a cohesive system:\\n  * Federated databases (unifying reads): A *federated database* or *polystore* provides a unified query interface to a wide variety of storage engines and processing methods.\\n  * Unbundled databases (unifying writes): Data capture and event logs allow *unbundling* a database\\'s index-maintenance features in a way that can synchronize writes across disparate technologies.\\n\\n###### Making unbundling work\\n\\n* Asynchronous event logs with idempotent retries is a more robust and practical approach than distributed transactions across heterogeneous storage systems.\\n* Log-based integration is *loose coupling* between storage components:\\n  * Asynchronous event streams make the system as a whole more robust to outages or performance degradation of individual components.\\n  * Unbundling data systems allows different software components and services to be developed, improved, and maintained independently from each other by different teams.\\n\\n###### Unbundling versus integrated systems\\n\\n* Unbundling allows you to combine several different databases in order to achieve good performance for a much wider range of workflows than is possible with a single piece of software.\\n* The advantages of unbundling and composition only come into the picture when there is no single piece of software that satisfies all your requirements.\\n\\n##### Designing Applications Around Dataflow\\n\\n* The approach of unbundling databases by composing specialized storage and processing systems with application code is becoming known as the \"database inside-out\" approach.\\n\\n###### Application code as a derivation function\\n\\n* A dataset derived from another must go through some transformation function. Examples include a secondary index, a full-text search index, a machine learning model (derived from the training data), or a cache (aggregating data in a form for rendering in the UI).\\n\\n###### Separation of application code and state\\n\\n* In the typical web application model, the database acts as a kind of mutable shared variable that can be accessed synchronously over the network.\\n* Subscribing to changes in a database is only now emerging as a feature; historically databases have inherited a passive approach to mutable data that requires polling for changes.\\n\\n###### Dataflow: Interplay between state changes and application code\\n\\n* Instead of thinking of a database as a passive variable that is updated only by the application, we must think of application code responding to state changes in one place by updating state in another place.\\n* Unbundling the database applies this idea to derived datasets from the primary database, including caches, full-text search indexes, machine learning, or analytics systems.\\n* This requires stable message ordering and fault-tolerant message processing, but those are less stringent demands than those imposed by distributed transactions.\\n\\n###### Stream processors and services\\n\\n* The advantage of a service-oriented architecture over a single monolithic application is primarily organizational scalability through loose coupling.\\n* Instead of one service querying another service for data, it can subscribe to to state changes from the other service and query a local database for improved speed and reliability.\\n\\n##### Observed Derived State\\n\\n* The write path between a primary database and its derived dataset and the read path between that derived dataset and the point it is consumed represents the whole journey of the data.\\n* Framing the journey in terms of functional programming languages, the write path is similar to eager evaluation, while the read path is similar to lazy evaluation.\\n* The derived dataset is where the write path and read path meet, and represents a tradeoff between the amount of work that must be done at write time and the amount that must be done at read time.\\n\\n###### Materialized views and caching\\n\\n* Caches, indexes, and materialized views allow us to do more work on the write path and less work on the read path, thereby shifting the boundary between the write path and read path.\\n\\n###### Stateful, offline-capable clients\\n\\n* When we move from stateless clients to a model where end-user devices maintain state, we can think of the on-device state as a *cache of state on the server*.\\n\\n###### Pushing state changes to clients\\n\\n* Actively pushing state changes all the way to client devices means extending the write path all the way to the end user.\\n* When a client is first initialized, it would still need to use a read path to get its initial state, but thereafter it could rely on a stream of state changes sent by the server.\\n\\n###### End-to-end event streams\\n\\n* Extending the write path all the way to the end user requires moving away from request/response interaction and toward a publish/subscribe dataflow.\\n\\n###### Reads are events too\\n\\n* We can treat reads as events, and send them to the same stream processor as writes; the processor can respond to a read event by emitting the result of the read on the output stream.\\n* This performs a stream-table join between the stream of read queries and the database.\\n* Recording a log of read events has benefits with regard to tracking causal dependencies and data provenances, as you can reconstruct what a user saw before making some decision.\\n\\n###### Multi-partition data processing\\n\\n* Representing queries as events and collecting responses from streams enables executing queries across several partitions, leveraging the infrastructure for message routing, partitioning, and joining already provided by stream processors.\\n\\n#### Aiming for Correctness\\n\\n* We want to build applications that are reliable and *correct*, i.e. programs whose semantics are well defined and understood, even in the presence of faults.\\n* Serializability and atomic commits provide strong assurances of correctness, but typically only work in a single datacenter, and they limit the scale of fault-tolerance properties you can achieve.\\n\\n##### The End-to-End Argument for Databases\\n\\n###### Exactly-once execution of an operation\\n\\n* *Exactly-once* means arranging the computation such that the final effect is the same as if no faults had occurred, even if the operation was retried due to some fault.\\n* One of the most effective approaches is to make the operation *idempotent*.\\n\\n###### Duplicate suppression\\n\\n* Even if you suppress duplicate transactions between the database client and server, you must still worry about the network between the end-user device and the application server.\\n\\n###### Uniquely identifying requests\\n\\n* Relational databases can generally maintain a uniqueness constraint correctly (such as on an idempotence token), even at weak isolation levels.\\n\\n###### The end-to-end argument\\n\\n* The *end-to-end argument* says some problems can be completely and correctly implemented only with the help of the endpoints of the communication system; providing a solution as a feature of the communication system is not possible.\\n* In such circumstances, low-level reliability features are not by themselves sufficient to ensure end-to-end correctness.\\n\\n###### Applying end-to-end thinking in data systems\\n\\n* Reasoning about concurrency and partial failure is difficult and counterintuitive, and so most application-level mechanisms likely do not work correctly, thereby resulting in lost or corrupted data.\\n\\n##### Enforcing Constraints\\n\\n###### Uniqueness constraints require consensus\\n\\n* Enforcing a uniqueness constraint requires consensus, as the system must decide which one of the conflicting operations is accepted, and reject the others as violating the constraint.\\n* Uniqueness constraints can be scaled out by partitioning based on the value that needs to be unique.\\n* Asynchronous multi-master replication cannot be used in this case, as two different masters could concurrently accept conflicting writes.\\n\\n###### Uniqueness in log-based messaging\\n\\n* If a log is partitioned based on the value that needs to be unique, then a stream processor can unambiguously and deterministically decide which of several conflicting operations comes first in the log.\\n* To scale the number of handled requests, simply increase the number of partitions.\\n* Because all writes that may conflict are routed to the same partition and processed sequentially, this works for many constraints other than uniqueness constraints.\\n\\n###### Multi-partition request processing\\n\\n* Single-object writes are atomic in almost all data systems, and so a request appended to a log either appears in the log or it doesn\\'t.\\n* By breaking down a multi-partition transaction into partitioned stages using the same end-to-end request ID, you can achieve correctness even in the presence of faults without using an atomic commit protocol.\\n\\n##### Timeliness and Integrity\\n\\n* The term *consistency* conflates two requirements that are worth considering separately:\\n  * *Timeliness* means ensuring the data is in an up-to-date state. Read data may be stale but that inconsistency is only temporary.\\n  * *Integrity* means absence of corruption, and no contradictory or false data. If integrity is violated, then the inconsistency is permanent.\\n* Violations of timeliness are \"eventual consistency\" whereas violations of integrity are \"permanent inconsistency.\"\\n\\n###### Correctness of dataflow systems\\n\\n* ACID transactions usually provide both timeliness (e.g. linearizability) and integrity (e.g. atomic commit) guarantees.\\n* Event-based dataflow systems decouple timeliness and integrity: You forfeit timeliness because of asynchrony, but *exactly-once* or *effectively-once* semantics preserve integrity.\\n\\n###### Loosely interpreted constraints\\n\\n* If an application can get away with weaker notions of uniqueness, then persisting a *compensating transaction* may undo a constraint violation.\\n* In many business contexts, it may be acceptable to temporarily violate a constraint and fix it up later by apologizing. The cost of the apology is a business decision.\\n* Such applications that tolerate optimistic writes and checking the constraint afterward do require integrity, but don\\'t require timeliness on the enforcement of the constraint.\\n\\n###### Coordinating-avoiding data systems\\n\\n* Systems that synchronize cross-partition coordination or have strict constraints reduce the number of apologies you must make due to inconsistencies, but potentially also reduce your performance and availability.\\n\\n##### Trust, but Verify\\n\\n* The *system model* is the set of assumptions that certain things might go wrong, but other things won\\'t.\\n* Faults are really modeled by probabilities. The question is whether violations of our assumptions happen often enough that we may encounter them in practice.\\n* If you have enough devices running your software, then even very unlikely things do happen.\\n\\n###### Maintaining integrity in the face of software bugs\\n\\n* If the application uses the database incorrectly in some way, for example using a weak isolation level unsafely, then the integrity of the database cannot be guaranteed.\\n\\n###### Don\\'t blindly trust what they promise\\n\\n* Checking the integrity of data is known as *auditing*.\\n* HDFS and Amazon S3 run background checks that continually read back files, compare them to other replicas, and move files between disks, in order to mitigate the risk of silent corruption.\\n\\n###### Designing for auditability\\n\\n* Mutations to database tables do not detail *why* those mutations were performed. The invocation of the application logic that decided on those mutations is transient and cannot be reproduced.\\n* Being explicit about dataflow makes the *provenance* of data much clearer, which makes integrity checking much more feasible.\\n\\n###### The end-to-end argument again\\n\\n* Checking the integrity of data systems is best done in an end-to-end fashion: The more systems we can include in an integrity check, the fewer opportunities there are for corruption to go unnoticed at some stage of the process.\\n\\n###### Tools for auditable data systems\\n\\n* Blockchains and distributed ledgers are essentially distributed databases run by mutually untrusting organizations. These replicas check each other\\'s work and use a consensus protocol to agree on the transactions that should be executed.\\n\\n#### Doing the Right Thing\\n\\n* Data may be an abstract thing, but many datasets are about people: their behavior, their interests, their identity. We must treat such data with humanity and respect.\\n\\n##### Predictive Analytics\\n\\n* The criminal justice system may presume innocence until proven guilty, but automated systems can systematically and arbitrarily exclude a person from participating in society without any proof of guilt, and with little chance of appeal.\\n\\n###### Bits and discrimination\\n\\n* Predictive analytics systems do not merely automate a human\\'s decision by using software to specify the rules for when to say yes or no; instead we leave the rules themselves to be inferred from the data.\\n* If there is systematic bias in the input to an algorithm, the system will most likely learn and amplify that bias in its output.\\n* Predictive analytics systems merely extrapolate from the past; if the past is discriminatory, then they codify that discrimination.\\n\\n###### Responsibility and accountability\\n\\n* As data-driven decision making becomes more widespread, we must discover how to make algorithms accountable and transparent, how to avoid reinforcing existing biases, and how to fix them when they inevitably make mistakes.\\n\\n###### Feedback loops\\n\\n* Consequences such as feedback loops can be predicted by thinking about the entire system, including the people interacting with it ‚Äì an approach known as *systems thinking*.\\n\\n##### Privacy and Tracking\\n\\n* When a system only stores data that has been explicitly entered, then the system is performing a service for the user: The user is the customer.\\n* Tracking the user serves not the individual but the needs of advertisers who are funding the service. This relationship is appropriately described as *surveillance*.\\n\\n###### Surveillance\\n\\n* In our attempts to make software \"eat the world\" we have built the greatest mass surveillance infrastructure that the world has ever seen.\\n\\n###### Consent and freedom of choice\\n\\n* Most privacy policies do more to obscure than to illuminate. But without understanding what happens to their data, users cannot give any meaningful consent.\\n* Data is extracted from users through a one-way process, not a relationship with true reciprocity, and not a fair value exchange.\\n* For the user who does not consent to surveillance, the only real alternative is simply to not use the service, which may have real social cost.\\n* For people in a less privileged position, there is no meaningful freedom of choice: surveillance becomes inescapable.\\n\\n###### Privacy and use of data\\n\\n* Privacy does not mean keeping everything secret, but having the freedom to choose which things to reveal to whom, what to make public, and what to keep secret.\\n* When data is extracted from people through surveillance infrastructure, privacy rights are usually not eroded but rather transferred to the data collector.\\n* Even if particular users cannot be personally identified from the group targeted by a particular ad, they have lost their agency about the disclosure of some intimate information.\\n* Whether something is \"undesirable\" or \"inappropriate\" is down to human judgment; algorithms are oblivious to such notions unless we explicitly program them to respect human needs.\\n* Surveillance has always existed, but it used to be expensive and manual. Trust relationships have always existed, but the are mostly governed by ethical, legal, and regulatory constraints.\\n\\n###### Data as assets and power\\n\\n* Behavioral data is sometimes called \"data exhaust\" because it is a byproduct of users interacting with a service.\\n* From an economic point of view, if targeted advertising is what pays for a service, then behavioral data about people is the service\\'s core asset.\\n* Because data can be abused so easily, critics have said that data is not just an asset, but a \"toxic asset\" or at least a \"hazardous material.\"\\n\\n###### Remembering the Industrial Revolution\\n\\n* Just as the Industrial Revolution had a dark side that needed to be managed, our transition to the information age has major problems that we must confront and solve.\\n* As Bruce Schneier said, data is the pollution problem of the information age, and protecting privacy is the environmental challenge.\\n',\n",
       " '## Effective C++, Third Edition\\n\\nby Scott Meyers\\n\\n*I, [Michael Parker](http://omgitsmgp.com/), own this book and took these notes to further my own learning. If you enjoy these notes, please [purchase the book](http://www.amazon.com/Effective-Specific-Improve-Programs-Designs/dp/0321334876)!*\\n\\n### Chapter 1: Accustoming Yourself to C++\\n\\n#### Item 1: View C++ as a federation of languages\\n* There is C, object-oriented C++, templates and template metaprogramming, and the STL.\\n\\n#### Item 2: Prefer `const`, `enum`, and `inline` to `#define`\\n* Unlike when using a `#define`, if an error occurs when using a constant, then its name is included in the error message.\\n* For a constant pointer in the header file, declare both the pointer and the data it points to as `const`.\\n* In-class initialization is allowed only for integral types, and only for constants.\\n* When using a macro, remember to parenthesize all the arguments, and beware expressions being evaluated multiple times if used as arguments.\\n\\n#### Item 3: Use `const` whenever possible\\n* Using `const` is wonderful because it allows the compiler to enforce a semantic constraint.\\n* Declaring an `iterator` `const` means it isn\\'t allowed to point to something different, but whatever it points to may be modified.\\n* One of the hallmarks of good user-defined types is that they avoid gratuitous incompatibilities with built-in types.\\n* A `const` member function can overload a non-`const` member function, and the former will be used on `const` objects.\\n* Bitwise `const` is C++\\'s definition of `const`. Logical `const` is when bits in the object are changed, but in ways that the client cannot detect.\\n* The `mutable` keyword frees non-static data members from the constraints of bitwise `const`.\\n* By calling `static_cast` to add `const` to `this`, and then `const_cast` to remove `const` from the return value, the overloaded non-`const` function can call the `const` one.\\n\\n#### Item 4: Make sure that objects are initialized before they\\'re used\\n* Reading uninitialized values yields undefined behavior, so always initialize objects before you use them.\\n* Always listing every data member on the initialization list avoids having to remember which data members may go uninitialized.\\n* Within a class, data members are initialized in the order they\\'re declared, and not in their order on the initialization list.\\n* If a non-local static object in one translation unit uses a non-local static object in another translation unit, it may be uninitialized, because their initialization order is undefined.\\n* Using `static` objects defined in functions eliminates this problem, and if you never call such a function, you don\\'t even construct its `static` object.\\n\\n### Chapter 2: Constructors, Destructors, and Assignment Operators\\n\\n#### Item 5: Know what functions C++ silently writes and calls\\n* A compiler declares a copy constructor, copy assignment operator, and destructor if you don\\'t declare them yourself, as well as a default constructor if you declare none.\\n* The generated destructor is not virtual unless the class inherits from a base class with a virtual destructor.\\n* You must define the copy constructor yourself if the class contains a reference member or a `const` member.\\n\\n#### Item 6: Explicitly disallow the use of compiler-generated functions you do not want\\n* Declare the copy constructor and the copy assignment operator private to prevent the compiler from generating its own version.\\n* To stop member and friend functions from still calling them, don\\'t actually define them; this generates an error during the linking stage.\\n\\n#### Item 7: Declare destructors virtual in polymorphic base classes\\n* When a derived class object is deleted through a pointer to the base class with a non-virtual destructor, results are undefined, but typically the derived part isn\\'t destroyed.\\n* If the class isn\\'t intended to be a base class, making the destructor virtual increases its size, as this adds a `vptr` (virtual table pointer) and `vtbl` (virtual table).\\n* The `string` class and STL container types (`vector`, `list`, `set`, etc.) lack virtual destructors, and so should never be inherited from.\\n\\n#### Item 8: Prevent exceptions from leaving destructors\\n* Depending on the circumstances, if two destructors simultaneously emit exceptions, program execution either terminates or yields undefined behavior.\\n* One option is to terminate the program in the destructor, thereby preventing any undefined behavior.\\n* The second option is to swallow the exception, but only if the program can reliably continue after the exception was ignored.\\n* Good practice is to try and move the operation that can generate an exception to outside the destructor.\\n\\n#### Item 9: Never call virtual functions during construction or destruction\\n* During base class construction, virtual function calls never go down into a derived class, because an object is not of a derived class until its constructor begins execution.\\n* The same reasoning applies during destruction.\\n* You must ensure that your constructors or destructors don\\'t call virtual functions, and that all the functions they call obey the same constraint.\\n\\n#### Item 10: Have assignment operators return a reference to `*this`\\n* This is what all built-in types do, thereby allowing chained assignments, and it applies to all assignment operators (such as `operator+=`).\\n\\n#### Item 11: Handle assignment to self in `operator=`\\n* Code that operates on references or pointers to multiple objects of the same type must consider that the objects might be the same.\\n* Guard against self-assignment by checking the argument\\'s address against `this` at the top of `operator+=`.\\n* In many cases, a careful ordering of statements can yield code that guards against both exceptions and self-assignment.\\n* Another alternative that guards against both exceptions and self-assignment is the \"copy and swap\" technique, which is covered in Item 29.\\n\\n#### Item 12: Copy all parts of an object\\n* When you add new data members to a class, be sure to update its copy constructor and copy assignment operator accordingly.\\n* A copying function should copy all local data members, and also invoke the appropriate copying function in all base classes.\\n\\n### Chapter 3: Resource Management\\n\\n#### Item 13: Use objects to manage resources\\n* A thrown exception or a premature `return`, `continue`, or `goto` statement might preclude execution from eaching a `delete` statement.\\n* By putting resources inside objects like `auto_ptr`, we can rely on C++\\'s destructor invocation to ensure that the resources are released.\\n* Resource Acquisition Is Initialization, or RAII, means acquiring a resource and initializing its managing object happen in the same statement.\\n* Copying an `auto_ptr` sets it to null. While this enforces an object being managed by only one `auto_ptr`, you cannot use them in STL containers.\\n* There is no `auto_ptr` or `shared_ptr` for dynamically allocated arrays because you should be using `vector` instead.\\n\\n#### Item 14: Think carefully about copying behavior in resource-managing classes\\n* For resources that are not heap-based, smart pointers like `auto_ptr` and `shared_ptr` are inappropriate as resource handlers.\\n* Policies for copying an RAII object include prohibiting copying, and reference counting, copying, and transferring ownership of the underlying resource.\\n\\n#### Item 15: Provide access to raw resources in resource-managing classes\\n* An implicit conversion function on the RAII class can make access to the raw resource easier, but this increases the chance of errors.\\n* Often an explicit conversion function simply named `get` is preferable, even if clients must explicitly call it each time.\\n* Returning the raw resource violates encapsulation, but RAII classes don\\'t exist simply to encapsulate it, but to ensure that it is released.\\n\\n#### Item 16: Use the same form in corresponding uses of `new` and `delete`\\n* Using the expression `delete` when `delete []` should be used results in undefined behavior.\\n* The memory for an array usually includes the size of the array, so that the `delete` operator knows how many destructors to call.\\n* Use the same form of `new` in all constructors that initialize a pointer member, or else you may use the wrong form of `delete`.\\n* The `string` and `vector` classes largely eliminate the need to dynamically allocate an array.\\n\\n#### Item 17: Store `new` objects in smart pointers in standalone statements\\n* Compilers are given less leeway in reordering operations across statements than within them.\\n\\n### Chapter 4: Designs and Declarations\\n\\n#### Item 18: Make interfaces easy to use correctly and hard to use incorrectly\\n* The type system is your primary ally in preventing undesirable code from compiling.\\n* Restrict what can be done with a type. A common way to impose restrictions is to add `const` wherever you can.\\n* Avoid gratuitous incompatibilities with the built-in types so that interfaces behave consistently, thereby reducing mental friction.\\n* Any interface that requires clients to remember to do something is prone to incorrect use, because clients can forget to do it.\\n* In many applications, the additional runtime costs of resource managers are unnoticeable, but the reduction in client errors will be noticed by everyone.\\n\\n#### Item 19: Treat class design as type design\\n* Approach class design with the same care that language designers lavish on the design of built-in types.\\n* Good types have a natural syntax, intuitive semantics, and one or more efficient implementations.\\n* If you inherit from existing classes, you are constrained by their design, particularly by whether their functions are virtual or not.\\n* Guarantees made with respect to performance, exception safety, and resource usage impose constraints on your implementation.\\n* If you\\'re defining a whole family of types, you don\\'t want to define a new class, you want to define a new class template.\\n* If you\\'re only subclassing so you can add functionality to an existing class, consider non-member functions or templates instead.\\n\\n#### Item 20: Prefer pass-by-reference-to-`const` to pass-by-value\\n* Passing by reference eliminates the *slicing problem*, where passing a derived class object to a function that accepts a base class object calls the base class copy constructor.\\n* References are typically implemented as pointers, so passing built-in types like `int` by value is usually more efficient.\\n* Implementers of iterators and function objects ensure that they are efficient to copy and are not subject to the slicing problem.\\n* Avoid passing a user-defined type by value because while its size may be small now, that is subject to change with its implementation.\\n\\n#### Item 21: Don\\'t try to return a reference when you must return an object\\n* A function should never return a reference or pointer to a local object that is destroyed when the function exits.\\n* Returning a reference from a function like `operator*` is incorrect. Such a function must return a new object.\\n* In some cases the construction and destruction of such a return value can be safely eliminated by the compiler.\\n\\n#### Item 22: Declare data members private\\n* Many data members should be hidden, and rarely does every data member require a getter and a setter.\\n* Only functional interfaces makes it easy to notify other objects when variables are read or written, to verify class invariants and function pre- and postconditions, to perform synchronization in threaded environments, etc.\\n* Public means unencapsulated, which means an unchangeable implementation, especially for classes that are widely used.\\n* Protected data is as unencapsulated as public data, since changing such a data member could break all derived classes that use it, which is an unknowably large amount of code.\\n\\n#### Item 23: Prefer non-member non-friend functions to member functions\\n* The more functions that can access data, the less the data is encapsulated, and the harder it is to change the characteristics of the data.\\n* Unlike a member function, a non-member non-friend function doesn\\'t increase the count of functions that can access the private parts of a class.\\n* Consider putting the non-member function in the same namespace as the class it operates on.\\n* Partitioning functionality in a namespace across multiple files promotes clean organization, and clients can freely add more functions to the namespace.\\n\\n#### Item 24: Declare non-member functions when type conversions should apply to all parameters\\n* For overloaded operators, compilers will also look at non-member functions accepting two parameters in the namespace or global scope.\\n* Parameters are eligible for implicit type conversion only if they are listed on the parameter list. The object on which the member function is invoked is never eligible.\\n* To support mixed mode arithmetic with operator overloading, make operators non-member functions accepting both operands as arguments.\\n* The opposite of a member function is a non-member function, not a friend function. Avoid friend functions when you can.\\n\\n#### Item 25: Consider support for a non-throwing `swap`\\n* If using the \"pimp idiom,\" define a member function named `swap` that does the actual swapping, then specialize `std::swap` to call that member function.\\n* It\\'s okay to totally specialize templates in `std`, adding new templates, classes, functions, or anything else to `std` results in undefined behavior.\\n* In addition to the specialization of `std::swap`, write a non-member version of `swap` in the same namespace of your class.\\n* By prefacing a call to `swap` with `using std::swap`, compilers will look for the namespace definition first, then the specialization in `std`, and finally the general form.\\n\\n### Chapter 5: Implementations\\n\\n#### Item 26: Postpone variable definitions as long as possible\\n* Postponing declaring a variable until you have its initialization arguments avoids unnecessary default constructions.\\n* Assigning a variable defined outside a loop is more efficient than initializing it on ever iteration, because it avoids destructing when each iteration completes.\\n\\n#### Item 27: Minimize casting\\n\\n* Casts can subvert the type system, which is there to ensure that you\\'re not trying to perform any unsafe or nonsensical operations on objects.\\n* Only use `reinterpret_cast` for low-level casts in low-level code, such as from a pointer to an `int`. It yields unportable results.\\n* Use `static_cast` to force implicit conversions as well as the reverse of such conversions, with the exception of `const` to non-`const`.\\n* Prefer the explicit, new-style casts. They are easier to search for, and their narrow purpose makes it possible for compilers to diagnose usage errors.\\n* Type conversions of any kind, either explicit via casts or implicit by compilers, often lead to additional code that is executed at runtime.\\n* Avoid making assumptions about how things are laid out in C++. Making casts based on such assumptions leads to undefined behavior.\\n* A `dynamic_cast` can have a significant runtime cost. If you need to perform derived class operations through a pointer or reference to the base class, explore alternative designs.\\n* Casts should be isolated as much as possible, typically hidden inside functions whose interfaces shield callers from the work done inside.\\n\\n#### Item 28: Avoid returning \"handles\" to object internals\\n* A data member is only as encapsulated as the most accessible function returning a reference to it.\\n* Returning `const` references to data members can still lead to *dangling handles*, which refer to parts of objects that don\\'t exist any longer.\\n* Functions that return a handle to an object internal are the exception and not the rule.\\n\\n#### Item 29: Strive for exception-safe code\\n* Exception-safe functions don\\'t leak resources, and don\\'t allow data structures or objects to enter a corrupted state.\\n* The *basic guarantee* ensures that the program remains in a valid state if an exception is thrown, but that exact state may ot be predictable. The *strong guarantee* ensures that the state is unchanged.\\n* Often you can\\'t guarantee that no exceptions are thrown, because anything that dynamically allocates memory can throw a `bad_alloc` exception.\\n* When functions have side-effects on non-local data instead of operating exclusively on local state, it\\'s much harder to offer the strong guarantee.\\n* If a system has even a single function that\\'s not exception-safe, then the system as a whole is not exception-safe.\\n* A function\\'s exception-safety guarantee is a visible part of its interface, and you should choose it as deliberately as you choose all other interface aspects.\\n\\n#### Item 30: Understand the ins and outs of inlining\\n* Compiler optimizations are typically designed for stretches of code that lack function calls, so inlining allows more optimizations.\\n* Inlined code can also lead to additional paging, a reduced instruction cache hit rate, and their accompanying performance penalties.\\n* The `inline` keyword is a request that compilers may ignore, and only the most trivial functions that are not virtual may be inlined.\\n* Even empty constructors and destructors are unlikely to be inlined, as they implicitly call the constructors of base classes and data members.\\n* Debuggers have trouble with inlined functions. For example, you can\\'t set a breakpoint in a function that isn\\'t there.\\n\\n#### Item 31: Minimize compilation dependencies between files\\n* Instead of trying to forward-declare parts of the standard library, use the proper `#include` statements and be done with it.\\n* You never need a class definition to declare a function using that class. Forward declare the class, and shift the burden of including its definition to clients that call the function.\\n* A class that employs the pimpl idiom is often called a *handle class*.\\n* If a function is declared as pure virtual in an interface class, then there is no need to include the keyword `virtual` in its declaration in the subclass.\\n* Handle classes and interface classes decouple interfaces from implementations and thereby reduce compilation dependencies between files.\\n\\n### Chapter 6: Inheritance and Object-Oriented Design\\n\\n#### Item 32: Make sure public inheritance models \"is-a\"\\n* The most important rule in object-oriented programming in C++ is that public inheritance means \"is-a\".\\n* There is no one ideal design for all software. The best design depends on what the system is expected to do, both now and in the future.\\n* Good interfaces prevent invalid code from compiling. Prefer design that rejects operations during compilation than one that rejects at runtime.\\n* A class named `Square` extending `Rectangle` is a classic example of the fragile nature of class hierarchies.\\n\\n#### Item 33: Avoid hiding inherited names\\n* If a function in a derived class has the same name as a function in the base class, it hides all overloaded forms of that function in the base class.\\n* To preserve the is-a relationship of inheritance, the derived class must include a `using` declaration to inherit all overloaded forms of a function with a given name.\\n\\n#### Item 34: Differentiate between inheritance of interface and inheritance of implementation\\n* Pure virtual functions must be redeclared by any concrete class that inherits them.\\n* A simple virtual function allows a derived class to inherit a function interface as well as an implementation.\\n* A pure virtual function can still have an implementations of its own. A subclass must redeclare it, but it can call down to this \"default\" implementation.\\n* A non-virtual function serves a mandatory implementation, and should never be redefined in a derived class.\\n* Don\\'t blindly declare all member functions virtual, and don\\'t blindly declare all member functions non-virtual. Consider each one individually.\\n\\n#### Item 35: Consider alternatives to `virtual` functions\\n* When a non-virtual member function calls a private virtual member function, subclasses can redefine the latter. This is a form of the template method design pattern.\\n* The only way to resolve the need for non-member functions to access the non-public parts of a class is to weaken its encapsulation.\\n* The `tr1::function` class can refer to anything that *acts* like a function and returns a type *convertible* to the specified type.\\n* The \"standard\" strategy pattern offers the possibility that an existing strategy can be tweaked via defining a subclass.\\n\\n#### Item 36: Never redefine an inherited non-virtual function\\n* Non-virtual functions are statically bound, so calling one on a base class pointer or reference uses the base class implementation, and not any derived class implementation.\\n* A non-virtual function is an invariant over specialization for the base class, and so no derived class should try to redefine it.\\n* Item 7, which warns against not specifying a virtual destructor in a base class, is a special case of this item.\\n\\n#### Item 37: Never redefine a function\\'s inherited default parameter value\\n* An object\\'s dynamic type is determined by the object to which it currently refers, which in turn determines how it will behave.\\n* Default parameters are statically bound, so invoking a virtual function defined in a derived class uses a default parameter value from the base class.\\n* To avoid duplication of the default parameter, use the non-virtual interface idiom, where the default parameter is only defined once in the public non-virtual function.\\n\\n#### Item 38: Model \"has-a\" or \"is-implemented-in-terms-of\" through composition\\n* Composition in the application domain expresses a has-a relationship, while in the implementation domain it expresses an is-implemented-in-terms-of relationship.\\n* Inappropriate subclassing violates the is-a principle, and should be replaced with composition.\\n\\n#### Item 39: Use private inheritance judiciously\\n* With private inheritance, you cannot assign a derived object to a base class pointer, and protected and public members in the base class are private in the derived class.\\n* Use composition whenever you can, and use private inheritance whenever you must.\\n* Use private inheritance if two classes don\\'t have an is-a relationship, but one needs to access the protected members or redefine a virtual function in the other.\\n* If a class privately inherits from another and defines a virtual function, its own subclasses can redefine that function, even if they can\\'t call it.\\n\\n#### Item 40: Use multiple inheritance judiciously\\n* If a function is defined in two base classes, then a call in the derived class to that function is always ambiguous, even if only one definition is accessible.\\n* Virtual inheritance prevents replicating data in the base class when multiple inheritance is used, but it\\'s slower and creates larger objects.\\n* The one reasonable of multiple inheritance is to combine public inheritance of an interface with private inheritance of an implementation.\\n* If the only design you can come up with involves multiple inheritance, you should think a little harder.\\n\\n',\n",
       " '## Effective Java, 2nd Edition\\n\\nby Joshua Bloch\\n\\n*I, [Michael Parker](http://omgitsmgp.com/), own this book and took these notes to further my own learning. If you enjoy these notes, please [purchase the book](http://www.amazon.com/Effective-Java-Edition-Joshua-Bloch/dp/0321356683)!*\\n\\n### Chapter 2: Creating and Destroying Objects\\n\\n#### Item 1: Consider static factories instead of constructors\\n* An instance-controlled class is one that uses static factories to strictly control what instances exist at any time.\\n* By convention, static factory methods for an interface named `Type` are put in a non-instantiable class named `Types`.\\n* When naming static factory methods, `getInstance` may return the same instance, while `newInstance` should not.\\n\\n#### Item 2: Consider a builder when faced with many constructor parameters\\n* The builder pattern simulates optional named parameters in Ada and Python.\\n* A builder whose parameters have been set makes a fine abstract factory, assuming some generic `Builder<T>` interface.\\n\\n#### Item 3: Enforce the singleton property with a private constructor or an enum type\\n* Adding `implements Serializable` to a singleton class is not enough, you must declare all fields `transient` and provide a `readResolve` method.\\n* A single-element `enum` type provides the serialization for free and is the best way to implement a singleton.\\n\\n#### Item 4: Enforce non-instantiability with a private constructor\\n* A private constructor not only supresses instantiation, but subclassing.\\n\\n#### Item 5: Avoid creating unnecessary objects\\n* Often lazy initialization only complicates the implementation and yields no noticeable performance increase.\\n* Prefer primitives to boxed primitives, as unintentional autoboxing can lead to creating many new instances.\\n* Highly optimized garbage collectors can easily outperform object pools that do not contain heavyweight objects.\\n\\n#### Item 6: Eliminate obsolete object references\\n* Whenever a class manages its own memory, like a stack or object pool, the programmer should be alert for memory leaks.\\n* Caches, listeners, and callbacks can all be sources of memory leaks, but weak references can help.\\n\\n#### Item 7: Avoid finalizers\\n* Do not think of finalizers as Java\\'s analogue of C++ destructors -- there\\'s no guarantee finalizers will be called at all!\\n* If an uncaught exception is thrown in a finalizer, it is ignored, and the finalization abruptly terminates.\\n* There is a severe performance penalty for using finalizers -- object creation and deletion increases from nanoseconds to microseconds.\\n\\n### Chapter 3: Methods Common to All Objects\\n\\n#### Item 8: Obey the general contract when overriding `equals`\\n* Override `equals` when a class has a notion of logical equality that differs from mere object identity, and the superclass has not provided a suitable implementation.\\n* For classes that represent a value, such as `Integer` or `Date`, the `equals` method should always be overridden.\\n* There is no way to extend an instantiable class and add a value component (field) while preserving the `equals` contract.\\n* By using composition with views to internal components, you can add value components to instantiable classes without violating the `equals` contract.\\n* To compare `float` and `double` values, use the `Float.compare` and `Double.compare` methods to deal with `NaN` and `-0.0` values.\\n* For best performance, first compare fields that are more likely to differ, or less expensive to compare.\\n\\n#### Item 9: Always override `hashCode` when you override `equals`\\n* To take the hash code of `float` and `double` values, use `Float.floatToIntBits` and `Double.doubleToLongBits`, respectively.\\n* Immutability offers the chance to cache hash codes if computing them is expensive.\\n* Try not to specify the behavior of your hash code method in Javadoc, as that limits your options for improving it later.\\n\\n#### Item 10: Always override `toString`\\n* If you specify the format in Javadoc, provide a static factory method accepting a `String` parameter so a client can convert between the two forms.\\n* Provide programmatic information to all the information provided by `toString`, or clients may try to parse the string to retrieve it.\\n\\n#### Item 12: Consider implementing `Comparable`\\n* Like the `equals` method, there is no way to extend and instantiable class with a new value component while preserving the `compareTo` contract.\\n* If `compareTo` is consistent with `equals`, note that sorted collections (e.g. `TreeSet`, `TreeMap`) use the equality test imposed by `compareTo` instead of `equals` and may break the interface (e.g. `Set`, `Map`) contract.\\n* Use methods `Double.compare` and `Float.compare` instead of relational operators, which don\\'t obey the `compareTo` contract for floating point values.\\n\\n### Chapter 4: Classes and Interfaces\\n\\n#### Item 13: Minimize the accessibility of classes and members\\n* Private and package-private members can \"leak\" into the exported API if the class implements Serializable.\\n* Even a protected member is part of the class\\'s exported API and must be supported forever.\\n* With the exception of `public static final` fields to immutable objects, public classes should have no public fields.\\n\\n#### Item 14: In public classes, use accessor methods, not public fields\\n* If a class is package-private or a private nested class, there\\'s nothing wrong with exposing its data.\\n* A public class with immutable public fields is okay because it can enforce their invariants upon construction.\\n\\n#### Item 15: Minimize mutability\\n* For immutable classes, the Java memory model requires that all fields be `final` to ensure correct behavior when passing an instance between threads without synchronization.\\n* Defend against \"leaking\" references by making defensive copies in constructors, accessors, and `readResolve` methods when needed.\\n* Instances of an immutable class can share internal objects with one another for efficiency.\\n* If a client requires performing expensive multi-stage operations on your class, expose them as primitive methods, or provide a mutable companion class (like `StringBuilder` for `String`).\\n\\n#### Item 16: Favor composition over inheritance\\n* Inheritance violates encapsulation because the subclass depends on the implementation details of the superclass for its proper function.\\n* Inheritance means you inherit the scope and flaws of an API, whereas composition allows you to design a better suited one.\\n* If an appropriate interface exists, using composition and forwarding allows you to instrument any implementation of the interface, instead of a single implementation through inheritance.\\n\\n#### Item 17: Design and document for inheritance, or else prohibit it\\n* The class that allows subclassing must document its self-use of overridable methods.\\n* The only way to test if a class is suitably designed for inheritance (e.g. provides all the necessary implementation hooks through protected methods) is to actually write subclasses.\\n* Eliminating a class\\'s self use of methods, typically through introducing private helper methods, can make a class safe to subclass.\\n\\n#### Item 18: Prefer interfaces to abstract classes\\n* Use interfaces to allow construction of non-hierarchical type frameworks.\\n* Simulated multiple inheritance is where a class implementing an interface can forward invocations to an instance of a private inner class that extends the skeletal implementation of that interface and hence does the bulk of the work.\\n* A variant of a skeletal implementation is the simple implementation, which is a concrete class that defines the simplest possible implementation, such as `AbstractMap.SimpleEntry`.\\n\\n#### Item 19: Use interfaces only to define types\\n* Don\\'t use constant interfaces, or interfaces that define no methods but only constants, which classes implement to access the constants without fully qualifying their names.\\n* If the constants are static members of a class, and you really don\\'t want to qualify their names, use the `static import` facility for brevity.\\n\\n#### Item 21: Use function objects to represent strategies\\n* Classes that implement concrete strategies, or simulate function pointers, should be stateless and made into singletons.\\n* When defining a strategy as an anonymous class that is inline in a method invocation, consider extracting the object as a `private static final` field so a new instance is not created upon every call.\\n\\n#### Item 22: Favor static member classes over nonstatic\\n* Nonstatic member classes are ideal for providing adapters, or views of an outer class as an instance of some unrelated class.\\n* An anonymous class have enclosing instances if and only if they occur in a non-static context.\\n\\n### Chapter 5: Generics\\n\\n#### Item 23: Don\\'t use raw types in new code\\n* `List<E>` is read as \"list of E\", and `List<String>` is read as \"list of string\", where `String` is the actual type parameter and `E` is the formal type parameter.\\n* While an instance of the raw type `List` could be designated to hold only types of a single class but opts out of type-checking, `List<Object>` is typesafe because it explicitly states that it can contain objects of any type.\\n* The unbound wildcard type, such as in `List<?>`, represents a list of some unknown type and so forbids inserting any element other than `null`, unlike the raw type `List` which is not typesafe.\\n\\n#### Item 24: Eliminate unchecked warnings\\n* Always use the `@SuppressWarnings` annotation on the smallest scope possible, to not mask other, critical warnings.\\n\\n#### Item 25: Prefer lists to arrays\\n* Arrays are covariant, so `Sub[]` is a subtype of `Super[]`, and reified, so they retain their type-information at runtime and `Super[]` can throw an `ArrayStoreException` when given a different subclass; by contrast, `List<E>` is invariant and erased.\\n* Consequently, arrays provide runtime safety but not compile-time safety, while a generic type like `List<E>` provides compile-time safety but not runtime safety.\\n\\n#### Item 27: Favor generic methods\\n* You can exploit the type inference provided by generic methods and write generic static factory methods that make instances easier to create.\\n* If you have an immutable, singleton instance of a generic class that could be shared across all types, make it private and of type `Object`, and then let a generic singleton factory method cast it to the caller\\'s desired type.\\n* The type bound `<T extends Comparable<T>>` may be read as \"for every type T that can be compared to itself\" and is the definition of a mutually comparable type.\\n\\n#### Item 28: Use bounded wildcards to improve API flexibility\\n* If a parameterized type represents a `T` producer, use `<? extends T>`; if a parameterized type represents a `T` consumer, use `<? super T>`.\\n* Do not use wildcard types as return types, or clients will be forced to use wildcard types in their code.\\n* Comparables and comparators are always consumers, so you should always use `Comparator<? super T>` in preference to `Comparator<T>`.\\n* If a type parameter appears only once in a method declaration, replace it with a wildcard, using a private helper method to capture the type if necessary.\\n\\n#### Item 29: Consider typesafe heterogeneous containers\\n* A type token is a class literal is passed among methods to communicate both compile-time and runtime type information.\\n* The cast method of the `Class` type is the dynamic analog of Java\\'s cast operator, throwing a `ClassCastException` if the operation fails.\\n* The \"checked\" collection wrappers in `java.util.Collections` use this method with type tokens to enforce, at runtime, that invalid types are not added to a collection through its raw type.\\n\\n### Chapter 6: Enums and Annotations\\n\\n#### Item 30: Use `enum` instead of `int` constants\\n* You can add or reorder constants in an `enum` type without recompiling its clients because the constant values are not compiled into the clients as they are with the int enum pattern.\\n* Enums are by their nature immutable, and so all their fields should be final.\\n* If you override the `toString` method of an `enum` type, consider writing a `fromString` method, similar to how the static `valueOf` method would perform if you had not overridden `toString`.\\n* To share a constant specific method implementations between `enum` values, move each implementation into a private nested `enum`, and pass an instance of this strategy enum to the constructor of the top-level `enum`.\\n\\n#### Item 32: Use `EnumSet` instead of bit fields\\n* An `EnumSet` is represented by one or more `long` values, and so many of its operations are effiiciently performed with bitwise arithmetic.\\n\\n#### Item 33: Use `EnumMap` instead of ordinal indexing\\n* When you access an array that is indexed by the ordinal value of an `enum`, it is your responsibility to use the correct `int` value, as no type safety is afforded.\\n* An `EnumMap` contains an array internally, offering the speed of an ordinal-indexed array with the type safety and richness of the `Map` interface.\\n* Instead of using an array of arrays to define a mapping from two enum values, use `EnumMap<..., EnumMap<...>>`, which is internally represented as an array of arrays.\\n\\n#### Item 34: Emulate extensible enums with interfaces\\n* If two enums implement the same interface, both their classes adhere to the type `<T extends Enum<T> & InterfaceName>`.\\n* Since implementations cannot be inherited from one `enum` type to another, the functionality must be encapsulated in a helper class or a static helper method.\\n\\n#### Item 35: Prefer annotations to naming patterns\\n* Annotations like `Retention` and `Target` for annotation type declarations are called meta-annotations.\\n* A marker annotation is one with no parameter and simply serves to mark some class, method, or field for interpretation by some other method or program.\\n\\n#### Item 36: Consistently use the `Override` annotation\\n* There is no need to use the `@Override` annotation when a concrete class overrides an abstract method, i.e. implements it, because a differing signature will be caught by the compiler anyway.\\n* In an abstract class or interface, annotate all methods you believe to override superclass or superinterface methods, whether concrete or abstract, to ensure that you don\\'t accidentally introduce any new methods.\\n\\n#### Item 37: Use marker interfaces to define types\\n* Use a marker interface instead of an annotation if you want to write one or more methods that accept only objects that have this marking, or implement the interface.\\n* Use a marker interface instead of an annotation if you want to limit the use of the marker to elements of a particular interface, by having the marker interface extend that interface.\\n* A marker annotation, however, allows marking elements other than classes and interfaces, and allows adding more information while retaining backwards compatibility through type elements with defaults.\\n\\n### Chapter 7: Methods\\n\\n#### Item 38: Check parameters for validity\\n* Nonpublic methods should check their parameters using assertions, which are enabled with the `-enableassertions` command line flag, instead of explicitly throwing exceptions.\\n* Skip checking a method\\'s parameters before performing the computation if the validity check would be expensive or impractical and the validity check is performed implicitly during the computation.\\n\\n#### Item 39: Make defensive copies when needed\\n* When making defensive copies of constructor parameters, check the validity of the copies instead of the originals to guard against malicious changes to the parameters by another thread.\\n* Do not use the `clone` method to make a defensive copy of a constructor parameter whose type is subclassable by untrusted parties.\\n* The defensive copy can be replaced by documented transfer of ownership if copying the object would be costly and the class trusts its clients not to modify the components inappropriately.\\n\\n#### Item 40: Design method signatures carefully\\n* If you go over four parameters, try to break up the method into orthogonal methods with fewer parameters, introduce helper classes that bundle related parameters together, or use a builder pattern.\\n* Prefer two-element `enum` types to boolean parameters.\\n\\n#### Item 41: Use overloading judiciously\\n* Beware that selection among overloaded methods is static, while selection among overridden methods is dynamic.\\n* Avoid cases where the same set of parameters can be passed to different overloadings of a method by the addition of casts.\\n* If a method with a primitive parameter overloads a method with a generic parameter, the two can become conflated from autoboxing.\\n\\n#### Item 42: Use varargs judiciously\\n* Don\\'t blindly retrofit every method that has a final array parameter to use varargs; use varargs only when a call operates on a variable-length sequence of values.\\n\\n#### Item 43: Return empty arrays or collections, not nulls\\n* Returning `null` from an array or collection-valued method complicates the caller logic, and usually the callee\\'s.\\n* To eliminate the overhead from creating an empty collection or array, return the immutable empty collections in `java.util.Collections`, or create a static empty array which is necessarily immutable.\\n\\n#### Item 44: Write doc comments for all exposed API elements\\n* To include a multiline code example in a doc comment, use a Javadoc `{@code}` tag wrapped inside an HTML `<pre>` tag.\\n* The `{@literal}` tag is like the `{@code}` tag in that it eliminates the need to escape HTML metacharacters, but doesn\\'t render the contents in monospaced font.\\n* No two members or constructors should have the same summary description, which is the first (sometimes incomplete) sentence of a doc comment.\\n\\n### Chapter 8: General Programming\\n\\n#### Item 45: Minimize the scope of local variables\\n* Declare a variable at the latest point possible, typically right before it is first used, and strive to provide an initializer.\\n* If the bound for a loop variable is expensive to compute on every iteration, store it in a loop variable that will fall out of scope with the counter variable.\\n\\n#### Item 46: Prefer for-each loops to traditional for loops\\n* If you are writing a type that represents a group of elements, have it implement the `Iterable` interface even if it does not implement `Collection`.\\n* The enhanced `for` loop cannot be used if you need to remove elements through an iterator, reassign elements through a list iterator, or iterate over collections in parallel.\\n\\n#### Item 47: Know and use the libraries\\n* Every programmer should be familiar with the contents of `java.lang` and `java.util` (particularly the collections framework), and to a lesser extent `java.io` and `java.util.concurrent`.\\n\\n#### Item 48: Avoid `float` and `double` if exact answers are required\\n* The `BigDecimal` can contain decimal values of arbitrary size and provides eight rounding modes, which is ideal for business calculations with legally mandated rounding behavior.\\n* If you choose to keep track of the decimal point yourself, `int` provides up to nine decimal digits, while `long` provides up to eighteen.\\n\\n#### Item 49: Prefer primitive types to boxed primitives\\n* Never use `==` on two boxed primitives, because this always performs identity comparison, while the `<` and `>` operators compare the underlying primitive values.\\n* When you mix primitives and boxed primitives in a single operation, the boxed primitive is auto-unboxed, which can result in `NullPointerExceptions`.\\n* Beware of boxed primitives being implicitly unboxed and then re-boxed, which can cause performance problems.\\n\\n#### Item 50: Avoid strings where other types are appropriate\\n* Instead of using a key to represent an aggregate type, write a private static member class -- even if it only has two fields.\\n\\n#### Item 51: Beware the performance of string concatenation\\n* A consequence of strings being immutable is that the time to concatenate *n* strings is quadratic in *n*.\\n* An alternative to using a `StringBuilder` is to try processing the strings one at a time to avoid all concatenation.\\n\\n#### Item 52: Refer to objects by their interfaces\\n* If you depend on any properties of an implementation not specified by its interface, such as its synchronization policy, use the class as a type and document the requirements.\\n\\n#### Item 53: Prefer interfaces to reflection\\n* If a class is unavailable at compile time but there exists an appropriate interface, create instances reflectively and access them normally through their interface.\\n* If writing a package that runs against multiple versions of some other package, you can compile it against the minimal environment required to support it, and access any newer classes or methods reflectively.\\n\\n#### Item 55: Optimize judiciously\\n* Strive for encapsulation so that a part of the system can be rewritten for performance without changing its other parts.\\n* You need to measure attempted optimization carefully on the Java platform, because the language does not have a strong performance model, or well-defined relative costs.\\n\\n#### Item 56: Adhere to generally accepted naming conventions\\n* Components of a package name should be short, generally eight or fewer characters, where abbreviations are encouraged and acronyms are acceptable.\\n* Names of `static final` fields whose values are immutable should be in uppercase with words separated by underscores.\\n* Methods that convert a type have the format `toType`, while methods that return a view have the format `asType`, and methods that return a primitive representation have the format `typeValue`.\\n\\n### Chapter 9: Exceptions\\n\\n#### Item 57: Use exceptions for exceptional conditions\\n* Do not force clients to use exceptions for ordinary control flow, and instead offer methods to test whether an exception could be thrown, such as `hasNext` for class `Iterator`.\\n* Use a distinguished return value, like `null`, if the object is accessed by multiple threads or if the state-testing method duplicates the work of the state-dependent method.\\n\\n#### Item 58: Use checked exceptions for recoverable conditions and runtime exceptions for programming errors\\n* Use runtime exceptions to indicate programming errors, typically precondition violations.\\n* Don\\'t implement any new `Error` subclasses, and don\\'t define a throwable that does not subclass `Exception` or `RuntimeException`.\\n\\n#### Item 59: Avoid unnecessary use of checked exceptions\\n* When designing an API, only throw a checked exception if it can be prevented by a proper use of the API, and the programmer can take some useful action once thrown.\\n\\n#### Item 60: Favor the use of standard exceptions\\n* Throw a `NullPointerException` instead of an `IllegalArgumentException` if a caller passes in a `null` parameter where prohibited.\\n* Throw an `IndexOutOfBoundsException` instead of an `IllegalArgumentException` if the caller passes in an invalid index for a sequence.\\n\\n#### Item 61: Throw exceptions appropriate to the abstraction\\n* Use exception translation, where low-level exceptions are caught and exceptions appropriate to the higher-level abstraction are thrown.\\n* If an exception does not have a chaining-aware constructor, use the `initCause` method of `Throwable`.\\n* Try to avoid low-level exceptions by checking the higher-level method\\'s parameters upfront.\\n\\n#### Item 62: Document all exceptions thrown by each method\\n* Document the unchecked exceptions a method can throw, thereby documenting its preconditions.\\n* Do not use the `throws` keyword to include unchecked exceptions in a method declaration.\\n\\n#### Item 63: Include failure-capture information in detail messages\\n* The `toString` method, or \"detail message,\" should contain the values of all the parameters and fields contributing to the exception.\\n* To capture this information easily, make them parameters to the constructor, and internally generate the detail message from them.\\n\\n#### Item 64: Strive for failure atomicity\\n* A failed method invocation should leave the object in the state it was prior to the invocation.\\n* Typically you can easily achieve failure atomicity by checking the parameters\\' validity before the operation.\\n\\n### Chapter 10: Concurrency\\n\\n#### Item 66: Synchronize access to shared mutable data\\n* Synchronization is not just for mutual exclusion, but ensuring that a value written by one thread is seen by another.\\n* Both read and write operations on mutable data must be `synchronized` or visibility is not guaranteed.\\n* Beware that the increment and decrement operators are not atomic on volatile integers.\\n\\n#### Item 67: Avoid excessive synchronization\\n* Inside a `synchronized` region, do not invoke a method that is provided by the client as a function object, or can be overriden.\\n* Reentrant locks simplify the construction of multi-threaded object oriented programs, but can allow foreign methods to access an object in an inconsistent state.\\n* Only make a mutable class thread-safe if intended for concurrent use and you can achieve better concurrency with internal locking; otherwise, punt locking to the client.\\n\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('./docs').load_data()\n",
    "documents = [doc.text for doc in documents if doc.text != '']\n",
    "\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(cohere_api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying using co.rerank to find relevant documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Rank: 1, Document Index: 1\n",
      "Document: \n",
      "\n",
      "gpt4all\n",
      "\n",
      "gpt4all is an ecosystem of open-source chatbots trained on massive collections of clean assistant data including code, stories and dialogue. It offers open-source, large language models such as LLaMA and GPT-J trained in an assistant-style.\n",
      "\n",
      "Keywords: Open-source, LLaMa, GPT-J, instruction, assistant\n",
      "\n",
      "\n",
      "Relevance Score: 0.98\n",
      "\n",
      "\n",
      "Document Rank: 2, Document Index: 14\n",
      "Document: \n",
      "\n",
      "DeepPavlov\n",
      "\n",
      "DeepPavlov is an open-source conversational AI library. It is designed for the development of production ready chat-bots and complex conversational systems, as well as research in the area of NLP and, particularly, of dialog systems.\n",
      "\n",
      "Keywords: Conversational, Chatbot, Dialog\n",
      "\n",
      "\n",
      "Relevance Score: 0.96\n",
      "\n",
      "\n",
      "Document Rank: 3, Document Index: 25\n",
      "Document: \n",
      "\n",
      "argilla\n",
      "\n",
      "Argilla is an open-source platform providing advanced NLP labeling, monitoring, and workspaces. It is compatible with many open source ecosystems such as Hugging Face, Stanza, FLAIR, and others.\n",
      "\n",
      "Keywords: NLP, Labeling, Monitoring, Workspaces\n",
      "\n",
      "\n",
      "Relevance Score: 0.62\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Where can I find open source chatbots?\"\n",
    "\n",
    "results = co.rerank(query=query, documents=documents, top_n=3, model='rerank-english-v2.0') \n",
    "for idx, r in enumerate(results):\n",
    "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
    "  print(f\"Document: {r.document['text']}\")\n",
    "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the given markdown about transformer projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_of_the_documents = documents[:len(documents)//2] # due to character limit\n",
    "\n",
    "response = co.summarize( \n",
    "    text='\\n'.join(half_of_the_documents),\n",
    "    model='summarize-xlarge', \n",
    "    length='medium',\n",
    "    extractiveness='medium'\n",
    ")\n",
    "\n",
    "summary = response.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This page lists awesome projects built on top of Transformers. Transformers is more than a toolkit to use pretrained models: it's a community of projects built around it and the Hugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone else to build their dream projects.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
